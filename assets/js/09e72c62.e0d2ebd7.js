"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[3547],{3063:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-2/chapter-7","title":"Chapter 7: Unity for Robotics","description":"Learning Outcomes","source":"@site/docs/module-2/chapter-7.md","sourceDirName":"module-2","slug":"/module-2/chapter-7","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2/chapter-7","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-2/chapter-7.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Sensors Simulation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2/chapter-6"},"next":{"title":"AI Notes - Module 2: Digital Twin (Gazebo & Unity)","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2/ai-notes"}}');var r=t(4848),o=t(8453);const a={sidebar_position:4},s="Chapter 7: Unity for Robotics",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"Unity in Robotics Context",id:"unity-in-robotics-context",level:3},{value:"Key Unity Robotics Components",id:"key-unity-robotics-components",level:3},{value:"Motion Blending in Robotics",id:"motion-blending-in-robotics",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"chapter-7-unity-for-robotics",children:"Chapter 7: Unity for Robotics"})}),"\n",(0,r.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Set up Unity for robotics simulation and visualization"}),"\n",(0,r.jsx)(e.li,{children:"Import and configure humanoid models in Unity"}),"\n",(0,r.jsx)(e.li,{children:"Implement motion blending and animation systems for robots"}),"\n",(0,r.jsx)(e.li,{children:"Create ray-interaction events for robot-environment interaction"}),"\n",(0,r.jsx)(e.li,{children:"Integrate Unity with ROS 2 for real-time control"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,r.jsx)(e.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,r.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Unity 2022.3 LTS or newer"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Unity Robotics Package (com.unity.robotics)"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Unity ML-Agents Toolkit (optional)"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","ROS 2 Humble Hawksbill with Unity integration"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,r.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Understanding of humanoid robot kinematics"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Basic knowledge of Unity 3D development"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Familiarity with C# programming"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed Chapter 5 and 6 content"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,r.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed robot model from Module 1"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Access to Unity Robotics documentation"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,r.jsx)(e.h3,{id:"unity-in-robotics-context",children:"Unity in Robotics Context"}),"\n",(0,r.jsx)(e.p,{children:"Unity is a powerful 3D development platform that offers advanced rendering, physics simulation, and real-time capabilities that make it valuable for robotics applications. Unlike Gazebo's physics-focused approach, Unity excels in visual fidelity, user interaction, and complex scene management."}),"\n",(0,r.jsx)(e.h3,{id:"key-unity-robotics-components",children:"Key Unity Robotics Components"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Robotics Package"}),": The core Unity package for robotics that provides:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"ROS-TCP-Connector for communication with ROS"}),"\n",(0,r.jsx)(e.li,{children:"URDF Importer for converting ROS robot models"}),"\n",(0,r.jsx)(e.li,{children:"Sample scenes and scripts for robotics applications"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Humanoid Rigs"}),": Unity's humanoid system for character animation that can be adapted for:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Human-like robot movement"}),"\n",(0,r.jsx)(e.li,{children:"Motion capture data integration"}),"\n",(0,r.jsx)(e.li,{children:"Advanced animation blending"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Physics Engine"}),": Unity's built-in physics system that provides:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Realistic collision detection"}),"\n",(0,r.jsx)(e.li,{children:"Joint constraints and articulation bodies"}),"\n",(0,r.jsx)(e.li,{children:"Raycasting and collision events"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"motion-blending-in-robotics",children:"Motion Blending in Robotics"}),"\n",(0,r.jsx)(e.p,{children:"Motion blending is crucial for creating natural-looking robot movements:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Locomotion Blending"}),": Smooth transitions between different movement patterns (walk, run, turn)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Task-Oriented Animation"}),": Pre-defined animations for specific tasks (grasping, manipulation)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IK (Inverse Kinematics)"}),": Real-time adjustment of limb positions to reach targets"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"State Machines"}),": Managing different motion states and transitions"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:"graph TD\r\n    A[Unity Robotics Integration] --\x3e B[Model Import]\r\n    A --\x3e C[Animation System]\r\n    A --\x3e D[Physics Simulation]\r\n    A --\x3e E[ROS Communication]\r\n\r\n    B --\x3e B1[URDF Importer]\r\n    B --\x3e B2[3D Model Import]\r\n    B --\x3e B3[Joint Configuration]\r\n\r\n    C --\x3e C1[Humanoid Rig Setup]\r\n    C --\x3e C2[Motion Blending]\r\n    C --\x3e C3[Animation State Machine]\r\n\r\n    D --\x3e D1[Physics Engine]\r\n    D --\x3e D2[Collision Detection]\r\n    D --\x3e D3[Ray Interactions]\r\n\r\n    E --\x3e E1[ROS TCP Connector]\r\n    E --\x3e E2[Message Handling]\r\n    E --\x3e E3[Real-time Control]\r\n\r\n    B1 --\x3e C\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    E1 --\x3e A\n"})}),"\n",(0,r.jsx)(e.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,r.jsx)(e.p,{children:"Let's create a Unity C# script that handles humanoid robot control and motion blending:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// RobotController.cs\r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing UnityEngine;\r\nusing Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Sensor;\r\nusing RosMessageTypes.Geometry;\r\n\r\n[RequireComponent(typeof(Animator))]\r\npublic class RobotController : MonoBehaviour\r\n{\r\n    [Header("ROS Connection")]\r\n    public string rosIPAddress = "127.0.0.1";\r\n    public int rosPort = 10000;\r\n\r\n    [Header("Joint Control")]\r\n    public float moveSpeed = 2.0f;\r\n    public float turnSpeed = 50.0f;\r\n    public float armSpeed = 10.0f;\r\n\r\n    [Header("Animation Parameters")]\r\n    public float walkBlendThreshold = 0.1f;\r\n    public float runBlendThreshold = 0.5f;\r\n    public float turnThreshold = 0.1f;\r\n\r\n    private Animator animator;\r\n    private ROSConnection ros;\r\n\r\n    // Internal state\r\n    private float horizontalInput;\r\n    private float verticalInput;\r\n    private bool isWalking = false;\r\n    private bool isRunning = false;\r\n\r\n    void Start()\r\n    {\r\n        // Get components\r\n        animator = GetComponent<Animator>();\r\n\r\n        // Initialize ROS connection\r\n        ros = ROSConnection.instance;\r\n        ros.Initialize(rosIPAddress, rosPort);\r\n\r\n        // Subscribe to ROS topics\r\n        ros.Subscribe<GeometryMsgsTwist>("/cmd_vel", HandleVelocityCommand);\r\n\r\n        // Log initialization\r\n        Debug.Log("Robot Controller initialized");\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        // Process animation based on current state\r\n        UpdateAnimationState();\r\n    }\r\n\r\n    void HandleVelocityCommand(GeometryMsgsTwist cmd)\r\n    {\r\n        // Convert ROS velocity commands to Unity input\r\n        verticalInput = cmd.linear.x / moveSpeed;  // Normalize to -1 to 1\r\n        horizontalInput = cmd.angular.z / turnSpeed; // Normalize to -1 to 1\r\n\r\n        // Clamp values to valid range\r\n        verticalInput = Mathf.Clamp(verticalInput, -1.0f, 1.0f);\r\n        horizontalInput = Mathf.Clamp(horizontalInput, -1.0f, 1.0f);\r\n\r\n        // Determine motion state\r\n        isWalking = Mathf.Abs(verticalInput) > walkBlendThreshold || Mathf.Abs(horizontalInput) > turnThreshold;\r\n        isRunning = Mathf.Abs(verticalInput) > runBlendThreshold;\r\n\r\n        // Apply movement to the robot\r\n        ApplyRobotMovement();\r\n    }\r\n\r\n    void ApplyRobotMovement()\r\n    {\r\n        // Calculate movement direction\r\n        Vector3 movement = new Vector3(horizontalInput, 0, verticalInput);\r\n        movement = transform.TransformDirection(movement);\r\n        movement *= moveSpeed * Time.deltaTime;\r\n\r\n        // Move the robot\r\n        transform.Translate(movement);\r\n\r\n        // Rotate the robot based on angular velocity\r\n        float rotation = horizontalInput * turnSpeed * Time.deltaTime;\r\n        transform.Rotate(0, rotation, 0);\r\n    }\r\n\r\n    void UpdateAnimationState()\r\n    {\r\n        if (animator == null) return;\r\n\r\n        // Set animation parameters based on current state\r\n        animator.SetFloat("ForwardSpeed", verticalInput);\r\n        animator.SetFloat("TurnSpeed", horizontalInput);\r\n        animator.SetBool("IsWalking", isWalking);\r\n        animator.SetBool("IsRunning", isRunning);\r\n        animator.SetFloat("Speed", Mathf.Abs(verticalInput) + Mathf.Abs(horizontalInput));\r\n    }\r\n\r\n    // Additional methods for robot-specific animations\r\n    public void ExecuteArmMovement(float joint1, float joint2, float joint3)\r\n    {\r\n        // Example: Update arm joint angles based on ROS commands\r\n        // This would involve updating specific joint transforms or animation weights\r\n        Debug.Log($"Executing arm movement: Joint1={joint1}, Joint2={joint2}, Joint3={joint3}");\r\n    }\r\n\r\n    public void ExecuteGripperCommand(bool open)\r\n    {\r\n        // Example: Open/close gripper based on ROS command\r\n        // This would involve updating gripper joint positions\r\n        Debug.Log($"Gripper command: {(open ? "Open" : "Close")}");\r\n    }\r\n\r\n    void OnValidate()\r\n    {\r\n        // Ensure parameters are valid\r\n        moveSpeed = Mathf.Max(0.1f, moveSpeed);\r\n        turnSpeed = Mathf.Max(0.1f, turnSpeed);\r\n        armSpeed = Mathf.Max(0.1f, armSpeed);\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"To use this script in Unity:"})}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Create a new C# script named ",(0,r.jsx)(e.code,{children:"RobotController.cs"})," in your Unity project"]}),"\n",(0,r.jsx)(e.li,{children:"Attach it to a robot GameObject that has an Animator component"}),"\n",(0,r.jsx)(e.li,{children:"Ensure you have imported the Unity Robotics package"}),"\n",(0,r.jsx)(e.li,{children:"Configure the ROS connection settings to match your ROS 2 bridge"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,r.jsx)(e.p,{children:"Now let's create a Unity script for handling ray-interaction events and environmental interaction:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// RayInteractionHandler.cs\r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing UnityEngine;\r\nusing Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Sensor;\r\nusing RosMessageTypes.Geometry;\r\nusing System.Linq;\r\n\r\npublic class RayInteractionHandler : MonoBehaviour\r\n{\r\n    [Header("Ray Configuration")]\r\n    public float rayDistance = 10.0f;\r\n    public float rayUpdateInterval = 0.1f;\r\n    public LayerMask detectionLayers = -1; // All layers\r\n\r\n    [Header("Object Interaction")]\r\n    public float interactionDistance = 3.0f;\r\n    public string interactionLayerName = "Interactive";\r\n    public float gripForce = 100.0f;\r\n\r\n    [Header("ROS Integration")]\r\n    public string rosIPAddress = "127.0.0.1";\r\n    public int rosPort = 10000;\r\n\r\n    private ROSConnection ros;\r\n    private float lastRayUpdate = 0.0f;\r\n\r\n    // For object detection and interaction\r\n    private RaycastHit detectedObject;\r\n    private bool hasDetectedObject = false;\r\n\r\n    void Start()\r\n    {\r\n        // Initialize ROS connection\r\n        ros = ROSConnection.instance;\r\n        ros.Initialize(rosIPAddress, rosPort);\r\n\r\n        Debug.Log("Ray Interaction Handler initialized");\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        // Update ray interactions at specified interval\r\n        if (Time.time - lastRayUpdate > rayUpdateInterval)\r\n        {\r\n            PerformRayInteractions();\r\n            lastRayUpdate = Time.time;\r\n        }\r\n    }\r\n\r\n    void PerformRayInteractions()\r\n    {\r\n        // Perform raycast from the robot\'s forward direction\r\n        Ray ray = new Ray(transform.position, transform.forward);\r\n        RaycastHit hit;\r\n\r\n        if (Physics.Raycast(ray, out hit, rayDistance, detectionLayers))\r\n        {\r\n            // Store detected object info\r\n            detectedObject = hit;\r\n            hasDetectedObject = true;\r\n\r\n            // Process different types of interactions\r\n            ProcessRayHit(hit);\r\n        }\r\n        else\r\n        {\r\n            hasDetectedObject = false;\r\n        }\r\n    }\r\n\r\n    void ProcessRayHit(RaycastHit hit)\r\n    {\r\n        GameObject hitObject = hit.collider.gameObject;\r\n\r\n        // Check if the object is in the interactive layer\r\n        if (hitObject.layer == LayerMask.NameToLayer(interactionLayerName))\r\n        {\r\n            // Send object detection information to ROS\r\n            SendObjectDetectionMessage(hit);\r\n\r\n            // Process specific interaction types\r\n            ProcessInteractiveObject(hitObject, hit);\r\n        }\r\n        else\r\n        {\r\n            // Process non-interactive object (obstacle, etc.)\r\n            ProcessNonInteractiveObject(hitObject, hit);\r\n        }\r\n    }\r\n\r\n    void SendObjectDetectionMessage(RaycastHit hit)\r\n    {\r\n        // In a real implementation, you would send a message like this:\r\n        // var detectionMsg = new ObjectDetectionMsg();\r\n        // detectionMsg.header.stamp = ROSConnection.GetROSTimestamp();\r\n        // detectionMsg.object_name = hit.collider.name;\r\n        // detectionMsg.distance = hit.distance;\r\n        // detectionMsg.angle = Vector3.Angle(transform.forward, hit.normal);\r\n        // ros.Publish("/object_detection", detectionMsg);\r\n\r\n        // For now, just log the detection\r\n        Debug.Log($"Object detected: {hit.collider.name} at distance {hit.distance:F2}m");\r\n    }\r\n\r\n    void ProcessInteractiveObject(GameObject hitObject, RaycastHit hit)\r\n    {\r\n        // Determine type of interactive object\r\n        string objectType = hitObject.tag; // You can use tags to identify object types\r\n\r\n        switch (objectType)\r\n        {\r\n            case "Graspable":\r\n                ProcessGraspableObject(hitObject, hit);\r\n                break;\r\n            case "Button":\r\n                ProcessButtonObject(hitObject, hit);\r\n                break;\r\n            case "Switch":\r\n                ProcessSwitchObject(hitObject, hit);\r\n                break;\r\n            default:\r\n                Debug.Log($"Detected unknown interactive object: {hitObject.name}");\r\n                break;\r\n        }\r\n    }\r\n\r\n    void ProcessGraspableObject(GameObject obj, RaycastHit hit)\r\n    {\r\n        // Check if object is within interaction distance\r\n        if (hit.distance <= interactionDistance)\r\n        {\r\n            // Highlight object or provide feedback\r\n            HighlightObject(obj, Color.yellow);\r\n\r\n            // Check for grasp command (in real implementation, this would come from ROS)\r\n            if (ShouldGraspObject())\r\n            {\r\n                AttemptGrasp(obj, hit);\r\n            }\r\n        }\r\n    }\r\n\r\n    void ProcessButtonObject(GameObject obj, RaycastHit hit)\r\n    {\r\n        // Check if button is within interaction distance\r\n        if (hit.distance <= interactionDistance)\r\n        {\r\n            HighlightObject(obj, Color.cyan);\r\n\r\n            // Check for button press command\r\n            if (ShouldPressButton())\r\n            {\r\n                PressButton(obj);\r\n            }\r\n        }\r\n    }\r\n\r\n    void ProcessSwitchObject(GameObject obj, RaycastHit hit)\r\n    {\r\n        if (hit.distance <= interactionDistance)\r\n        {\r\n            HighlightObject(obj, Color.magenta);\r\n\r\n            // Check for switch toggle command\r\n            if (ShouldToggleSwitch())\r\n            {\r\n                ToggleSwitch(obj);\r\n            }\r\n        }\r\n    }\r\n\r\n    void ProcessNonInteractiveObject(GameObject obj, RaycastHit hit)\r\n    {\r\n        // This could be an obstacle, wall, etc.\r\n        // Send obstacle detection to ROS\r\n        if (hit.distance < 2.0f) // Close obstacle\r\n        {\r\n            Debug.Log($"Obstacle detected: {obj.name} at {hit.distance:F2}m");\r\n            // In real implementation, send obstacle message to ROS\r\n        }\r\n    }\r\n\r\n    void HighlightObject(GameObject obj, Color highlightColor)\r\n    {\r\n        // Add visual feedback to the object\r\n        Renderer renderer = obj.GetComponent<Renderer>();\r\n        if (renderer != null)\r\n        {\r\n            // Store original material for later restoration\r\n            if (!renderer.material.HasProperty("_EmissionColor"))\r\n            {\r\n                renderer.material.EnableKeyword("_EMISSION");\r\n            }\r\n\r\n            // Apply highlight effect\r\n            renderer.material.SetColor("_EmissionColor", highlightColor * 0.5f);\r\n        }\r\n    }\r\n\r\n    void AttemptGrasp(GameObject obj, RaycastHit hit)\r\n    {\r\n        Debug.Log($"Attempting to grasp: {obj.name}");\r\n\r\n        // In a real implementation, you would:\r\n        // 1. Send grasp command to robot\'s gripper\r\n        // 2. Calculate grasp position and orientation\r\n        // 3. Apply physics constraints to attach object\r\n        // 4. Send confirmation to ROS\r\n\r\n        // For simulation, just log the attempt\r\n        Debug.Log($"Grasp command sent for {obj.name}");\r\n    }\r\n\r\n    void PressButton(GameObject button)\r\n    {\r\n        Debug.Log($"Pressing button: {button.name}");\r\n\r\n        // Visual feedback for button press\r\n        button.GetComponent<Renderer>().material.SetColor("_EmissionColor", Color.red);\r\n\r\n        // Simulate button press for a short time\r\n        StartCoroutine(ReleaseButton(button));\r\n    }\r\n\r\n    IEnumerator ReleaseButton(GameObject button)\r\n    {\r\n        yield return new WaitForSeconds(0.5f);\r\n\r\n        // Reset button appearance\r\n        button.GetComponent<Renderer>().material.SetColor("_EmissionColor", Color.black);\r\n        Debug.Log($"Button released: {button.name}");\r\n    }\r\n\r\n    void ToggleSwitch(GameObject switchObj)\r\n    {\r\n        Debug.Log($"Toggling switch: {switchObj.name}");\r\n\r\n        // In real implementation, this would change the switch state\r\n        // and possibly trigger other events\r\n    }\r\n\r\n    // Helper methods to determine if actions should be taken\r\n    // (In real implementation, these would be driven by ROS commands)\r\n    bool ShouldGraspObject()\r\n    {\r\n        // This would be triggered by a ROS command\r\n        // For demo purposes, return false\r\n        return false;\r\n    }\r\n\r\n    bool ShouldPressButton()\r\n    {\r\n        return false;\r\n    }\r\n\r\n    bool ShouldToggleSwitch()\r\n    {\r\n        return false;\r\n    }\r\n\r\n    // Debug visualization\r\n    void OnDrawGizmos()\r\n    {\r\n        // Draw the ray in the editor\r\n        Gizmos.color = Color.red;\r\n        Gizmos.DrawRay(transform.position, transform.forward * rayDistance);\r\n    }\r\n\r\n    public bool IsObjectDetected()\r\n    {\r\n        return hasDetectedObject;\r\n    }\r\n\r\n    public string GetDetectedObjectName()\r\n    {\r\n        return hasDetectedObject ? detectedObject.collider.name : "None";\r\n    }\r\n\r\n    public float GetDetectedObjectDistance()\r\n    {\r\n        return hasDetectedObject ? detectedObject.distance : float.MaxValue;\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"To use this script in Unity:"})}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Create a new C# script named ",(0,r.jsx)(e.code,{children:"RayInteractionHandler.cs"})," in your Unity project"]}),"\n",(0,r.jsx)(e.li,{children:"Attach it to your robot GameObject"}),"\n",(0,r.jsx)(e.li,{children:"Configure the layers and tags for interactive objects"}),"\n",(0,r.jsx)(e.li,{children:"Set up the ROS connection parameters"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,r.jsx)(e.p,{children:"Create a Unity scene that demonstrates a complete humanoid robot with:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"A humanoid model with proper rig setup"}),"\n",(0,r.jsx)(e.li,{children:"Multiple animation states (idle, walk, turn, grasp)"}),"\n",(0,r.jsx)(e.li,{children:"Smooth transitions between states"}),"\n",(0,r.jsx)(e.li,{children:"Integration with a simple ROS node that sends commands to control the animations"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Hint:"})," Use Unity's Animation Controller with state machines and blend trees to create smooth motion transitions, and connect to ROS to receive commands that trigger different animation states."]}),"\n",(0,r.jsx)(e.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,r.jsx)(e.p,{children:"To verify that your Unity robotics setup is working correctly:"}),"\n",(0,r.jsx)(e.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"When connecting to ROS: Connection establishment messages"}),"\n",(0,r.jsx)(e.li,{children:"When processing interactions: Detection and action logs"}),"\n",(0,r.jsx)(e.li,{children:"When animations play: State transition information"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"In Unity Editor: Real-time robot movement and animation based on ROS commands"}),"\n",(0,r.jsx)(e.li,{children:"In Unity Play Mode: Robot responds to virtual sensors and interactions"}),"\n",(0,r.jsx)(e.li,{children:"In ROS environment: Unity publishes sensor data and receives control commands"}),"\n",(0,r.jsx)(e.li,{children:"System monitoring: Proper message flow between Unity and ROS"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,r.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Robot controller script created with ROS integration"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Ray interaction system implemented for environmental sensing"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Animation state machine configured for smooth motion blending"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Unity-ROS communication established and tested"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Complete humanoid robot demonstration scene (Try Yourself task)"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"This chapter covered the integration of Unity with robotics applications, focusing on humanoid robot simulation and control. You learned about Unity's robotics packages, humanoid rig setup, motion blending techniques, and ray-interaction systems for environmental sensing. The examples demonstrated how to create responsive robot controllers that integrate with ROS 2 and handle complex interactions with the virtual environment."}),"\n",(0,r.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Unity Technologies. (2023). ",(0,r.jsx)(e.em,{children:"Unity Robotics Hub Documentation"}),". Retrieved from ",(0,r.jsx)(e.a,{href:"https://docs.unity3d.com/Packages/com.unity.robotics@latest",children:"https://docs.unity3d.com/Packages/com.unity.robotics@latest"})]}),"\n",(0,r.jsx)(e.li,{children:"Source 013: Research on Unity's capabilities for robotics simulation"}),"\n",(0,r.jsxs)(e.li,{children:["Unity Technologies. (2022). ",(0,r.jsx)(e.em,{children:"Unity ML-Agents Toolkit: A Novel Approach to Reinforcement Learning in Games"}),". arXiv preprint arXiv:1809.00382."]}),"\n",(0,r.jsx)(e.li,{children:"Source 017: Technical paper on humanoid rigging systems in Unity"}),"\n",(0,r.jsx)(e.li,{children:"Source 018: Research on ray-interaction events and physics-based simulation for robotics"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>s});var i=t(6540);const r={},o=i.createContext(r);function a(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);