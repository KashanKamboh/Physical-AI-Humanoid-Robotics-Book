"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[3361],{3780:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-3/chapter-10","title":"Chapter 10: Perception Pipeline","description":"Learning Outcomes","source":"@site/docs/module-3/chapter-10.md","sourceDirName":"module-3","slug":"/module-3/chapter-10","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3/chapter-10","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-3/chapter-10.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Autonomous Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3/chapter-9"},"next":{"title":"AI Notes - Module 3: AI-Robot Brain (NVIDIA Isaac)","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3/ai-notes"}}');var s=r(4848),t=r(8453);const o={sidebar_position:4},a="Chapter 10: Perception Pipeline",c={},l=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:3},{value:"Isaac ROS Perception Components",id:"isaac-ros-perception-components",level:3},{value:"GPU Acceleration in Perception",id:"gpu-acceleration-in-perception",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-10-perception-pipeline",children:"Chapter 10: Perception Pipeline"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement GPU-accelerated perception pipelines using Isaac ROS"}),"\n",(0,s.jsx)(n.li,{children:"Configure camera and sensor drivers for real-time processing"}),"\n",(0,s.jsx)(n.li,{children:"Create object detection and tracking systems"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception with navigation and manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Optimize perception pipelines for real-time performance"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,s.jsx)(n.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 Humble Hawksbill (or newer)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Isaac ROS packages (isaac_ros_apriltag, isaac_ros_detectnet, etc.)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","CUDA 11.8+ with compatible GPU"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","OpenCV and related computer vision libraries"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed Module 1 and 2 content"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understanding of ROS 2 communication patterns"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Basic knowledge of computer vision concepts"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Familiarity with sensor data processing"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed Chapter 8 and 9 content"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Access to Isaac ROS documentation and tutorials"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sample images and video data for testing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,s.jsx)(n.h3,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The perception pipeline processes raw sensor data to extract meaningful information about the environment. It typically follows this sequence:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Input"}),": Raw data from cameras, LiDAR, IMU, etc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Calibration, rectification, noise reduction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Extraction"}),": Detection of edges, corners, objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Recognition"}),": Classification and identification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tracking"}),": Following objects over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scene Understanding"}),": Semantic segmentation and spatial relationships"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-perception-components",children:"Isaac ROS Perception Components"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": Detects and localizes AprilTag fiducial markers:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU-accelerated marker detection"}),"\n",(0,s.jsx)(n.li,{children:"6-DOF pose estimation"}),"\n",(0,s.jsx)(n.li,{children:"Real-time performance"}),"\n",(0,s.jsx)(n.li,{children:"Multi-marker support"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS DetectNet"}),": Performs object detection using deep learning:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"TensorRT-optimized neural networks"}),"\n",(0,s.jsx)(n.li,{children:"Real-time inference on GPU"}),"\n",(0,s.jsx)(n.li,{children:"Custom model support"}),"\n",(0,s.jsx)(n.li,{children:"Multiple object class detection"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Stereo DNN"}),": Stereo vision and depth estimation:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU-accelerated stereo matching"}),"\n",(0,s.jsx)(n.li,{children:"Depth map generation"}),"\n",(0,s.jsx)(n.li,{children:"Disparity computation"}),"\n",(0,s.jsx)(n.li,{children:"3D point cloud creation"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": Image processing and enhancement:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Color correction and calibration"}),"\n",(0,s.jsx)(n.li,{children:"Noise reduction and filtering"}),"\n",(0,s.jsx)(n.li,{children:"Format conversion and compression"}),"\n",(0,s.jsx)(n.li,{children:"Real-time image enhancement"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"gpu-acceleration-in-perception",children:"GPU Acceleration in Perception"}),"\n",(0,s.jsx)(n.p,{children:"GPU acceleration enables real-time processing of perception tasks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Parallel processing of image pixels"}),"\n",(0,s.jsx)(n.li,{children:"Tensor operations for neural networks"}),"\n",(0,s.jsx)(n.li,{children:"Memory transfer optimization"}),"\n",(0,s.jsx)(n.li,{children:"Pipeline parallelism for throughput"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Perception Pipeline] --\x3e B[Sensor Input]\r\n    A --\x3e C[Preprocessing]\r\n    A --\x3e D[Feature Extraction]\r\n    A --\x3e E[Object Recognition]\r\n    A --\x3e F[Tracking]\r\n    A --\x3e G[Scene Understanding]\r\n\r\n    B --\x3e B1[Camera Data]\r\n    B --\x3e B2[Lidar Data]\r\n    B --\x3e B3[IMU Data]\r\n\r\n    C --\x3e C1[Calibration]\r\n    C --\x3e C2[Rectification]\r\n    C --\x3e C3[Noise Reduction]\r\n\r\n    D --\x3e D1[Edge Detection]\r\n    D --\x3e D2[Corner Detection]\r\n    D --\x3e D3[Feature Matching]\r\n\r\n    E --\x3e E1[Classification]\r\n    E --\x3e E2[Detection]\r\n    E --\x3e E3[Segmentation]\r\n\r\n    F --\x3e F1[Object Tracking]\r\n    F --\x3e F2[Feature Tracking]\r\n    F --\x3e F3[Optical Flow]\r\n\r\n    G --\x3e G1[Semantic Segmentation]\r\n    G --\x3e G2[3D Reconstruction]\r\n    G --\x3e G3[Scene Graph]\r\n\r\n    B1 --\x3e C\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    E1 --\x3e F\r\n    F1 --\x3e G\n"})}),"\n",(0,s.jsx)(n.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a perception pipeline that processes camera data for object detection:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# perception_pipeline.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\r\nfrom geometry_msgs.msg import Point\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nfrom PIL import Image as PILImage\r\nimport time\r\n\r\n\r\nclass PerceptionPipeline(Node):\r\n    \"\"\"\r\n    A perception pipeline that processes camera data for object detection.\r\n    This demonstrates GPU-accelerated perception using Isaac ROS concepts.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('perception_pipeline')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Create QoS profile for sensor data\r\n        sensor_qos = QoSProfile(\r\n            reliability=ReliabilityPolicy.BEST_EFFORT,\r\n            history=HistoryPolicy.KEEP_LAST,\r\n            depth=1\r\n        )\r\n\r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(\r\n            Detection2DArray,\r\n            '/perception/detections',\r\n            10\r\n        )\r\n        self.processed_image_pub = self.create_publisher(\r\n            Image,\r\n            '/perception/processed_image',\r\n            10\r\n        )\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_raw',\r\n            self.image_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            '/camera/rgb/camera_info',\r\n            self.camera_info_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        # Internal state\r\n        self.camera_info = None\r\n        self.detection_model = None\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Resize((416, 416)),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        ])\r\n\r\n        # Initialize detection model (simplified - in real implementation, use Isaac ROS DetectNet)\r\n        self.initialize_detection_model()\r\n\r\n        self.get_logger().info('Perception Pipeline initialized')\r\n\r\n    def initialize_detection_model(self):\r\n        \"\"\"Initialize the object detection model\"\"\"\r\n        # In real implementation, this would load a TensorRT-optimized model\r\n        # For demonstration, we'll use a simple OpenCV-based approach\r\n        self.get_logger().info('Initializing detection model...')\r\n\r\n        # For demonstration purposes, we'll use OpenCV's DNN module\r\n        # In Isaac ROS, this would be replaced with TensorRT-optimized models\r\n        try:\r\n            # Load a pre-trained model (simplified)\r\n            # In Isaac ROS, this would be a TensorRT engine\r\n            self.detection_model = {\r\n                'initialized': True,\r\n                'model_name': 'YOLOv5s (simulated)',\r\n                'input_size': (416, 416),\r\n                'classes': ['person', 'bicycle', 'car', 'motorcycle', 'airplane',\r\n                           'bus', 'train', 'truck', 'boat', 'traffic light']\r\n            }\r\n            self.get_logger().info('Detection model initialized successfully')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to initialize detection model: {e}')\r\n            self.detection_model = None\r\n\r\n    def camera_info_callback(self, msg):\r\n        \"\"\"Process camera calibration information\"\"\"\r\n        self.camera_info = msg\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera images\"\"\"\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n\r\n            # Process the image through the perception pipeline\r\n            detections, processed_image = self.process_image(cv_image)\r\n\r\n            # Publish detections\r\n            self.publish_detections(detections, msg.header)\r\n\r\n            # Publish processed image\r\n            processed_msg = self.cv_bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')\r\n            processed_msg.header = msg.header\r\n            self.processed_image_pub.publish(processed_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n\r\n    def process_image(self, cv_image):\r\n        \"\"\"Process image through the perception pipeline\"\"\"\r\n        start_time = time.time()\r\n\r\n        # Clone image for processing\r\n        processed_image = cv_image.copy()\r\n\r\n        # Resize image for model input\r\n        h, w = cv_image.shape[:2]\r\n        input_image = cv2.resize(cv_image, (416, 416))\r\n\r\n        # Perform object detection (simulated)\r\n        detections = self.simulate_object_detection(input_image)\r\n\r\n        # Draw detections on processed image\r\n        for detection in detections:\r\n            bbox = detection['bbox']\r\n            class_name = detection['class']\r\n            confidence = detection['confidence']\r\n\r\n            # Draw bounding box\r\n            cv2.rectangle(processed_image,\r\n                         (int(bbox[0]), int(bbox[1])),\r\n                         (int(bbox[2]), int(bbox[3])),\r\n                         (0, 255, 0), 2)\r\n\r\n            # Draw label\r\n            label = f\"{class_name}: {confidence:.2f}\"\r\n            cv2.putText(processed_image, label,\r\n                       (int(bbox[0]), int(bbox[1])-10),\r\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n\r\n        # Add processing time info\r\n        processing_time = time.time() - start_time\r\n        cv2.putText(processed_image, f\"Processing Time: {processing_time*1000:.1f}ms\",\r\n                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\r\n\r\n        self.get_logger().debug(f'Processed image in {processing_time*1000:.1f}ms')\r\n\r\n        return detections, processed_image\r\n\r\n    def simulate_object_detection(self, image):\r\n        \"\"\"Simulate object detection (in real implementation, use Isaac ROS DetectNet)\"\"\"\r\n        # In Isaac ROS, this would be replaced with actual GPU-accelerated inference\r\n        # For demonstration, we'll simulate detection results\r\n\r\n        # Simulate some detections with random bounding boxes\r\n        detections = []\r\n\r\n        # Add some simulated detections\r\n        for i in range(np.random.randint(0, 5)):  # 0-4 random detections\r\n            # Random bounding box (x, y, x+w, y+h)\r\n            x = np.random.randint(0, 300)\r\n            y = np.random.randint(0, 300)\r\n            w = np.random.randint(50, 150)\r\n            h = np.random.randint(50, 150)\r\n\r\n            # Ensure bounding box is within image bounds\r\n            x2 = min(x + w, 416)\r\n            y2 = min(y + h, 416)\r\n\r\n            detection = {\r\n                'bbox': [x, y, x2, y2],\r\n                'class': np.random.choice(['person', 'car', 'truck', 'bicycle']),\r\n                'confidence': np.random.uniform(0.6, 0.95)\r\n            }\r\n            detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def publish_detections(self, detections, header):\r\n        \"\"\"Publish detection results\"\"\"\r\n        detection_array_msg = Detection2DArray()\r\n        detection_array_msg.header = header\r\n\r\n        for detection in detections:\r\n            detection_msg = Detection2D()\r\n\r\n            # Set bounding box\r\n            bbox = detection['bbox']\r\n            detection_msg.bbox.center.x = (bbox[0] + bbox[2]) / 2.0\r\n            detection_msg.bbox.center.y = (bbox[1] + bbox[3]) / 2.0\r\n            detection_msg.bbox.size_x = abs(bbox[2] - bbox[0])\r\n            detection_msg.bbox.size_y = abs(bbox[3] - bbox[1])\r\n\r\n            # Set detection result\r\n            hypothesis = ObjectHypothesisWithPose()\r\n            hypothesis.hypothesis.class_id = detection['class']\r\n            hypothesis.hypothesis.score = detection['confidence']\r\n            detection_msg.results.append(hypothesis)\r\n\r\n            detection_array_msg.detections.append(detection_msg)\r\n\r\n        self.detection_pub.publish(detection_array_msg)\r\n\r\n        # Log detection info\r\n        if detections:\r\n            classes = [d['class'] for d in detections]\r\n            self.get_logger().info(f'Detected objects: {\", \".join(classes)}')\r\n\r\n\r\nclass IsaacPerceptionNode(PerceptionPipeline):\r\n    \"\"\"\r\n    Extended perception node with Isaac ROS-specific features\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Additional Isaac ROS specific publishers/subscribers\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        self.pointcloud_pub = self.create_publisher(\r\n            Point,  # In real implementation, this would be sensor_msgs/PointCloud2\r\n            '/perception/pointcloud',\r\n            10\r\n        )\r\n\r\n        self.get_logger().info('Isaac Perception Node initialized')\r\n\r\n    def depth_callback(self, msg):\r\n        \"\"\"Process depth camera data\"\"\"\r\n        try:\r\n            # Convert depth image to OpenCV format\r\n            depth_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\r\n\r\n            # Process depth data (simplified)\r\n            self.process_depth_data(depth_image, msg.header)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing depth: {e}')\r\n\r\n    def process_depth_data(self, depth_image, header):\r\n        \"\"\"Process depth data for 3D reconstruction\"\"\"\r\n        # In Isaac ROS, this would integrate with depth processing nodes\r\n        # For demonstration, we'll simulate point cloud generation\r\n\r\n        # Find non-zero depth values (valid depth readings)\r\n        valid_depths = depth_image > 0\r\n        if np.any(valid_depths):\r\n            # Get camera intrinsic parameters from camera info\r\n            if self.camera_info:\r\n                # Calculate 3D points (simplified)\r\n                height, width = depth_image.shape\r\n                cx = self.camera_info.k[2]  # Principal point x\r\n                cy = self.camera_info.k[5]  # Principal point y\r\n                fx = self.camera_info.k[0]  # Focal length x\r\n                fy = self.camera_info.k[4]  # Focal length y\r\n\r\n                # Sample points for visualization\r\n                sample_points = []\r\n                for y in range(0, height, 10):  # Sample every 10th row\r\n                    for x in range(0, width, 10):  # Sample every 10th column\r\n                        depth = depth_image[y, x]\r\n                        if depth > 0:\r\n                            # Calculate 3D position\r\n                            z = depth\r\n                            x_3d = (x - cx) * z / fx\r\n                            y_3d = (y - cy) * z / fy\r\n\r\n                            point = Point()\r\n                            point.x = x_3d\r\n                            point.y = y_3d\r\n                            point.z = z\r\n\r\n                            # Publish a sample point (in real implementation, publish full point cloud)\r\n                            self.pointcloud_pub.publish(point)\r\n                            break  # Only publish one point per callback for efficiency\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    # Create perception pipeline node\r\n    perception_node = IsaacPerceptionNode()\r\n\r\n    try:\r\n        perception_node.get_logger().info('Perception Pipeline running...')\r\n        rclpy.spin(perception_node)\r\n    except KeyboardInterrupt:\r\n        perception_node.get_logger().info('Shutting down Perception Pipeline')\r\n    finally:\r\n        perception_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To run this perception pipeline:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Save it as ",(0,s.jsx)(n.code,{children:"perception_pipeline.py"})]}),"\n",(0,s.jsx)(n.li,{children:"Make sure Isaac ROS packages are installed"}),"\n",(0,s.jsxs)(n.li,{children:["Run: ",(0,s.jsx)(n.code,{children:"ros2 run <package_name> perception_pipeline"})]}),"\n",(0,s.jsx)(n.li,{children:"Provide camera data from simulation or real sensors"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a perception fusion system that combines multiple sensors:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# perception_fusion.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\r\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan, CameraInfo\r\nfrom sensor_msgs_py import point_cloud2\r\nfrom vision_msgs.msg import Detection2DArray, Detection2D\r\nfrom geometry_msgs.msg import Point, TransformStamped\r\nfrom tf2_ros import TransformBuffer, TransformListener\r\nfrom tf2_ros.buffer import Buffer\r\nfrom tf2_ros.transform_listener import TransformListener\r\nfrom std_msgs.msg import Header\r\nfrom builtin_interfaces.msg import Time\r\nimport numpy as np\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport threading\r\nimport time\r\nfrom collections import deque\r\nimport json\r\n\r\n\r\nclass PerceptionFusionNode(Node):\r\n    """\r\n    A perception fusion system that combines multiple sensors for enhanced understanding.\r\n    This demonstrates sensor fusion concepts used in Isaac ROS.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'perception_fusion\')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Create QoS profiles\r\n        sensor_qos = QoSProfile(\r\n            reliability=ReliabilityPolicy.BEST_EFFORT,\r\n            history=HistoryPolicy.KEEP_LAST,\r\n            depth=1\r\n        )\r\n\r\n        # TF buffer and listener\r\n        self.tf_buffer = Buffer()\r\n        self.tf_listener = TransformListener(self.tf_buffer, self)\r\n\r\n        # Publishers\r\n        self.fused_detections_pub = self.create_publisher(\r\n            Detection2DArray,\r\n            \'/perception/fused_detections\',\r\n            10\r\n        )\r\n        self.fused_pointcloud_pub = self.create_publisher(\r\n            PointCloud2,\r\n            \'/perception/fused_pointcloud\',\r\n            10\r\n        )\r\n        self.debug_image_pub = self.create_publisher(\r\n            Image,\r\n            \'/perception/debug_image\',\r\n            10\r\n        )\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/depth/image_raw\',\r\n            self.depth_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        self.lidar_sub = self.create_subscription(\r\n            PointCloud2,\r\n            \'/lidar/points\',\r\n            self.lidar_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.scan_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/rgb/camera_info\',\r\n            self.camera_info_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        # Internal state\r\n        self.camera_info = None\r\n        self.latest_image = None\r\n        self.latest_depth = None\r\n        self.latest_lidar = None\r\n        self.latest_scan = None\r\n        self.image_lock = threading.Lock()\r\n        self.depth_lock = threading.Lock()\r\n        self.lidar_lock = threading.Lock()\r\n        self.scan_lock = threading.Lock()\r\n\r\n        # Data buffers for fusion\r\n        self.image_buffer = deque(maxlen=5)\r\n        self.depth_buffer = deque(maxlen=5)\r\n        self.lidar_buffer = deque(maxlen=5)\r\n\r\n        # Fusion parameters\r\n        self.fusion_window = 0.1  # 100ms window for fusion\r\n        self.min_detection_confidence = 0.5\r\n\r\n        # Timer for fusion processing\r\n        self.fusion_timer = self.create_timer(0.05, self.fusion_callback)  # 20Hz\r\n\r\n        self.get_logger().info(\'Perception Fusion Node initialized\')\r\n\r\n    def camera_info_callback(self, msg):\r\n        """Process camera calibration information"""\r\n        self.camera_info = msg\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera images"""\r\n        with self.image_lock:\r\n            self.latest_image = msg\r\n            # Add to buffer for fusion\r\n            self.image_buffer.append((time.time(), msg))\r\n\r\n    def depth_callback(self, msg):\r\n        """Process incoming depth images"""\r\n        with self.depth_lock:\r\n            self.latest_depth = msg\r\n            # Add to buffer for fusion\r\n            self.depth_buffer.append((time.time(), msg))\r\n\r\n    def lidar_callback(self, msg):\r\n        """Process incoming LiDAR point cloud"""\r\n        with self.lidar_lock:\r\n            self.latest_lidar = msg\r\n            # Add to buffer for fusion\r\n            self.lidar_buffer.append((time.time(), msg))\r\n\r\n    def scan_callback(self, msg):\r\n        """Process incoming laser scan"""\r\n        with self.scan_lock:\r\n            self.latest_scan = msg\r\n\r\n    def fusion_callback(self):\r\n        """Perform sensor fusion"""\r\n        try:\r\n            # Get synchronized data from buffers\r\n            sync_data = self.get_synchronized_data()\r\n\r\n            if sync_data:\r\n                image_msg, depth_msg, lidar_msg = sync_data\r\n\r\n                # Perform fusion\r\n                fused_detections = self.perform_sensor_fusion(\r\n                    image_msg, depth_msg, lidar_msg\r\n                )\r\n\r\n                # Publish fused results\r\n                if fused_detections:\r\n                    header = Header()\r\n                    header.stamp = self.get_clock().now().to_msg()\r\n                    header.frame_id = \'map\'\r\n\r\n                    detection_array_msg = Detection2DArray()\r\n                    detection_array_msg.header = header\r\n                    detection_array_msg.detections = fused_detections\r\n\r\n                    self.fused_detections_pub.publish(detection_array_msg)\r\n\r\n                    self.get_logger().info(f\'Published {len(fused_detections)} fused detections\')\r\n\r\n                # Create debug visualization\r\n                if self.latest_image:\r\n                    debug_image = self.create_debug_visualization(\r\n                        fused_detections, image_msg\r\n                    )\r\n                    if debug_image is not None:\r\n                        debug_msg = self.cv_bridge.cv2_to_imgmsg(\r\n                            debug_image, encoding=\'bgr8\'\r\n                        )\r\n                        debug_msg.header = image_msg.header\r\n                        self.debug_image_pub.publish(debug_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in fusion callback: {e}\')\r\n\r\n    def get_synchronized_data(self):\r\n        """Get synchronized data from all sensors"""\r\n        current_time = time.time()\r\n\r\n        # Look for data within fusion window\r\n        for img_time, img_msg in reversed(self.image_buffer):\r\n            for depth_time, depth_msg in reversed(self.depth_buffer):\r\n                for lidar_time, lidar_msg in reversed(self.lidar_buffer):\r\n                    # Check if all data is within fusion window\r\n                    if (abs(img_time - depth_time) < self.fusion_window and\r\n                        abs(img_time - lidar_time) < self.fusion_window):\r\n\r\n                        return img_msg, depth_msg, lidar_msg\r\n\r\n        return None\r\n\r\n    def perform_sensor_fusion(self, image_msg, depth_msg, lidar_msg):\r\n        """Perform sensor fusion between vision and LiDAR"""\r\n        try:\r\n            # Convert image to OpenCV\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(image_msg, desired_encoding=\'bgr8\')\r\n\r\n            # Process image for 2D detections\r\n            image_detections = self.process_image_for_detections(cv_image)\r\n\r\n            # Process LiDAR for 3D objects\r\n            lidar_objects = self.process_lidar_for_objects(lidar_msg)\r\n\r\n            # Fuse detections based on spatial correlation\r\n            fused_detections = self.fuse_2d_3d_detections(\r\n                image_detections, lidar_objects, depth_msg\r\n            )\r\n\r\n            return fused_detections\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in sensor fusion: {e}\')\r\n            return []\r\n\r\n    def process_image_for_detections(self, cv_image):\r\n        """Process image for 2D object detections"""\r\n        # Simulate 2D object detection (in real implementation, use Isaac ROS DetectNet)\r\n        h, w = cv_image.shape[:2]\r\n\r\n        # Simulate some detections\r\n        detections = []\r\n        for i in range(np.random.randint(1, 4)):  # 1-3 random detections\r\n            x = np.random.randint(50, w-100)\r\n            y = np.random.randint(50, h-100)\r\n            w_det = np.random.randint(30, 100)\r\n            h_det = np.random.randint(30, 100)\r\n\r\n            detection = {\r\n                \'bbox\': [x, y, x + w_det, y + h_det],\r\n                \'class\': np.random.choice([\'person\', \'car\', \'truck\', \'bicycle\']),\r\n                \'confidence\': np.random.uniform(0.6, 0.9)\r\n            }\r\n            detections.append(detection)\r\n\r\n        return detections\r\n\r\n    def process_lidar_for_objects(self, lidar_msg):\r\n        """Process LiDAR point cloud for 3D objects"""\r\n        # In real implementation, use Isaac ROS LiDAR processing\r\n        # For demonstration, simulate object detection from point cloud\r\n\r\n        try:\r\n            # Read point cloud data\r\n            points = list(point_cloud2.read_points(\r\n                lidar_msg,\r\n                field_names=("x", "y", "z"),\r\n                skip_nans=True\r\n            ))\r\n\r\n            # Cluster points to find objects (simplified)\r\n            objects = []\r\n            if len(points) > 10:  # Need some points to cluster\r\n                # Group points into clusters (simulated)\r\n                for i in range(min(3, len(points) // 20)):  # 3 potential objects\r\n                    # Calculate centroid of random cluster\r\n                    cluster_points = points[i*20:(i+1)*20] if i*20 < len(points) else points[i*20:]\r\n                    if cluster_points:\r\n                        centroid = np.mean(cluster_points, axis=0)\r\n                        obj = {\r\n                            \'position\': centroid[:3],\r\n                            \'size\': np.random.uniform(0.5, 2.0),  # estimated size\r\n                            \'class\': np.random.choice([\'person\', \'car\', \'obstacle\'])\r\n                        }\r\n                        objects.append(obj)\r\n\r\n            return objects\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error processing LiDAR: {e}\')\r\n            return []\r\n\r\n    def fuse_2d_3d_detections(self, image_detections, lidar_objects, depth_msg):\r\n        """Fuse 2D image detections with 3D LiDAR objects"""\r\n        fused_detections = []\r\n\r\n        for img_det in image_detections:\r\n            # Project 3D LiDAR objects to 2D image space\r\n            for lidar_obj in lidar_objects:\r\n                # Convert 3D point to 2D using camera parameters\r\n                if self.camera_info:\r\n                    # Simplified projection (in real implementation, use proper camera model)\r\n                    x_2d = int(lidar_obj[\'position\'][0] * self.camera_info.k[0] / lidar_obj[\'position\'][2] + self.camera_info.k[2])\r\n                    y_2d = int(lidar_obj[\'position\'][1] * self.camera_info.k[4] / lidar_obj[\'position\'][2] + self.camera_info.k[5])\r\n\r\n                    # Check if 2D projection is near image detection\r\n                    bbox = img_det[\'bbox\']\r\n                    center_x = (bbox[0] + bbox[2]) / 2\r\n                    center_y = (bbox[1] + bbox[3]) / 2\r\n\r\n                    distance = np.sqrt((x_2d - center_x)**2 + (y_2d - center_y)**2)\r\n\r\n                    if distance < 50:  # Within 50 pixels\r\n                        # Create fused detection\r\n                        fused_detection = Detection2D()\r\n                        fused_detection.bbox.center.x = center_x\r\n                        fused_detection.bbox.center.y = center_y\r\n                        fused_detection.bbox.size_x = bbox[2] - bbox[0]\r\n                        fused_detection.bbox.size_y = bbox[3] - bbox[1]\r\n\r\n                        # Use image detection class with higher confidence\r\n                        from vision_msgs.msg import ObjectHypothesisWithPose\r\n                        hypothesis = ObjectHypothesisWithPose()\r\n                        hypothesis.hypothesis.class_id = img_det[\'class\']\r\n                        hypothesis.hypothesis.score = max(img_det[\'confidence\'], 0.7)  # Boost confidence due to fusion\r\n                        fused_detection.results.append(hypothesis)\r\n\r\n                        fused_detections.append(fused_detection)\r\n\r\n        return fused_detections\r\n\r\n    def create_debug_visualization(self, detections, image_msg):\r\n        """Create debug visualization showing fused detections"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(image_msg, desired_encoding=\'bgr8\')\r\n            debug_image = cv_image.copy()\r\n\r\n            # Draw fused detections\r\n            for detection in detections:\r\n                center_x = int(detection.bbox.center.x)\r\n                center_y = int(detection.bbox.center.y)\r\n                size_x = int(detection.bbox.size_x / 2)\r\n                size_y = int(detection.bbox.size_y / 2)\r\n\r\n                # Draw bounding box\r\n                pt1 = (center_x - size_x, center_y - size_y)\r\n                pt2 = (center_x + size_x, center_y + size_y)\r\n                cv2.rectangle(debug_image, pt1, pt2, (0, 255, 255), 2)\r\n\r\n                # Draw class label\r\n                if detection.results:\r\n                    class_name = detection.results[0].hypothesis.class_id\r\n                    confidence = detection.results[0].hypothesis.score\r\n                    label = f"{class_name}: {confidence:.2f}"\r\n                    cv2.putText(debug_image, label,\r\n                               (pt1[0], pt1[1]-10),\r\n                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\r\n\r\n            # Add fusion info\r\n            cv2.putText(debug_image, "Sensor Fusion",\r\n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\r\n            cv2.putText(debug_image, f"Detections: {len(detections)}",\r\n                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\r\n\r\n            return debug_image\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error creating debug visualization: {e}\')\r\n            return None\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    # Create perception fusion node\r\n    fusion_node = PerceptionFusionNode()\r\n\r\n    try:\r\n        fusion_node.get_logger().info(\'Perception Fusion running...\')\r\n        rclpy.spin(fusion_node)\r\n    except KeyboardInterrupt:\r\n        fusion_node.get_logger().info(\'Shutting down Perception Fusion\')\r\n    finally:\r\n        fusion_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To run this perception fusion system:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Save it as ",(0,s.jsx)(n.code,{children:"perception_fusion.py"})]}),"\n",(0,s.jsx)(n.li,{children:"Make sure Isaac ROS packages are installed"}),"\n",(0,s.jsxs)(n.li,{children:["Run: ",(0,s.jsx)(n.code,{children:"ros2 run <package_name> perception_fusion"})]}),"\n",(0,s.jsx)(n.li,{children:"Provide synchronized data from cameras, depth sensors, and LiDAR"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,s.jsx)(n.p,{children:"Create an advanced perception system that includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time semantic segmentation using Isaac ROS Segmentation"}),"\n",(0,s.jsx)(n.li,{children:"Multi-object tracking that maintains object identities across frames"}),"\n",(0,s.jsx)(n.li,{children:"3D object detection that combines stereo vision with LiDAR data"}),"\n",(0,s.jsx)(n.li,{children:"A perception quality assessment module that evaluates confidence in detections"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hint:"})," Use Isaac ROS's specialized packages for each perception task and implement a modular architecture that can be easily extended with new perception capabilities."]}),"\n",(0,s.jsx)(n.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,s.jsx)(n.p,{children:"To verify that your perception system is working correctly:"}),"\n",(0,s.jsx)(n.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When starting perception nodes: Initialization messages and GPU detection"}),"\n",(0,s.jsx)(n.li,{children:"When processing data: Frame rates and processing times"}),"\n",(0,s.jsx)(n.li,{children:"When detecting objects: Detection results and confidence scores"}),"\n",(0,s.jsx)(n.li,{children:"When fusing sensors: Fusion success rates and correlation metrics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"In RViz2: Visualization of detected objects and bounding boxes"}),"\n",(0,s.jsx)(n.li,{children:"In image viewers: Overlay of detection results on camera images"}),"\n",(0,s.jsx)(n.li,{children:"In point cloud viewers: Fused sensor data visualization"}),"\n",(0,s.jsx)(n.li,{children:"System monitoring: CPU/GPU utilization and memory usage"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Basic perception pipeline with object detection implemented"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multi-sensor fusion system with camera and LiDAR"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","GPU-accelerated processing for real-time performance"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Debug visualization showing detection results"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Advanced perception system with tracking and segmentation (Try Yourself task)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Perception quality assessment module completed"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covered perception pipeline development using Isaac ROS, focusing on GPU-accelerated computer vision and sensor fusion. You learned about the key components of perception systems including object detection, depth estimation, and sensor fusion. The examples demonstrated implementing a perception pipeline with real-time processing and a fusion system that combines multiple sensors for enhanced environmental understanding."}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["NVIDIA. (2023). ",(0,s.jsx)(n.em,{children:"Isaac ROS Perception Packages Documentation"}),". Retrieved from ",(0,s.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_perception/index.html",children:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_perception/index.html"})]}),"\n",(0,s.jsxs)(n.li,{children:["NVIDIA. (2023). ",(0,s.jsx)(n.em,{children:"GPU-Accelerated Computer Vision for Robotics"}),". Technical Report."]}),"\n",(0,s.jsxs)(n.li,{children:["Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ",(0,s.jsx)(n.em,{children:"arXiv preprint arXiv:1804.02767"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. ",(0,s.jsx)(n.em,{children:"CVPR"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Behley, J., et al. (2019). SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. ",(0,s.jsx)(n.em,{children:"arXiv preprint arXiv:1904.01416"}),"."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);