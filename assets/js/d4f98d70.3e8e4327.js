"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[5089],{2496:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/chapter-11","title":"Chapter 11: Voice-to-Action Agents","description":"Learning Outcomes","source":"@site/docs/module-4/chapter-11.md","sourceDirName":"module-4","slug":"/module-4/chapter-11","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/chapter-11","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-4/chapter-11.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/intro"},"next":{"title":"Chapter 12: Cognitive Task Planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/chapter-12"}}');var i=r(4848),o=r(8453);const s={sidebar_position:2},a="Chapter 11: Voice-to-Action Agents",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:3},{value:"Speech Recognition in Robotics",id:"speech-recognition-in-robotics",level:3},{value:"Natural Language Understanding (NLU)",id:"natural-language-understanding-nlu",level:3},{value:"Voice Command Integration Architecture",id:"voice-command-integration-architecture",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-11-voice-to-action-agents",children:"Chapter 11: Voice-to-Action Agents"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement speech recognition systems for robotic command interpretation"}),"\n",(0,i.jsx)(n.li,{children:"Create natural language processing pipelines for command parsing"}),"\n",(0,i.jsx)(n.li,{children:"Develop intent recognition models for robotic actions"}),"\n",(0,i.jsx)(n.li,{children:"Integrate voice commands with ROS 2 action servers"}),"\n",(0,i.jsx)(n.li,{children:"Build voice-activated task execution systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,i.jsx)(n.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 Humble Hawksbill (or newer)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Python 3.8+ with speech recognition libraries"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","OpenAI Whisper or similar ASR system"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Transformers library for NLP processing"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed Module 1-3 content"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understanding of ROS 2 communication patterns"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Basic knowledge of natural language processing"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Familiarity with action servers and clients"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed Chapter 10 content"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Access to speech recognition models and APIs"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sample voice command datasets for training"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,i.jsx)(n.h3,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The voice-to-action system processes natural language commands and converts them into executable robotic actions through several stages:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text using ASR systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding"}),": Parsing text for intent and entities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Mapping"}),": Translating understood commands to robot actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Execution"}),": Executing mapped actions through ROS 2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Generation"}),": Providing status updates to the user"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"speech-recognition-in-robotics",children:"Speech Recognition in Robotics"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"})," systems convert spoken language to text:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenAI Whisper"}),": State-of-the-art multilingual ASR"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Google Speech-to-Text"}),": Cloud-based recognition service"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Kaldi"}),": Open-source speech recognition framework"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vosk"}),": Lightweight offline speech recognition"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-time Processing"}),": Robotics applications require low-latency recognition:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Streaming audio processing"}),"\n",(0,i.jsx)(n.li,{children:"Wake word detection"}),"\n",(0,i.jsx)(n.li,{children:"Noise cancellation and filtering"}),"\n",(0,i.jsx)(n.li,{children:"Speaker identification and separation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"natural-language-understanding-nlu",children:"Natural Language Understanding (NLU)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Intent Recognition"}),": Identifying the user's goal or action:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Navigation commands (go to, move to, navigate)"}),"\n",(0,i.jsx)(n.li,{children:"Manipulation commands (pick up, place, grasp)"}),"\n",(0,i.jsx)(n.li,{children:"Information requests (what is, where is, how many)"}),"\n",(0,i.jsx)(n.li,{children:"Control commands (stop, start, continue)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying specific objects, locations, or parameters:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Object names (ball, cup, table)"}),"\n",(0,i.jsx)(n.li,{children:"Locations (kitchen, bedroom, office)"}),"\n",(0,i.jsx)(n.li,{children:"Quantities (one, two, several)"}),"\n",(0,i.jsx)(n.li,{children:"Colors and attributes (red, large, heavy)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"voice-command-integration-architecture",children:"Voice Command Integration Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The system architecture typically includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Input"}),": Microphones and audio preprocessing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ASR Engine"}),": Speech-to-text conversion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NLU Module"}),": Intent and entity extraction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Router"}),": Mapping to ROS actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Executor"}),": ROS 2 action servers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback System"}),": Voice and visual feedback"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Voice Command System] --\x3e B[Audio Input]\r\n    A --\x3e C[Speech Recognition]\r\n    A --\x3e D[Natural Language Understanding]\r\n    A --\x3e E[Command Mapping]\r\n    A --\x3e F[Action Execution]\r\n    A --\x3e G[Feedback System]\r\n\r\n    B --\x3e B1[Microphone Array]\r\n    B --\x3e B2[Audio Preprocessing]\r\n    B --\x3e B3[Noise Filtering]\r\n\r\n    C --\x3e C1[ASR Engine]\r\n    C --\x3e C2[Text Output]\r\n    C --\x3e C3[Confidence Scoring]\r\n\r\n    D --\x3e D1[Intent Recognition]\r\n    D --\x3e D2[Entity Extraction]\r\n    D --\x3e D3[Command Parsing]\r\n\r\n    E --\x3e E1[Action Mapping]\r\n    E --\x3e E2[Parameter Extraction]\r\n    E --\x3e E3[Validation]\r\n\r\n    F --\x3e F1[ROS Action Server]\r\n    F --\x3e F2[Navigation Actions]\r\n    F --\x3e F3[Manipulation Actions]\r\n\r\n    G --\x3e G1[Voice Feedback]\r\n    G --\x3e G2[Visual Feedback]\r\n    G --\x3e G3[Status Updates]\r\n\r\n    B1 --\x3e C\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    E1 --\x3e F\r\n    F1 --\x3e G\r\n    G1 --\x3e A\n"})}),"\n",(0,i.jsx)(n.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,i.jsx)(n.p,{children:"Let's create a voice command processor that converts speech to ROS actions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# voice_command_processor.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.qos import QoSProfile\r\nfrom std_msgs.msg import String, Bool\r\nfrom sensor_msgs.msg import AudioData\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom builtin_interfaces.msg import Duration\r\n\r\nimport speech_recognition as sr\r\nimport threading\r\nimport queue\r\nimport time\r\nimport json\r\nimport re\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional, Dict, List\r\n\r\n\r\n@dataclass\r\nclass ParsedCommand:\r\n    \"\"\"Data class for parsed voice commands\"\"\"\r\n    intent: str\r\n    entities: Dict[str, str]\r\n    confidence: float\r\n    raw_text: str\r\n\r\n\r\nclass VoiceCommandProcessor(Node):\r\n    \"\"\"\r\n    A voice command processor that converts speech to ROS actions.\r\n    This demonstrates the integration of speech recognition with robotic actions.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('voice_command_processor')\r\n\r\n        # Initialize speech recognition\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n\r\n        # Set microphone energy threshold for silence detection\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n\r\n        self.recognizer.energy_threshold = 400  # Adjust based on environment\r\n        self.recognizer.dynamic_energy_threshold = True\r\n\r\n        # Publishers\r\n        self.status_pub = self.create_publisher(String, '/voice/status', 10)\r\n        self.command_pub = self.create_publisher(String, '/voice/commands', 10)\r\n\r\n        # Subscribers\r\n        self.audio_sub = self.create_subscription(\r\n            AudioData,\r\n            '/audio/input',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n\r\n        # Action clients\r\n        self.nav_action_client = ActionClient(\r\n            self,\r\n            NavigateToPose,\r\n            'navigate_to_pose'\r\n        )\r\n\r\n        # Internal state\r\n        self.command_queue = queue.Queue()\r\n        self.is_listening = False\r\n        self.listening_thread = None\r\n        self.wake_word = \"robot\"\r\n        self.confidence_threshold = 0.7\r\n\r\n        # Start voice recognition thread\r\n        self.start_voice_recognition()\r\n\r\n        self.get_logger().info('Voice Command Processor initialized')\r\n\r\n    def start_voice_recognition(self):\r\n        \"\"\"Start the voice recognition thread\"\"\"\r\n        self.is_listening = True\r\n        self.listening_thread = threading.Thread(target=self.voice_recognition_loop)\r\n        self.listening_thread.daemon = True\r\n        self.listening_thread.start()\r\n\r\n    def voice_recognition_loop(self):\r\n        \"\"\"Main loop for continuous voice recognition\"\"\"\r\n        self.get_logger().info('Starting voice recognition loop')\r\n\r\n        while self.is_listening:\r\n            try:\r\n                with self.microphone as source:\r\n                    # Listen for audio with timeout\r\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\r\n\r\n                # Process audio\r\n                self.process_audio(audio)\r\n\r\n            except sr.WaitTimeoutError:\r\n                # No audio detected, continue loop\r\n                continue\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error in voice recognition: {e}')\r\n                time.sleep(0.1)  # Brief pause before retrying\r\n\r\n    def process_audio(self, audio):\r\n        \"\"\"Process captured audio data\"\"\"\r\n        try:\r\n            # Use Google Speech Recognition (you can use other engines like Whisper)\r\n            # For offline recognition, you might use pocketsphinx or vosk\r\n            text = self.recognizer.recognize_google(audio).lower()\r\n\r\n            self.get_logger().info(f'Recognized: {text}')\r\n\r\n            # Check for wake word\r\n            if self.wake_word in text:\r\n                # Extract command after wake word\r\n                command_start = text.find(self.wake_word) + len(self.wake_word)\r\n                command_text = text[command_start:].strip()\r\n\r\n                if command_text:\r\n                    self.process_command(command_text, confidence=0.9)  # High confidence for wake word detection\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f\"Recognized: {text}\"\r\n            self.status_pub.publish(status_msg)\r\n\r\n        except sr.UnknownValueError:\r\n            self.get_logger().debug('Speech recognition could not understand audio')\r\n        except sr.RequestError as e:\r\n            self.get_logger().error(f'Speech recognition request error: {e}')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing audio: {e}')\r\n\r\n    def audio_callback(self, msg):\r\n        \"\"\"Process audio data from ROS topic (alternative to direct microphone)\"\"\"\r\n        # In a real implementation, this would process audio from a ROS audio topic\r\n        # For now, we'll just log the callback\r\n        self.get_logger().debug(f'Received audio message of size: {len(msg.data)}')\r\n\r\n    def process_command(self, command_text: str, confidence: float = 1.0):\r\n        \"\"\"Process recognized command text\"\"\"\r\n        try:\r\n            # Parse the command\r\n            parsed_command = self.parse_command(command_text, confidence)\r\n\r\n            if parsed_command and parsed_command.confidence >= self.confidence_threshold:\r\n                # Execute the command\r\n                success = self.execute_command(parsed_command)\r\n\r\n                if success:\r\n                    self.get_logger().info(f'Command executed: {command_text}')\r\n\r\n                    # Publish command for logging\r\n                    cmd_msg = String()\r\n                    cmd_msg.data = json.dumps({\r\n                        'raw_text': command_text,\r\n                        'intent': parsed_command.intent,\r\n                        'entities': parsed_command.entities,\r\n                        'confidence': parsed_command.confidence\r\n                    })\r\n                    self.command_pub.publish(cmd_msg)\r\n                else:\r\n                    self.get_logger().warning(f'Command execution failed: {command_text}')\r\n            else:\r\n                self.get_logger().info(f'Command below confidence threshold: {command_text} (confidence: {confidence})')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing command: {e}')\r\n\r\n    def parse_command(self, command_text: str, confidence: float) -> Optional[ParsedCommand]:\r\n        \"\"\"Parse natural language command into structured format\"\"\"\r\n        # Define command patterns\r\n        patterns = {\r\n            'navigation': [\r\n                r'go to the (.+)',\r\n                r'move to the (.+)',\r\n                r'navigate to (.+)',\r\n                r'go to (.+)',\r\n                r'go to (.+) room',\r\n                r'move to (.+) room'\r\n            ],\r\n            'stop': [\r\n                r'stop',\r\n                r'hold on',\r\n                r'wait',\r\n                r'pause'\r\n            ],\r\n            'follow': [\r\n                r'follow me',\r\n                r'follow (.+)',\r\n                r'come with me'\r\n            ],\r\n            'find': [\r\n                r'find (.+)',\r\n                r'look for (.+)',\r\n                r'find the (.+)'\r\n            ]\r\n        }\r\n\r\n        # Extract intent\r\n        intent = None\r\n        entities = {}\r\n\r\n        for intent_type, intent_patterns in patterns.items():\r\n            for pattern in intent_patterns:\r\n                match = re.search(pattern, command_text)\r\n                if match:\r\n                    intent = intent_type\r\n                    # Extract entities based on pattern\r\n                    if match.groups():\r\n                        entities['target'] = match.group(1).strip()\r\n                    break\r\n            if intent:\r\n                break\r\n\r\n        # If no intent matched, try more general patterns\r\n        if not intent:\r\n            # Look for location entities\r\n            location_match = re.search(r'to the (.+?)(?:\\s|$)', command_text)\r\n            if location_match:\r\n                intent = 'navigation'\r\n                entities['target'] = location_match.group(1).strip()\r\n\r\n        if intent:\r\n            return ParsedCommand(\r\n                intent=intent,\r\n                entities=entities,\r\n                confidence=confidence,\r\n                raw_text=command_text\r\n            )\r\n\r\n        return None\r\n\r\n    def execute_command(self, parsed_command: ParsedCommand) -> bool:\r\n        \"\"\"Execute the parsed command\"\"\"\r\n        intent = parsed_command.intent\r\n        entities = parsed_command.entities\r\n\r\n        if intent == 'navigation':\r\n            target = entities.get('target', '').lower()\r\n\r\n            # Convert location to coordinates (simplified - in real implementation, use a map)\r\n            location_coords = self.get_location_coordinates(target)\r\n\r\n            if location_coords:\r\n                return self.navigate_to_location(location_coords)\r\n            else:\r\n                self.get_logger().warning(f'Unknown location: {target}')\r\n                return False\r\n\r\n        elif intent == 'stop':\r\n            # In real implementation, this would send a stop command to the navigation system\r\n            self.get_logger().info('Stop command received')\r\n            return True\r\n\r\n        elif intent == 'follow':\r\n            # In real implementation, this would start a following behavior\r\n            target = entities.get('target', 'me')\r\n            self.get_logger().info(f'Following {target}')\r\n            return True\r\n\r\n        elif intent == 'find':\r\n            target = entities.get('target', '')\r\n            self.get_logger().info(f'Finding {target}')\r\n            # In real implementation, this would start a search behavior\r\n            return True\r\n\r\n        else:\r\n            self.get_logger().warning(f'Unknown intent: {intent}')\r\n            return False\r\n\r\n    def get_location_coordinates(self, location_name: str) -> Optional[tuple]:\r\n        \"\"\"Get coordinates for named locations (simplified)\"\"\"\r\n        # In real implementation, this would use a semantic map\r\n        location_map = {\r\n            'kitchen': (3.0, 2.0, 0.0),\r\n            'bedroom': (-2.0, 1.0, 0.0),\r\n            'living room': (0.0, 0.0, 0.0),\r\n            'office': (1.0, -2.0, 0.0),\r\n            'bathroom': (-1.0, -1.0, 0.0)\r\n        }\r\n\r\n        return location_map.get(location_name)\r\n\r\n    def navigate_to_location(self, coordinates: tuple) -> bool:\r\n        \"\"\"Send navigation goal to the robot\"\"\"\r\n        try:\r\n            # Wait for action server\r\n            if not self.nav_action_client.wait_for_server(timeout_sec=1.0):\r\n                self.get_logger().error('Navigation action server not available')\r\n                return False\r\n\r\n            # Create navigation goal\r\n            goal_msg = NavigateToPose.Goal()\r\n            goal_msg.pose.header.frame_id = 'map'\r\n            goal_msg.pose.pose.position.x = coordinates[0]\r\n            goal_msg.pose.pose.position.y = coordinates[1]\r\n            goal_msg.pose.pose.position.z = coordinates[2]\r\n            goal_msg.pose.pose.orientation.w = 1.0  # Default orientation\r\n\r\n            # Send goal asynchronously\r\n            goal_future = self.nav_action_client.send_goal_async(goal_msg)\r\n\r\n            # Wait for result (in real implementation, you might want to handle this asynchronously)\r\n            rclpy.spin_until_future_complete(self, goal_future)\r\n\r\n            goal_handle = goal_future.result()\r\n            if not goal_handle.accepted:\r\n                self.get_logger().error('Navigation goal was rejected')\r\n                return False\r\n\r\n            result_future = goal_handle.get_result_async()\r\n            rclpy.spin_until_future_complete(self, result_future)\r\n\r\n            result = result_future.result().result\r\n            status = result.error_code\r\n\r\n            if status == result.SUCCESS:\r\n                self.get_logger().info('Navigation completed successfully')\r\n                return True\r\n            else:\r\n                self.get_logger().error(f'Navigation failed with status: {status}')\r\n                return False\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in navigation: {e}')\r\n            return False\r\n\r\n    def cleanup(self):\r\n        \"\"\"Clean up resources\"\"\"\r\n        self.is_listening = False\r\n        if self.listening_thread:\r\n            self.listening_thread.join(timeout=1.0)\r\n\r\n\r\nclass AdvancedVoiceProcessor(VoiceCommandProcessor):\r\n    \"\"\"\r\n    Extended voice processor with more sophisticated NLP capabilities\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Enhanced NLP components\r\n        self.intent_classifier = self.initialize_intent_classifier()\r\n        self.entity_extractor = self.initialize_entity_extractor()\r\n\r\n        self.get_logger().info('Advanced Voice Processor initialized')\r\n\r\n    def initialize_intent_classifier(self):\r\n        \"\"\"Initialize intent classification model\"\"\"\r\n        # In real implementation, this would load a trained NLP model\r\n        # For demonstration, we'll use a simple rule-based approach\r\n        return {\r\n            'initialized': True,\r\n            'model_type': 'rule_based',\r\n            'intents': ['navigation', 'manipulation', 'information', 'control']\r\n        }\r\n\r\n    def initialize_entity_extractor(self):\r\n        \"\"\"Initialize entity extraction model\"\"\"\r\n        # In real implementation, this would load an NER model\r\n        return {\r\n            'initialized': True,\r\n            'model_type': 'rule_based',\r\n            'entity_types': ['location', 'object', 'person', 'action']\r\n        }\r\n\r\n    def parse_command(self, command_text: str, confidence: float) -> Optional[ParsedCommand]:\r\n        \"\"\"Enhanced command parsing with better NLP\"\"\"\r\n        # Use more sophisticated parsing\r\n        intent, entities, confidence = self.enhanced_parse(command_text)\r\n\r\n        if intent:\r\n            return ParsedCommand(\r\n                intent=intent,\r\n                entities=entities,\r\n                confidence=confidence,\r\n                raw_text=command_text\r\n            )\r\n\r\n        return None\r\n\r\n    def enhanced_parse(self, command_text: str):\r\n        \"\"\"Enhanced parsing with better context understanding\"\"\"\r\n        # More sophisticated parsing logic\r\n        command_lower = command_text.lower().strip()\r\n\r\n        # Complex command patterns\r\n        patterns = {\r\n            'navigation': [\r\n                (r'go to the (.+?) and (.+)', ['location', 'action']),\r\n                (r'move to (.+?) then (.+)', ['location', 'action']),\r\n                (r'navigate to (.+?) and (.+)', ['location', 'action'])\r\n            ],\r\n            'complex_manipulation': [\r\n                (r'pick up the (.+?) from (.+?) and place it in (.+)', ['object', 'source', 'destination']),\r\n                (r'bring me the (.+?) from (.+)', ['object', 'location'])\r\n            ]\r\n        }\r\n\r\n        for intent_type, pattern_list in patterns.items():\r\n            for pattern, entity_types in pattern_list:\r\n                match = re.search(pattern, command_lower)\r\n                if match:\r\n                    entities = {}\r\n                    for i, entity_type in enumerate(entity_types):\r\n                        if i < len(match.groups()):\r\n                            entities[entity_type] = match.group(i + 1).strip()\r\n                    return intent_type, entities, confidence\r\n\r\n        # Fall back to simple parsing\r\n        return super().parse_command(command_text, confidence).intent, {}, confidence\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    # Create voice command processor\r\n    voice_processor = AdvancedVoiceProcessor()\r\n\r\n    try:\r\n        voice_processor.get_logger().info('Voice Command Processor running...')\r\n        rclpy.spin(voice_processor)\r\n    except KeyboardInterrupt:\r\n        voice_processor.get_logger().info('Shutting down Voice Command Processor')\r\n    finally:\r\n        voice_processor.cleanup()\r\n        voice_processor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"To run this voice command processor:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Save it as ",(0,i.jsx)(n.code,{children:"voice_command_processor.py"})]}),"\n",(0,i.jsxs)(n.li,{children:["Install required dependencies: ",(0,i.jsx)(n.code,{children:"pip install speechrecognition pyaudio"})]}),"\n",(0,i.jsxs)(n.li,{children:["Run: ",(0,i.jsx)(n.code,{children:"ros2 run <package_name> voice_command_processor"})]}),"\n",(0,i.jsx)(n.li,{children:'Speak commands like "robot go to kitchen" or "robot stop"'}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,i.jsx)(n.p,{children:"Now let's create a more advanced voice-to-action system with OpenAI Whisper integration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# whisper_voice_agent.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.qos import QoSProfile\r\nfrom std_msgs.msg import String, Int8\r\nfrom sensor_msgs.msg import AudioData\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom control_msgs.action import FollowJointTrajectory\r\nfrom builtin_interfaces.msg import Duration\r\n\r\nimport torch\r\nimport whisper\r\nimport pyaudio\r\nimport wave\r\nimport threading\r\nimport queue\r\nimport time\r\nimport json\r\nimport re\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional, Dict, List, Any\r\nimport openai\r\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\r\n\r\n\r\n@dataclass\r\nclass VoiceCommand:\r\n    \"\"\"Data class for voice commands with full context\"\"\"\r\n    text: str\r\n    intent: str\r\n    entities: Dict[str, Any]\r\n    confidence: float\r\n    timestamp: float\r\n    audio_duration: float\r\n\r\n\r\nclass WhisperVoiceAgent(Node):\r\n    \"\"\"\r\n    Advanced voice-to-action agent using OpenAI Whisper for speech recognition\r\n    and transformer models for natural language understanding.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('whisper_voice_agent')\r\n\r\n        # Initialize Whisper model (this will download if not present)\r\n        self.get_logger().info('Loading Whisper model...')\r\n        try:\r\n            self.whisper_model = whisper.load_model(\"base\")  # Use \"tiny\" for faster processing\r\n            self.get_logger().info('Whisper model loaded successfully')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Failed to load Whisper model: {e}')\r\n            self.whisper_model = None\r\n\r\n        # Initialize NLP pipeline for intent classification\r\n        self.get_logger().info('Loading NLP models...')\r\n        try:\r\n            self.intent_classifier = pipeline(\r\n                \"text-classification\",\r\n                model=\"microsoft/DialoGPT-medium\"  # This is a placeholder - use appropriate model\r\n            )\r\n            # For demonstration, we'll use a simpler approach\r\n            self.intent_classifier = None\r\n        except Exception as e:\r\n            self.get_logger().warning(f'Could not load advanced NLP model: {e}')\r\n            self.intent_classifier = None\r\n\r\n        # Audio recording setup\r\n        self.audio_format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000  # Whisper works best at 16kHz\r\n        self.chunk = 1024\r\n        self.audio = pyaudio.PyAudio()\r\n\r\n        # Publishers\r\n        self.status_pub = self.create_publisher(String, '/voice_agent/status', 10)\r\n        self.command_pub = self.create_publisher(String, '/voice_agent/commands', 10)\r\n        self.intent_pub = self.create_publisher(String, '/voice_agent/intents', 10)\r\n\r\n        # Action clients\r\n        self.nav_action_client = ActionClient(\r\n            self,\r\n            NavigateToPose,\r\n            'navigate_to_pose'\r\n        )\r\n\r\n        self.manipulation_action_client = ActionClient(\r\n            self,\r\n            FollowJointTrajectory,\r\n            'manipulation_controller/follow_joint_trajectory'\r\n        )\r\n\r\n        # Internal state\r\n        self.command_queue = queue.Queue()\r\n        self.is_listening = False\r\n        self.listening_thread = None\r\n        self.wake_word = \"robot\"\r\n        self.confidence_threshold = 0.7\r\n        self.command_history = []\r\n        self.max_history = 10\r\n\r\n        # Start audio recording thread\r\n        self.start_audio_recording()\r\n\r\n        self.get_logger().info('Whisper Voice Agent initialized')\r\n\r\n    def start_audio_recording(self):\r\n        \"\"\"Start the audio recording thread\"\"\"\r\n        self.is_listening = True\r\n        self.listening_thread = threading.Thread(target=self.audio_recording_loop)\r\n        self.listening_thread.daemon = True\r\n        self.listening_thread.start()\r\n\r\n    def audio_recording_loop(self):\r\n        \"\"\"Main loop for continuous audio recording\"\"\"\r\n        self.get_logger().info('Starting audio recording loop')\r\n\r\n        # Open audio stream\r\n        stream = self.audio.open(\r\n            format=self.audio_format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n\r\n        frames = []\r\n        recording = False\r\n        silence_threshold = 500  # Adjust based on environment\r\n        max_frames = int(self.rate / self.chunk * 5)  # 5 seconds max recording\r\n\r\n        while self.is_listening:\r\n            try:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                audio_data = np.frombuffer(data, dtype=np.int16)\r\n\r\n                # Calculate volume (RMS)\r\n                rms = np.sqrt(np.mean(audio_data**2))\r\n\r\n                if rms > silence_threshold and not recording:\r\n                    # Start recording\r\n                    recording = True\r\n                    frames = [data]\r\n                    self.get_logger().debug('Started recording')\r\n                elif recording:\r\n                    frames.append(data)\r\n\r\n                    # Check if we've reached max duration or returned to silence\r\n                    if len(frames) >= max_frames or rms < silence_threshold:\r\n                        # Stop recording and process audio\r\n                        self.get_logger().debug(f'Stopped recording: {len(frames)} frames')\r\n\r\n                        # Save to temporary WAV file for Whisper processing\r\n                        temp_filename = f\"/tmp/recording_{int(time.time())}.wav\"\r\n                        self.save_audio_to_wav(frames, temp_filename)\r\n\r\n                        # Process with Whisper\r\n                        self.process_audio_with_whisper(temp_filename)\r\n\r\n                        # Clean up temp file\r\n                        import os\r\n                        try:\r\n                            os.remove(temp_filename)\r\n                        except:\r\n                            pass\r\n\r\n                        recording = False\r\n                        frames = []\r\n\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error in audio recording: {e}')\r\n                time.sleep(0.1)\r\n\r\n        # Clean up\r\n        stream.stop_stream()\r\n        stream.close()\r\n\r\n    def save_audio_to_wav(self, frames, filename):\r\n        \"\"\"Save recorded frames to WAV file\"\"\"\r\n        wf = wave.open(filename, 'wb')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(self.audio_format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b''.join(frames))\r\n        wf.close()\r\n\r\n    def process_audio_with_whisper(self, audio_file):\r\n        \"\"\"Process audio file with Whisper ASR\"\"\"\r\n        if not self.whisper_model:\r\n            self.get_logger().error('Whisper model not loaded')\r\n            return\r\n\r\n        try:\r\n            # Transcribe audio\r\n            result = self.whisper_model.transcribe(audio_file)\r\n            text = result['text'].strip().lower()\r\n\r\n            if text:\r\n                self.get_logger().info(f'Whisper recognized: {text}')\r\n\r\n                # Check for wake word and process command\r\n                if self.wake_word in text:\r\n                    command_start = text.find(self.wake_word) + len(self.wake_word)\r\n                    command_text = text[command_start:].strip()\r\n\r\n                    if command_text:\r\n                        # Calculate confidence based on transcription probability\r\n                        avg_logprob = result.get('avg_logprob', -0.5)\r\n                        confidence = max(0.0, min(1.0, (avg_logprob + 2) / 2))  # Normalize to 0-1\r\n\r\n                        self.process_command(command_text, confidence)\r\n\r\n                # Publish transcription status\r\n                status_msg = String()\r\n                status_msg.data = f\"Transcribed: {text} (confidence: {confidence:.2f})\"\r\n                self.status_pub.publish(status_msg)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing audio with Whisper: {e}')\r\n\r\n    def process_command(self, command_text: str, confidence: float = 1.0):\r\n        \"\"\"Process recognized command text with advanced NLU\"\"\"\r\n        try:\r\n            # Parse the command using advanced NLU\r\n            voice_command = self.parse_advanced_command(command_text, confidence)\r\n\r\n            if voice_command and voice_command.confidence >= self.confidence_threshold:\r\n                # Add to command history\r\n                self.command_history.append(voice_command)\r\n                if len(self.command_history) > self.max_history:\r\n                    self.command_history.pop(0)\r\n\r\n                # Execute the command\r\n                success = self.execute_advanced_command(voice_command)\r\n\r\n                if success:\r\n                    self.get_logger().info(f'Command executed: {command_text}')\r\n\r\n                    # Publish command details\r\n                    cmd_msg = String()\r\n                    cmd_data = {\r\n                        'text': voice_command.text,\r\n                        'intent': voice_command.intent,\r\n                        'entities': voice_command.entities,\r\n                        'confidence': voice_command.confidence,\r\n                        'timestamp': voice_command.timestamp\r\n                    }\r\n                    cmd_msg.data = json.dumps(cmd_data)\r\n                    self.command_pub.publish(cmd_msg)\r\n\r\n                    # Publish intent for visualization\r\n                    intent_msg = String()\r\n                    intent_msg.data = f\"{voice_command.intent}: {voice_command.text}\"\r\n                    self.intent_pub.publish(intent_msg)\r\n                else:\r\n                    self.get_logger().warning(f'Command execution failed: {command_text}')\r\n            else:\r\n                self.get_logger().info(f'Command below confidence threshold: {command_text} (confidence: {confidence:.2f})')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing command: {e}')\r\n\r\n    def parse_advanced_command(self, command_text: str, confidence: float) -> Optional[VoiceCommand]:\r\n        \"\"\"Advanced command parsing with context and entity recognition\"\"\"\r\n        # Define comprehensive command patterns\r\n        command_patterns = {\r\n            # Navigation commands\r\n            'navigation': [\r\n                (r'go to the (.+)', ['location']),\r\n                (r'move to (.+)', ['location']),\r\n                (r'navigate to (.+)', ['location']),\r\n                (r'go to (.+)', ['location']),\r\n                (r'bring me to (.+)', ['location']),\r\n                (r'take me to (.+)', ['location'])\r\n            ],\r\n            # Manipulation commands\r\n            'manipulation': [\r\n                (r'pick up the (.+)', ['object']),\r\n                (r'grasp the (.+)', ['object']),\r\n                (r'grab the (.+)', ['object']),\r\n                (r'pick (.+) from (.+)', ['object', 'location']),\r\n                (r'take (.+) and (.+)', ['object', 'action']),\r\n                (r'place (.+) in (.+)', ['object', 'destination'])\r\n            ],\r\n            # Information commands\r\n            'information': [\r\n                (r'what is (.+)', ['query']),\r\n                (r'where is (.+)', ['object']),\r\n                (r'how many (.+)', ['query']),\r\n                (r'tell me about (.+)', ['object']),\r\n                (r'describe (.+)', ['object'])\r\n            ],\r\n            # Control commands\r\n            'control': [\r\n                (r'stop', []),\r\n                (r'wait', []),\r\n                (r'pause', []),\r\n                (r'continue', []),\r\n                (r'resume', []),\r\n                (r'follow me', []),\r\n                (r'come here', []),\r\n                (r'look at (.+)', ['object'])\r\n            ]\r\n        }\r\n\r\n        # Extract intent and entities\r\n        intent = None\r\n        entities = {}\r\n        extracted_entities = []\r\n\r\n        for intent_type, patterns in command_patterns.items():\r\n            for pattern, entity_types in patterns:\r\n                match = re.search(pattern, command_text, re.IGNORECASE)\r\n                if match:\r\n                    intent = intent_type\r\n                    # Extract entities based on pattern groups\r\n                    groups = match.groups()\r\n                    for i, entity_type in enumerate(entity_types):\r\n                        if i < len(groups):\r\n                            entities[entity_type] = groups[i].strip()\r\n                    break\r\n            if intent:\r\n                break\r\n\r\n        # If no specific intent matched, try general classification\r\n        if not intent:\r\n            intent = self.classify_intent_general(command_text)\r\n\r\n        if intent:\r\n            return VoiceCommand(\r\n                text=command_text,\r\n                intent=intent,\r\n                entities=entities,\r\n                confidence=confidence,\r\n                timestamp=time.time(),\r\n                audio_duration=0.0  # Would be calculated in real implementation\r\n            )\r\n\r\n        return None\r\n\r\n    def classify_intent_general(self, text: str) -> str:\r\n        \"\"\"General intent classification when specific patterns don't match\"\"\"\r\n        # Simple keyword-based classification\r\n        text_lower = text.lower()\r\n\r\n        navigation_keywords = ['go', 'move', 'navigate', 'to', 'toward', 'location', 'room']\r\n        manipulation_keywords = ['pick', 'grasp', 'grab', 'take', 'place', 'put', 'object']\r\n        information_keywords = ['what', 'where', 'how', 'tell', 'describe', 'show']\r\n        control_keywords = ['stop', 'wait', 'pause', 'continue', 'follow', 'come']\r\n\r\n        scores = {\r\n            'navigation': sum(1 for keyword in navigation_keywords if keyword in text_lower),\r\n            'manipulation': sum(1 for keyword in manipulation_keywords if keyword in text_lower),\r\n            'information': sum(1 for keyword in information_keywords if keyword in text_lower),\r\n            'control': sum(1 for keyword in control_keywords if keyword in text_lower)\r\n        }\r\n\r\n        # Return intent with highest score, or 'control' as default\r\n        return max(scores, key=scores.get) if max(scores.values()) > 0 else 'control'\r\n\r\n    def execute_advanced_command(self, command: VoiceCommand) -> bool:\r\n        \"\"\"Execute the parsed command with context awareness\"\"\"\r\n        intent = command.intent\r\n        entities = command.entities\r\n\r\n        if intent == 'navigation':\r\n            target_location = entities.get('location', '').lower()\r\n            return self.execute_navigation_command(target_location)\r\n\r\n        elif intent == 'manipulation':\r\n            obj = entities.get('object')\r\n            action = entities.get('action', 'grasp')\r\n            location = entities.get('location')\r\n\r\n            if action == 'grasp' or 'pick' in action or 'grab' in action:\r\n                return self.execute_grasp_command(obj, location)\r\n            elif action == 'place' or 'put' in action:\r\n                destination = entities.get('destination', location)\r\n                return self.execute_place_command(obj, destination)\r\n            else:\r\n                self.get_logger().info(f'Manipulation action: {action} for {obj}')\r\n                return True  # Placeholder\r\n\r\n        elif intent == 'information':\r\n            query = entities.get('query', entities.get('object', ''))\r\n            self.execute_information_command(query)\r\n            return True\r\n\r\n        elif intent == 'control':\r\n            action = command.text.lower().strip()\r\n            return self.execute_control_command(action)\r\n\r\n        else:\r\n            self.get_logger().warning(f'Unknown intent: {intent}')\r\n            return False\r\n\r\n    def execute_navigation_command(self, location: str) -> bool:\r\n        \"\"\"Execute navigation command to specified location\"\"\"\r\n        # In real implementation, this would use semantic mapping\r\n        location_coords = self.get_semantic_location(location)\r\n\r\n        if location_coords:\r\n            return self.navigate_to_pose(location_coords)\r\n        else:\r\n            self.get_logger().warning(f'Unknown location: {location}')\r\n            return False\r\n\r\n    def execute_grasp_command(self, obj: str, location: str = None) -> bool:\r\n        \"\"\"Execute object grasping command\"\"\"\r\n        self.get_logger().info(f'Attempting to grasp {obj}')\r\n        # In real implementation, this would use perception and manipulation systems\r\n        return True\r\n\r\n    def execute_place_command(self, obj: str, destination: str) -> bool:\r\n        \"\"\"Execute object placement command\"\"\"\r\n        self.get_logger().info(f'Attempting to place {obj} at {destination}')\r\n        # In real implementation, this would use navigation and manipulation\r\n        return True\r\n\r\n    def execute_information_command(self, query: str) -> bool:\r\n        \"\"\"Execute information query command\"\"\"\r\n        self.get_logger().info(f'Information query: {query}')\r\n        # In real implementation, this would use perception and knowledge systems\r\n        return True\r\n\r\n    def execute_control_command(self, action: str) -> bool:\r\n        \"\"\"Execute control command\"\"\"\r\n        if 'stop' in action:\r\n            self.get_logger().info('Stop command received')\r\n            # In real implementation, stop all current actions\r\n        elif 'follow' in action:\r\n            self.get_logger().info('Follow command received')\r\n            # In real implementation, start following behavior\r\n        elif 'look' in action:\r\n            obj = action.replace('look at', '').strip()\r\n            self.get_logger().info(f'Look at command: {obj}')\r\n            # In real implementation, orient camera toward object\r\n        else:\r\n            self.get_logger().info(f'Control action: {action}')\r\n\r\n        return True\r\n\r\n    def get_semantic_location(self, location_name: str) -> Optional[tuple]:\r\n        \"\"\"Get coordinates for semantic location names\"\"\"\r\n        # In real implementation, this would use a semantic map\r\n        semantic_map = {\r\n            'kitchen': (3.0, 2.0, 0.0),\r\n            'bedroom': (-2.0, 1.0, 0.0),\r\n            'living room': (0.0, 0.0, 0.0),\r\n            'office': (1.0, -2.0, 0.0),\r\n            'bathroom': (-1.0, -1.0, 0.0),\r\n            'dining room': (2.0, -1.0, 0.0)\r\n        }\r\n\r\n        # Handle variations in location names\r\n        for key, coords in semantic_map.items():\r\n            if location_name in key or key in location_name:\r\n                return coords\r\n\r\n        return None\r\n\r\n    def navigate_to_pose(self, coordinates: tuple) -> bool:\r\n        \"\"\"Send navigation goal to the robot\"\"\"\r\n        try:\r\n            # Wait for action server\r\n            if not self.nav_action_client.wait_for_server(timeout_sec=1.0):\r\n                self.get_logger().error('Navigation action server not available')\r\n                return False\r\n\r\n            # Create navigation goal\r\n            goal_msg = NavigateToPose.Goal()\r\n            goal_msg.pose.header.frame_id = 'map'\r\n            goal_msg.pose.pose.position.x = coordinates[0]\r\n            goal_msg.pose.pose.position.y = coordinates[1]\r\n            goal_msg.pose.pose.position.z = coordinates[2]\r\n            goal_msg.pose.pose.orientation.w = 1.0  # Default orientation\r\n\r\n            # Send goal asynchronously\r\n            goal_future = self.nav_action_client.send_goal_async(goal_msg)\r\n\r\n            # Wait for result\r\n            rclpy.spin_until_future_complete(self, goal_future)\r\n\r\n            goal_handle = goal_future.result()\r\n            if not goal_handle.accepted:\r\n                self.get_logger().error('Navigation goal was rejected')\r\n                return False\r\n\r\n            result_future = goal_handle.get_result_async()\r\n            rclpy.spin_until_future_complete(self, result_future)\r\n\r\n            result = result_future.result().result\r\n            status = result.error_code\r\n\r\n            return status == result.SUCCESS\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in navigation: {e}')\r\n            return False\r\n\r\n    def cleanup(self):\r\n        \"\"\"Clean up resources\"\"\"\r\n        self.is_listening = False\r\n        if self.listening_thread:\r\n            self.listening_thread.join(timeout=1.0)\r\n\r\n        self.audio.terminate()\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    # Create Whisper voice agent\r\n    voice_agent = WhisperVoiceAgent()\r\n\r\n    try:\r\n        voice_agent.get_logger().info('Whisper Voice Agent running...')\r\n        rclpy.spin(voice_agent)\r\n    except KeyboardInterrupt:\r\n        voice_agent.get_logger().info('Shutting down Whisper Voice Agent')\r\n    finally:\r\n        voice_agent.cleanup()\r\n        voice_agent.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"To run this advanced voice agent:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Save it as ",(0,i.jsx)(n.code,{children:"whisper_voice_agent.py"})]}),"\n",(0,i.jsxs)(n.li,{children:["Install required dependencies: ",(0,i.jsx)(n.code,{children:"pip install openai-whisper torch torchaudio transformers pyaudio"})]}),"\n",(0,i.jsxs)(n.li,{children:["Run: ",(0,i.jsx)(n.code,{children:"ros2 run <package_name> whisper_voice_agent"})]}),"\n",(0,i.jsx)(n.li,{children:'Speak commands like "robot go to kitchen" or "robot pick up the red ball"'}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,i.jsx)(n.p,{children:"Create a complete voice-to-action system that includes:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Wake word detection using a lightweight model (like Porcupine)"}),"\n",(0,i.jsx)(n.li,{children:"Context-aware command execution that remembers previous commands"}),"\n",(0,i.jsx)(n.li,{children:"Multi-turn conversation capabilities for complex task breakdown"}),"\n",(0,i.jsx)(n.li,{children:"Voice feedback system that confirms actions and asks for clarification"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hint:"})," Use a combination of lightweight wake word detection, context management with a dialogue state tracker, and a text-to-speech system for feedback."]}),"\n",(0,i.jsx)(n.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,i.jsx)(n.p,{children:"To verify that your voice-to-action system is working correctly:"}),"\n",(0,i.jsx)(n.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"When starting the system: Model loading and initialization messages"}),"\n",(0,i.jsx)(n.li,{children:"When recognizing speech: Transcribed text and confidence scores"}),"\n",(0,i.jsx)(n.li,{children:"When executing commands: Action execution status and results"}),"\n",(0,i.jsx)(n.li,{children:"When processing errors: Error messages and fallback behaviors"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robot responds to voice commands in Gazebo/Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"RViz2 shows navigation goals and action execution"}),"\n",(0,i.jsx)(n.li,{children:"Audio input is processed in real-time"}),"\n",(0,i.jsx)(n.li,{children:"System state is updated based on voice commands"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Basic voice command processor with speech recognition"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Advanced Whisper-based voice agent with NLP"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integration with ROS 2 action servers for navigation/manipulation"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Context-aware command execution with entity recognition"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Complete voice-to-action system with feedback (Try Yourself task)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multi-turn conversation capabilities implemented"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered voice-to-action systems that convert natural language commands into robotic actions. You learned about the key components of voice processing including speech recognition, natural language understanding, and command execution. The examples demonstrated implementing both basic and advanced voice processing systems that can understand and execute complex robotic commands through voice interaction."}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Radford, A., et al. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. ",(0,i.jsx)(n.em,{children:"arXiv preprint arXiv:2212.04356"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["OpenAI. (2023). ",(0,i.jsx)(n.em,{children:"Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"}),". Technical Report."]}),"\n",(0,i.jsxs)(n.li,{children:["Young, S., et al. (2013). The dialog state tracking challenge. ",(0,i.jsx)(n.em,{children:"Proceedings of the SIGDIAL 2013 Conference"}),", 404-413."]}),"\n",(0,i.jsxs)(n.li,{children:["Hermann, K. M., et al. (2017). Grounded Language Learning in a Simulated 3D World. ",(0,i.jsx)(n.em,{children:"arXiv preprint arXiv:1706.06551"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Misra, D., et al. (2022). VoiceLM: Efficient Voice-First Language Modeling. ",(0,i.jsx)(n.em,{children:"arXiv preprint arXiv:2208.02729"}),"."]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);