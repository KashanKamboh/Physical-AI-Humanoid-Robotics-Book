"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[5601],{6844:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-2/chapter-6","title":"Chapter 6: Sensors Simulation","description":"Learning Outcomes","source":"@site/docs/module-2/chapter-6.md","sourceDirName":"module-2","slug":"/module-2/chapter-6","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2/chapter-6","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-2/chapter-6.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Setting Up Gazebo","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2/chapter-5"},"next":{"title":"Chapter 7: Unity for Robotics","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2/chapter-7"}}');var s=r(4848),a=r(8453);const t={sidebar_position:3},o="Chapter 6: Sensors Simulation",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"Sensor Simulation Fundamentals",id:"sensor-simulation-fundamentals",level:3},{value:"Common Sensor Types in Robotics",id:"common-sensor-types-in-robotics",level:3},{value:"Sensor Integration with ROS 2",id:"sensor-integration-with-ros-2",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function c(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-6-sensors-simulation",children:"Chapter 6: Sensors Simulation"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement realistic sensor models in Gazebo simulation"}),"\n",(0,s.jsx)(e.li,{children:"Configure sensor noise and drift characteristics"}),"\n",(0,s.jsx)(e.li,{children:"Simulate various sensor types (camera, LiDAR, IMU, etc.)"}),"\n",(0,s.jsx)(e.li,{children:"Integrate simulated sensors with ROS 2 nodes"}),"\n",(0,s.jsx)(e.li,{children:"Validate sensor simulation against real-world characteristics"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,s.jsx)(e.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Gazebo Garden (or newer version)"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","ROS 2 Humble Hawksbill with Gazebo plugins"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Python 3.8+ with pip"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed Chapter 5 content"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Understanding of Gazebo world and model files"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Basic knowledge of ROS 2 sensor message types"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Familiarity with URDF/SDF robot models"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed robot model from Chapter 5"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Access to sensor specifications and datasheets"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,s.jsx)(e.h3,{id:"sensor-simulation-fundamentals",children:"Sensor Simulation Fundamentals"}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation in Gazebo aims to replicate the behavior of real-world sensors with appropriate noise, latency, and physical limitations. Key aspects include:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Realistic Noise Modeling"}),": Simulated sensors must include realistic noise patterns that match real hardware, including:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Gaussian noise for measurement uncertainty"}),"\n",(0,s.jsx)(e.li,{children:"Bias and drift for long-term inaccuracies"}),"\n",(0,s.jsx)(e.li,{children:"Quantization effects for digital sensors"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Physics-Based Rendering"}),": For vision sensors, the simulation must account for:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Lighting conditions and shadows"}),"\n",(0,s.jsx)(e.li,{children:"Material properties and reflectance"}),"\n",(0,s.jsx)(e.li,{children:"Lens effects and distortion"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Temporal Characteristics"}),": Sensors have timing properties such as:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Update rates and latency"}),"\n",(0,s.jsx)(e.li,{children:"Integration times for cameras"}),"\n",(0,s.jsx)(e.li,{children:"Sampling patterns for various sensors"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"common-sensor-types-in-robotics",children:"Common Sensor Types in Robotics"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Camera Sensors"}),": Provide visual information with RGB, depth, or both. Key parameters include:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Resolution and field of view"}),"\n",(0,s.jsx)(e.li,{children:"Frame rate and exposure settings"}),"\n",(0,s.jsx)(e.li,{children:"Noise models and distortion coefficients"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"LiDAR Sensors"}),": Provide 3D point cloud data. Key parameters include:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Range and resolution"}),"\n",(0,s.jsx)(e.li,{children:"Field of view (vertical and horizontal)"}),"\n",(0,s.jsx)(e.li,{children:"Noise characteristics and beam divergence"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"IMU Sensors"}),": Provide inertial measurements. Key parameters include:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Accelerometer and gyroscope noise"}),"\n",(0,s.jsx)(e.li,{children:"Bias and drift characteristics"}),"\n",(0,s.jsx)(e.li,{children:"Sampling rate and latency"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"GPS Sensors"}),": Provide global position. Key parameters include:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Accuracy and update rate"}),"\n",(0,s.jsx)(e.li,{children:"Multipath effects and signal blocking"}),"\n",(0,s.jsx)(e.li,{children:"Environmental factors affecting performance"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"sensor-integration-with-ros-2",children:"Sensor Integration with ROS 2"}),"\n",(0,s.jsxs)(e.p,{children:["Gazebo sensors connect to ROS 2 through the ",(0,s.jsx)(e.code,{children:"gazebo_ros"})," package, which provides plugins that:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Publish sensor data to ROS topics"}),"\n",(0,s.jsx)(e.li,{children:"Subscribe to control commands"}),"\n",(0,s.jsx)(e.li,{children:"Handle parameter configuration"}),"\n",(0,s.jsx)(e.li,{children:"Provide TF transforms for sensor frames"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-mermaid",children:"graph TD\r\n    A[Sensor Simulation Pipeline] --\x3e B[Physics Engine]\r\n    A --\x3e C[Sensor Model]\r\n    A --\x3e D[Noise Generation]\r\n    A --\x3e E[ROS Integration]\r\n\r\n    B --\x3e B1[Collision Detection]\r\n    B --\x3e B2[Lighting Calculation]\r\n    B --\x3e B3[Material Properties]\r\n\r\n    C --\x3e C1[Camera Model]\r\n    C --\x3e C2[Lidar Model]\r\n    C --\x3e C3[IMU Model]\r\n    C --\x3e C4[Other Sensors]\r\n\r\n    D --\x3e D1[Gaussian Noise]\r\n    D --\x3e D2[Bias Drift]\r\n    D --\x3e D3[Quantization]\r\n\r\n    E --\x3e E1[ROS Publishers]\r\n    E --\x3e E2[TF Transforms]\r\n    E --\x3e E3[Parameter Server]\r\n\r\n    B1 --\x3e C\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    E1 --\x3e A\n"})}),"\n",(0,s.jsx)(e.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,s.jsx)(e.p,{children:"Let's create a robot model with multiple sensors and realistic noise characteristics:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- robot_with_sensors.sdf --\x3e\r\n<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <model name="robot_with_sensors">\r\n    \x3c!-- Robot base --\x3e\r\n    <link name="base_link">\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>10</mass>\r\n        <inertia>\r\n          <ixx>0.4</ixx>\r\n          <ixy>0</ixy>\r\n          <ixz>0</ixz>\r\n          <iyy>0.4</iyy>\r\n          <iyz>0</iyz>\r\n          <izz>0.2</izz>\r\n        </inertia>\r\n      </inertial>\r\n\r\n      <collision name="collision">\r\n        <geometry>\r\n          <cylinder>\r\n            <radius>0.3</radius>\r\n            <length>0.2</length>\r\n          </cylinder>\r\n        </geometry>\r\n      </collision>\r\n\r\n      <visual name="visual">\r\n        <geometry>\r\n          <cylinder>\r\n            <radius>0.3</radius>\r\n            <length>0.2</length>\r\n          </cylinder>\r\n        </geometry>\r\n        <material>\r\n          <ambient>0.8 0.2 0.2 1</ambient>\r\n          <diffuse>0.8 0.2 0.2 1</diffuse>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    \x3c!-- RGB Camera --\x3e\r\n    <link name="camera_link">\r\n      <pose>0.2 0 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>0.01</mass>\r\n        <inertia>\r\n          <ixx>0.0001</ixx>\r\n          <ixy>0</ixy>\r\n          <ixz>0</ixz>\r\n          <iyy>0.0001</iyy>\r\n          <iyz>0</iyz>\r\n          <izz>0.0001</izz>\r\n        </inertia>\r\n      </inertial>\r\n\r\n      <collision name="camera_collision">\r\n        <geometry>\r\n          <box>\r\n            <size>0.05 0.05 0.05</size>\r\n          </box>\r\n        </geometry>\r\n      </collision>\r\n\r\n      <visual name="camera_visual">\r\n        <geometry>\r\n          <box>\r\n            <size>0.05 0.05 0.05</size>\r\n          </box>\r\n        </geometry>\r\n        <material>\r\n          <ambient>0.5 0.5 0.5 1</ambient>\r\n          <diffuse>0.5 0.5 0.5 1</diffuse>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <joint name="camera_joint" type="fixed">\r\n      <parent>base_link</parent>\r\n      <child>camera_link</child>\r\n      <pose>0.2 0 0.1 0 0 0</pose>\r\n    </joint>\r\n\r\n    \x3c!-- RGB Camera Sensor --\x3e\r\n    <sensor name="camera" type="camera">\r\n      <pose>0.2 0 0.1 0 0 0</pose>\r\n      <camera>\r\n        <horizontal_fov>1.047</horizontal_fov>\r\n        <image>\r\n          <width>640</width>\r\n          <height>480</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.1</near>\r\n          <far>10</far>\r\n        </clip>\r\n        <noise>\r\n          <type>gaussian</type>\r\n          <mean>0.0</mean>\r\n          <stddev>0.007</stddev>\r\n        </noise>\r\n      </camera>\r\n      <always_on>1</always_on>\r\n      <update_rate>30</update_rate>\r\n      <visualize>true</visualize>\r\n    </sensor>\r\n\r\n    \x3c!-- Depth Camera --\x3e\r\n    <link name="depth_camera_link">\r\n      <pose>0.2 0.1 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>0.01</mass>\r\n        <inertia>\r\n          <ixx>0.0001</ixx>\r\n          <ixy>0</ixy>\r\n          <ixz>0</ixz>\r\n          <iyy>0.0001</iyy>\r\n          <iyz>0</iyz>\r\n          <izz>0.0001</izz>\r\n        </inertia>\r\n      </inertial>\r\n\r\n      <collision name="depth_camera_collision">\r\n        <geometry>\r\n          <box>\r\n            <size>0.05 0.05 0.05</size>\r\n          </box>\r\n        </geometry>\r\n      </collision>\r\n\r\n      <visual name="depth_camera_visual">\r\n        <geometry>\r\n          <box>\r\n            <size>0.05 0.05 0.05</size>\r\n          </box>\r\n        </geometry>\r\n        <material>\r\n          <ambient>0.3 0.3 0.3 1</ambient>\r\n          <diffuse>0.3 0.3 0.3 1</diffuse>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <joint name="depth_camera_joint" type="fixed">\r\n      <parent>base_link</parent>\r\n      <child>depth_camera_link</child>\r\n      <pose>0.2 0.1 0.1 0 0 0</pose>\r\n    </joint>\r\n\r\n    \x3c!-- Depth Camera Sensor --\x3e\r\n    <sensor name="depth_camera" type="depth">\r\n      <pose>0.2 0.1 0.1 0 0 0</pose>\r\n      <camera>\r\n        <horizontal_fov>1.047</horizontal_fov>\r\n        <image>\r\n          <width>320</width>\r\n          <height>240</height>\r\n        </image>\r\n        <clip>\r\n          <near>0.1</near>\r\n          <far>8</far>\r\n        </clip>\r\n        <noise>\r\n          <type>gaussian</type>\r\n          <mean>0.0</mean>\r\n          <stddev>0.01</stddev>\r\n        </noise>\r\n      </camera>\r\n      <always_on>1</always_on>\r\n      <update_rate>15</update_rate>\r\n      <visualize>true</visualize>\r\n    </sensor>\r\n\r\n    \x3c!-- IMU Sensor --\x3e\r\n    <link name="imu_link">\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>0.001</mass>\r\n        <inertia>\r\n          <ixx>0.000001</ixx>\r\n          <ixy>0</ixy>\r\n          <ixz>0</ixz>\r\n          <iyy>0.000001</iyy>\r\n          <iyz>0</iyz>\r\n          <izz>0.000001</izz>\r\n        </inertia>\r\n      </inertial>\r\n    </link>\r\n\r\n    <joint name="imu_joint" type="fixed">\r\n      <parent>base_link</parent>\r\n      <child>imu_link</child>\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n    </joint>\r\n\r\n    \x3c!-- IMU Sensor --\x3e\r\n    <sensor name="imu_sensor" type="imu">\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n      <imu>\r\n        <angular_velocity>\r\n          <x>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.001</stddev>\r\n            </noise>\r\n          </x>\r\n          <y>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.001</stddev>\r\n            </noise>\r\n          </y>\r\n          <z>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.001</stddev>\r\n            </noise>\r\n          </z>\r\n        </angular_velocity>\r\n        <linear_acceleration>\r\n          <x>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.017</stddev>\r\n            </noise>\r\n          </x>\r\n          <y>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.017</stddev>\r\n            </noise>\r\n          </y>\r\n          <z>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.017</stddev>\r\n            </noise>\r\n          </z>\r\n        </linear_acceleration>\r\n      </imu>\r\n      <always_on>1</always_on>\r\n      <update_rate>100</update_rate>\r\n      <visualize>false</visualize>\r\n    </sensor>\r\n\r\n    \x3c!-- 360-degree LiDAR --\x3e\r\n    <link name="lidar_link">\r\n      <pose>0 0 0.25 0 0 0</pose>\r\n      <inertial>\r\n        <mass>0.1</mass>\r\n        <inertia>\r\n          <ixx>0.001</ixx>\r\n          <ixy>0</ixy>\r\n          <ixz>0</ixz>\r\n          <iyy>0.001</iyy>\r\n          <iyz>0</iyz>\r\n          <izz>0.001</izz>\r\n        </inertia>\r\n      </inertial>\r\n\r\n      <collision name="lidar_collision">\r\n        <geometry>\r\n          <cylinder>\r\n            <radius>0.05</radius>\r\n            <length>0.1</length>\r\n          </cylinder>\r\n        </geometry>\r\n      </collision>\r\n\r\n      <visual name="lidar_visual">\r\n        <geometry>\r\n          <cylinder>\r\n            <radius>0.05</radius>\r\n            <length>0.1</length>\r\n          </cylinder>\r\n        </geometry>\r\n        <material>\r\n          <ambient>0.7 0.7 0.7 1</ambient>\r\n          <diffuse>0.7 0.7 0.7 1</diffuse>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <joint name="lidar_joint" type="fixed">\r\n      <parent>base_link</parent>\r\n      <child>lidar_link</child>\r\n      <pose>0 0 0.25 0 0 0</pose>\r\n    </joint>\r\n\r\n    \x3c!-- LiDAR Sensor --\x3e\r\n    <sensor name="lidar" type="ray">\r\n      <pose>0 0 0.25 0 0 0</pose>\r\n      <ray>\r\n        <scan>\r\n          <horizontal>\r\n            <samples>360</samples>\r\n            <resolution>1</resolution>\r\n            <min_angle>-3.14159</min_angle>\r\n            <max_angle>3.14159</max_angle>\r\n          </horizontal>\r\n        </scan>\r\n        <range>\r\n          <min>0.1</min>\r\n          <max>10</max>\r\n          <resolution>0.01</resolution>\r\n        </range>\r\n        <noise>\r\n          <type>gaussian</type>\r\n          <mean>0.0</mean>\r\n          <stddev>0.01</stddev>\r\n        </noise>\r\n      </ray>\r\n      <always_on>1</always_on>\r\n      <update_rate>10</update_rate>\r\n      <visualize>true</visualize>\r\n    </sensor>\r\n  </model>\r\n</sdf>\n'})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"To run this simulation:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["Save it as ",(0,s.jsx)(e.code,{children:"robot_with_sensors.sdf"})]}),"\n",(0,s.jsxs)(e.li,{children:["Launch Gazebo: ",(0,s.jsx)(e.code,{children:"gz sim robot_with_sensors.sdf"})]}),"\n",(0,s.jsxs)(e.li,{children:["Or use ROS 2: ",(0,s.jsx)(e.code,{children:"ros2 launch gazebo_ros spawn_entity.launch.py entity:=robot_with_sensors file:=/path/to/robot_with_sensors.sdf"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,s.jsx)(e.p,{children:"Now let's create a ROS 2 node that processes and validates the sensor data from our simulated robot:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# sensor_processor.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, Imu, LaserScan\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\n\r\nclass SensorProcessor(Node):\r\n    \"\"\"\r\n    A node that processes and validates sensor data from the simulated robot.\r\n    This demonstrates how to work with different sensor types and validate their outputs.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('sensor_processor')\r\n\r\n        # Initialize CvBridge for image processing\r\n        self.bridge = CvBridge()\r\n\r\n        # Create subscribers for different sensor types\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/robot_with_sensors/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/robot_with_sensors/depth_camera/depth/image_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_sub = self.create_subscription(\r\n            Imu,\r\n            '/robot_with_sensors/imu_sensor/imu',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/robot_with_sensors/lidar/scan',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n        # Internal state for validation\r\n        self.image_count = 0\r\n        self.depth_count = 0\r\n        self.imu_count = 0\r\n        self.lidar_count = 0\r\n\r\n        # Statistics for validation\r\n        self.imu_linear_acceleration_history = []\r\n        self.lidar_range_history = []\r\n\r\n        self.get_logger().info('Sensor Processor initialized and waiting for data...')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process RGB camera data\"\"\"\r\n        self.image_count += 1\r\n\r\n        try:\r\n            # Convert ROS Image to OpenCV image\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n\r\n            # Basic validation: check image dimensions\r\n            height, width, channels = cv_image.shape\r\n            if width != 640 or height != 480 or channels != 3:\r\n                self.get_logger().warn(f'Unexpected image dimensions: {width}x{height}x{channels}')\r\n\r\n            # Calculate basic statistics\r\n            mean_brightness = np.mean(cv_image)\r\n            std_brightness = np.std(cv_image)\r\n\r\n            if self.image_count % 30 == 0:  # Log every 30 images\r\n                self.get_logger().info(\r\n                    f'Image #{self.image_count}: '\r\n                    f'Size: {width}x{height}, '\r\n                    f'Mean brightness: {mean_brightness:.2f}, '\r\n                    f'Std: {std_brightness:.2f}'\r\n                )\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n\r\n    def depth_callback(self, msg):\r\n        \"\"\"Process depth camera data\"\"\"\r\n        self.depth_count += 1\r\n\r\n        try:\r\n            # Convert ROS Image to OpenCV image (depth format)\r\n            cv_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\r\n\r\n            # Validate depth values (should be in reasonable range)\r\n            valid_depths = cv_depth[np.isfinite(cv_depth) & (cv_depth > 0) & (cv_depth < 10)]\r\n\r\n            if len(valid_depths) > 0:\r\n                mean_depth = np.mean(valid_depths)\r\n                min_depth = np.min(valid_depths)\r\n                max_depth = np.max(valid_depths)\r\n\r\n                if self.depth_count % 15 == 0:  # Log every 15 depth images\r\n                    self.get_logger().info(\r\n                        f'Depth #{self.depth_count}: '\r\n                        f'Mean: {mean_depth:.2f}m, '\r\n                        f'Min: {min_depth:.2f}m, '\r\n                        f'Max: {max_depth:.2f}m, '\r\n                        f'Valid pixels: {len(valid_depths)}'\r\n                    )\r\n            else:\r\n                self.get_logger().warn('No valid depth values in image')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing depth: {e}')\r\n\r\n    def imu_callback(self, msg):\r\n        \"\"\"Process IMU data and validate against physical constraints\"\"\"\r\n        self.imu_count += 1\r\n\r\n        # Extract linear acceleration\r\n        ax = msg.linear_acceleration.x\r\n        ay = msg.linear_acceleration.y\r\n        az = msg.linear_acceleration.z\r\n\r\n        # Calculate total acceleration (should be around 9.8 m/s\xb2 when stationary)\r\n        total_acc = np.sqrt(ax**2 + ay**2 + az**2)\r\n\r\n        # Store for statistical analysis\r\n        self.imu_linear_acceleration_history.append(total_acc)\r\n        if len(self.imu_linear_acceleration_history) > 100:\r\n            self.imu_linear_acceleration_history.pop(0)\r\n\r\n        # Check if acceleration is reasonable\r\n        if abs(total_acc - 9.8) > 2.0:  # More than 2 m/s\xb2 deviation\r\n            self.get_logger().warn(\r\n                f'Unusual acceleration: {total_acc:.2f} m/s\xb2 '\r\n                f'(x: {ax:.2f}, y: {ay:.2f}, z: {az:.2f})'\r\n            )\r\n\r\n        if self.imu_count % 100 == 0:  # Log every 100 IMU readings\r\n            if len(self.imu_linear_acceleration_history) > 0:\r\n                avg_acc = np.mean(self.imu_linear_acceleration_history)\r\n                std_acc = np.std(self.imu_linear_acceleration_history)\r\n                self.get_logger().info(\r\n                    f'IMU #{self.imu_count}: '\r\n                    f'Current: {total_acc:.2f} m/s\xb2, '\r\n                    f'Avg: {avg_acc:.2f} m/s\xb2, '\r\n                    f'Std: {std_acc:.2f}'\r\n                )\r\n\r\n    def lidar_callback(self, msg):\r\n        \"\"\"Process LiDAR data and validate against physical constraints\"\"\"\r\n        self.lidar_count += 1\r\n\r\n        # Convert ranges to numpy array for analysis\r\n        ranges = np.array(msg.ranges)\r\n\r\n        # Filter out invalid ranges (inf, nan) and get statistics\r\n        valid_ranges = ranges[np.isfinite(ranges) & (ranges > 0)]\r\n\r\n        if len(valid_ranges) > 0:\r\n            min_range = np.min(valid_ranges)\r\n            max_range = np.max(valid_ranges)\r\n            mean_range = np.mean(valid_ranges)\r\n\r\n            # Store for statistical analysis\r\n            self.lidar_range_history.append(mean_range)\r\n            if len(self.lidar_range_history) > 50:\r\n                self.lidar_range_history.pop(0)\r\n\r\n            # Check for reasonable values\r\n            if min_range < msg.range_min or max_range > msg.range_max:\r\n                self.get_logger().warn(\r\n                    f'Lidar range violation: '\r\n                    f'min={min_range:.2f}, max={max_range:.2f}, '\r\n                    f'limits=({msg.range_min:.2f}, {msg.range_max:.2f})'\r\n                )\r\n\r\n            if self.lidar_count % 50 == 0:  # Log every 50 LiDAR readings\r\n                self.get_logger().info(\r\n                    f'Lidar #{self.lidar_count}: '\r\n                    f'Valid beams: {len(valid_ranges)}/{len(ranges)}, '\r\n                    f'Min: {min_range:.2f}m, '\r\n                    f'Max: {max_range:.2f}m, '\r\n                    f'Mean: {mean_range:.2f}m'\r\n                )\r\n        else:\r\n            self.get_logger().warn('No valid ranges in LiDAR scan')\r\n\r\n    def get_sensor_status(self):\r\n        \"\"\"Return a summary of sensor activity\"\"\"\r\n        return {\r\n            'images_processed': self.image_count,\r\n            'depth_images_processed': self.depth_count,\r\n            'imu_readings': self.imu_count,\r\n            'lidar_scans': self.lidar_count\r\n        }\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    processor = SensorProcessor()\r\n\r\n    try:\r\n        rclpy.spin(processor)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        status = processor.get_sensor_status()\r\n        processor.get_logger().info(f'Final sensor status: {status}')\r\n        processor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"To run this code:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["Save it as ",(0,s.jsx)(e.code,{children:"sensor_processor.py"})]}),"\n",(0,s.jsx)(e.li,{children:"Make sure your robot simulation is running with the sensor model"}),"\n",(0,s.jsxs)(e.li,{children:["Run: ",(0,s.jsx)(e.code,{children:"ros2 run <package_name> sensor_processor"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,s.jsx)(e.p,{children:"Create a sensor fusion node that combines data from multiple sensors to improve perception accuracy. Your node should:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Subscribe to both camera and LiDAR data"}),"\n",(0,s.jsx)(e.li,{children:"Implement a simple method to correlate 2D image features with 3D LiDAR points"}),"\n",(0,s.jsx)(e.li,{children:"Publish a fused perception result showing detected objects with both visual and distance information"}),"\n",(0,s.jsx)(e.li,{children:"Include validation to ensure consistency between sensor readings"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Hint:"})," Use geometric relationships between the sensors (known from your robot model) to project LiDAR points into the camera image plane."]}),"\n",(0,s.jsx)(e.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,s.jsx)(e.p,{children:"To verify that your sensor simulation is working correctly:"}),"\n",(0,s.jsx)(e.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"When running the sensor processor: Continuous logging of sensor statistics and validation results"}),"\n",(0,s.jsx)(e.li,{children:"When sensor values are out of range: Warning messages indicating potential issues"}),"\n",(0,s.jsx)(e.li,{children:"When sensors are operating normally: Regular status updates showing data rates and values"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"In Gazebo, you should see visual representations of sensor fields of view"}),"\n",(0,s.jsx)(e.li,{children:"Sensor data should be published to ROS topics at the expected rates"}),"\n",(0,s.jsx)(e.li,{children:"Validation node should show realistic sensor values within expected ranges"}),"\n",(0,s.jsx)(e.li,{children:"System monitoring tools should show proper sensor data flow"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Robot model created with multiple sensor types"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Sensor noise and characteristics properly configured"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","ROS 2 node created to process sensor data"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Sensor validation and monitoring implemented"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Sensor fusion node with multi-sensor integration (Try Yourself task)"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"This chapter covered the implementation of realistic sensor models in Gazebo simulation and their integration with ROS 2. You learned about different sensor types (camera, depth, IMU, LiDAR), how to configure their noise characteristics, and how to validate sensor data in ROS 2 nodes. The examples demonstrated creating complex sensor models and processing their outputs, providing a foundation for advanced perception systems."}),"\n",(0,s.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Source 014: Research on voxel-based representations for 3D scene understanding"}),"\n",(0,s.jsx)(e.li,{children:"Source 015: Study on depth map processing and IMU drift modeling"}),"\n",(0,s.jsxs)(e.li,{children:["Wellman, M., Jia, Z., Zhu, Y., & Savva, M. (2018). Learning to parse indoor scenes with weak semantic localization. ",(0,s.jsx)(e.em,{children:"Proceedings of the European Conference on Computer Vision"}),", 460-475."]}),"\n",(0,s.jsx)(e.li,{children:"Source 016: Analysis of real-time collision handling systems in physics-based simulation"}),"\n",(0,s.jsxs)(e.li,{children:["ROS 2 Documentation Team. (2023). ",(0,s.jsx)(e.em,{children:"Sensor Integration with Gazebo"}),". Retrieved from ",(0,s.jsx)(e.a,{href:"https://classic.gazebosim.org/tutorials?tut=ros2_overview",children:"https://classic.gazebosim.org/tutorials?tut=ros2_overview"})]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>t,x:()=>o});var i=r(6540);const s={},a=i.createContext(s);function t(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);