"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[2784],{2570:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"module-4/intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview","source":"@site/docs/module-4/intro.md","sourceDirName":"module-4","slug":"/module-4/intro","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-4/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Research Source Register - Module 3","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3/research-source"},"next":{"title":"Chapter 11: Voice-to-Action Agents","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4/chapter-11"}}');var t=i(4848),s=i(8453);const l={sidebar_position:1},r="Module 4: Vision-Language-Action (VLA)",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Tools and Technologies",id:"tools-and-technologies",level:2},{value:"What You Will Be Able to Do at the End",id:"what-you-will-be-able-to-do-at-the-end",level:2},{value:"Expected Completion Time",id:"expected-completion-time",level:2},{value:"Capstone Connection",id:"capstone-connection",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Module 4 focuses on the integration of vision, language, and action systems that enable natural human-robot interaction. Students will learn to build systems that can understand natural language commands, perceive visual scenes, and execute complex robotic actions in response to human instructions."}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this module, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate speech recognition with robotic action planning"}),"\n",(0,t.jsx)(n.li,{children:"Implement cognitive task planning for complex behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Create vision-language systems for scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"Develop end-to-end voice-commanded robot operation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(n.p,{children:"This module consists of 3 chapters that build toward the final capstone system:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 11"}),": Voice-to-Action Agents"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 12"}),": Cognitive Task Planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 13"}),": Capstone Execution"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, students should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed all previous modules (ROS 2, simulation, AI-brain)"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of natural language processing concepts"}),"\n",(0,t.jsx)(n.li,{children:"Experience with multimodal AI systems"}),"\n",(0,t.jsx)(n.li,{children:"Completed perception and navigation from Module 3"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tools-and-technologies",children:"Tools and Technologies"}),"\n",(0,t.jsx)(n.p,{children:"This module utilizes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI Whisper"})," (speech recognition)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Large Language Models"})," (GPT or open-source alternatives)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2"})," (action execution)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Computer Vision libraries"})," (scene understanding)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task planning frameworks"})," (behavior trees or PDDL)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"what-you-will-be-able-to-do-at-the-end",children:"What You Will Be Able to Do at the End"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will have built a complete VLA system that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Responds to natural language commands"}),"\n",(0,t.jsx)(n.li,{children:"Perceives and understands visual scenes"}),"\n",(0,t.jsx)(n.li,{children:"Plans and executes complex robotic tasks"}),"\n",(0,t.jsx)(n.li,{children:"Integrates all previous module capabilities"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"expected-completion-time",children:"Expected Completion Time"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Estimated Duration"}),": 3-4 weeks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Weekly Commitment"}),": 6-8 hours"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Total Effort"}),": 24-32 hours"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-connection",children:"Capstone Connection"}),"\n",(0,t.jsx)(n.p,{children:'This module culminates in the complete capstone project where all system components work together to execute the "Autonomous Humanoid" scenario: VOICE \u27f6 PLAN \u27f6 NAVIGATE \u27f6 RECOGNIZE OBJECT \u27f6 MANIPULATE.'})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function l(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);