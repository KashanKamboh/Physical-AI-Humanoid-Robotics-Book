"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[8233],{6690:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-3/chapter-8","title":"Chapter 8: Isaac Simulation Pipeline","description":"Learning Outcomes","source":"@site/docs/module-3/chapter-8.md","sourceDirName":"module-3","slug":"/module-3/chapter-8","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-3/chapter-8","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-3/chapter-8.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: AI-Robot Brain (NVIDIA Isaac)","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-3/intro"},"next":{"title":"Chapter 9: Autonomous Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-3/chapter-9"}}');var t=r(4848),a=r(8453);const s={sidebar_position:2},o="Chapter 8: Isaac Simulation Pipeline",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"What is NVIDIA Isaac Sim?",id:"what-is-nvidia-isaac-sim",level:3},{value:"Key Isaac Sim Components",id:"key-isaac-sim-components",level:3},{value:"Isaac Sim Pipeline Architecture",id:"isaac-sim-pipeline-architecture",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function c(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-8-isaac-simulation-pipeline",children:"Chapter 8: Isaac Simulation Pipeline"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Set up NVIDIA Isaac Sim with proper GPU acceleration"}),"\n",(0,t.jsx)(e.li,{children:"Create complex simulation environments with realistic lighting"}),"\n",(0,t.jsx)(e.li,{children:"Configure physics properties for accurate robot behavior"}),"\n",(0,t.jsx)(e.li,{children:"Integrate perception sensors with Isaac extensions"}),"\n",(0,t.jsx)(e.li,{children:"Generate synthetic training data for AI models"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,t.jsx)(e.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,t.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","NVIDIA Isaac Sim (2023.1.1 or newer)"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","CUDA 11.8+ with compatible GPU"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Isaac ROS packages (isaac_ros_apriltag, isaac_ros_detectnet, etc.)"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Omniverse Kit and extensions"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed Module 1 and 2 content"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,t.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Understanding of ROS 2 and simulation concepts"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Basic knowledge of GPU computing and CUDA"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Familiarity with computer vision sensors"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed Chapter 5-7 content"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,t.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Completed robot model from Module 1"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Access to Isaac Sim documentation and tutorials"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,t.jsx)(e.h3,{id:"what-is-nvidia-isaac-sim",children:"What is NVIDIA Isaac Sim?"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA Isaac Sim is a GPU-accelerated robotics simulator built on NVIDIA's Omniverse platform. It provides high-fidelity simulation capabilities with accurate physics, rendering, and sensor simulation that closely matches real-world behavior. Isaac Sim is designed for developing and testing AI algorithms for robotics applications."}),"\n",(0,t.jsx)(e.h3,{id:"key-isaac-sim-components",children:"Key Isaac Sim Components"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Omniverse Extensions"}),": Modular components that extend Isaac Sim functionality:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Isaac Sim Sensors: Realistic sensor simulation with noise models"}),"\n",(0,t.jsx)(e.li,{children:"Isaac Sim Robotics: Robot-specific extensions and utilities"}),"\n",(0,t.jsx)(e.li,{children:"Isaac Sim Assets: High-quality robot and environment models"}),"\n",(0,t.jsx)(e.li,{children:"Isaac Sim Navigation: Navigation-specific tools and assets"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"GPU Acceleration"}),": Isaac Sim leverages NVIDIA GPUs for:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Real-time ray tracing and rendering"}),"\n",(0,t.jsx)(e.li,{children:"Parallel physics simulation"}),"\n",(0,t.jsx)(e.li,{children:"Sensor processing acceleration"}),"\n",(0,t.jsx)(e.li,{children:"AI inference acceleration"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Synthetic Data Generation"}),": Isaac Sim enables:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Labeled training data generation"}),"\n",(0,t.jsx)(e.li,{children:"Domain randomization techniques"}),"\n",(0,t.jsx)(e.li,{children:"Photorealistic environment simulation"}),"\n",(0,t.jsx)(e.li,{children:"Multi-modal sensor data synthesis"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"isaac-sim-pipeline-architecture",children:"Isaac Sim Pipeline Architecture"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim follows a component-based architecture where different extensions work together:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"World Building"}),": Creating and configuring simulation environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot Integration"}),": Adding robots with proper physics and sensors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Simulation"}),": Configuring realistic sensor models"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Data Generation"}),": Producing training datasets for AI models"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS Bridge"}),": Connecting to external ROS systems"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-mermaid",children:"graph TD\r\n    A[Isaac Sim Pipeline] --\x3e B[World Building]\r\n    A --\x3e C[Robot Integration]\r\n    A --\x3e D[Sensor Simulation]\r\n    A --\x3e E[Data Generation]\r\n    A --\x3e F[ROS Bridge]\r\n\r\n    B --\x3e B1[Environment Setup]\r\n    B --\x3e B2[Lighting Configuration]\r\n    B --\x3e B3[Physics Properties]\r\n\r\n    C --\x3e C1[URDF Import]\r\n    C --\x3e C2[Physics Config]\r\n    C --\x3e C3[Joint Setup]\r\n\r\n    D --\x3e D1[Camera Simulation]\r\n    D --\x3e D2[Lidar Simulation]\r\n    D --\x3e D3[IMU/Force Sensors]\r\n\r\n    E --\x3e E1[Synthetic Data]\r\n    E --\x3e E2[Domain Randomization]\r\n    E --\x3e E3[Label Generation]\r\n\r\n    F --\x3e F1[ROS 2 Bridge]\r\n    F --\x3e F2[Message Conversion]\r\n    F --\x3e F3[External Control]\r\n\r\n    B1 --\x3e D\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    F1 --\x3e A\n"})}),"\n",(0,t.jsx)(e.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,t.jsx)(e.p,{children:"Let's create a Python script that sets up Isaac Sim with a robot and sensors:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# isaac_sim_setup.py\r\nimport omni\r\nfrom pxr import Usd, UsdGeom, Gf\r\nimport carb\r\nimport omni.usd\r\nimport omni.kit.commands\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.robots import Robot\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.viewports import set_camera_view\r\nfrom omni.isaac.sensor import Camera\r\nimport numpy as np\r\n\r\nclass IsaacSimEnvironment:\r\n    """\r\n    A class to set up and manage Isaac Sim environments with robots and sensors.\r\n    """\r\n\r\n    def __init__(self):\r\n        self.world = None\r\n        self.robot = None\r\n        self.cameras = []\r\n\r\n    def setup_world(self):\r\n        """Initialize the Isaac Sim world"""\r\n        self.world = World(stage_units_in_meters=1.0)\r\n\r\n        # Set up default lighting\r\n        omni.kit.commands.execute("CreateLightCommand",\r\n                                 light_type_name="DistantLight",\r\n                                 position=Gf.Vec3f(0, 0, 10),\r\n                                 look_at=Gf.Vec3f(0, 0, 0))\r\n\r\n        # Add ground plane\r\n        omni.kit.commands.execute("AddXformCommand",\r\n                                 stage=self.world.stage,\r\n                                 xform_name="ground_plane",\r\n                                 select_new_prim=True)\r\n\r\n        # Set up camera view\r\n        set_camera_view(eye=Gf.Vec3f(5, 5, 3),\r\n                       target=Gf.Vec3f(0, 0, 0))\r\n\r\n        print("Isaac Sim world initialized")\r\n\r\n    def add_robot(self, robot_path, position=(0, 0, 0)):\r\n        """Add a robot to the simulation"""\r\n        try:\r\n            # Add robot from USD file\r\n            robot_prim_path = "/World/Robot"\r\n            add_reference_to_stage(\r\n                usd_path=robot_path,\r\n                prim_path=robot_prim_path\r\n            )\r\n\r\n            # Create robot object\r\n            self.robot = Robot(\r\n                prim_path=robot_prim_path,\r\n                name="isaac_robot",\r\n                position=position,\r\n                orientation=[0, 0, 0, 1]\r\n            )\r\n\r\n            # Add robot to world\r\n            self.world.scene.add(self.robot)\r\n\r\n            print(f"Robot added from {robot_path}")\r\n            return True\r\n\r\n        except Exception as e:\r\n            print(f"Error adding robot: {e}")\r\n            return False\r\n\r\n    def add_camera(self, name, position, parent="/World/Robot"):\r\n        """Add a camera sensor to the robot"""\r\n        try:\r\n            camera_path = f"{parent}/{name}"\r\n\r\n            # Create camera prim\r\n            camera_prim = omni.kit.commands.execute(\r\n                "CreatePrimWithDefaultXform",\r\n                prim_type="Camera",\r\n                prim_path=camera_path\r\n            )\r\n\r\n            # Set camera properties\r\n            camera = Camera(\r\n                prim_path=camera_path,\r\n                frequency=30,\r\n                resolution=(640, 480)\r\n            )\r\n\r\n            # Set camera position\r\n            import omni.kit.commands\r\n            omni.kit.commands.execute(\r\n                "TransformMultiPrimsSRTCommand",\r\n                count=1,\r\n                paths=[camera_path],\r\n                new_transform_matrix=Gf.Matrix4d().SetTranslate(Gf.Vec3d(*position))\r\n            )\r\n\r\n            self.cameras.append(camera)\r\n            print(f"Camera {name} added at {position}")\r\n            return camera\r\n\r\n        except Exception as e:\r\n            print(f"Error adding camera: {e}")\r\n            return None\r\n\r\n    def add_lidar(self, name, parent="/World/Robot"):\r\n        """Add a LIDAR sensor to the robot"""\r\n        try:\r\n            lidar_path = f"{parent}/{name}"\r\n\r\n            # Create LIDAR prim\r\n            omni.kit.commands.execute(\r\n                "CreatePrimWithDefaultXform",\r\n                prim_type="RotatingLidarSensor",\r\n                prim_path=lidar_path\r\n            )\r\n\r\n            print(f"LIDAR {name} added")\r\n            # Note: In real implementation, you\'d use the actual Isaac Sim LIDAR\r\n\r\n        except Exception as e:\r\n            print(f"Error adding LIDAR: {e}")\r\n\r\n    def run_simulation(self, steps=1000):\r\n        """Run the simulation for a specified number of steps"""\r\n        print(f"Running simulation for {steps} steps...")\r\n\r\n        for i in range(steps):\r\n            self.world.step(render=True)\r\n\r\n            if i % 100 == 0:\r\n                print(f"Step {i}/{steps}")\r\n\r\n                # Example: Get camera data\r\n                if self.cameras:\r\n                    camera = self.cameras[0]\r\n                    # In real implementation, capture image data\r\n                    print(f"Camera frame captured at step {i}")\r\n\r\n        print("Simulation completed")\r\n\r\n    def cleanup(self):\r\n        """Clean up the simulation environment"""\r\n        if self.world:\r\n            self.world.clear()\r\n        print("Isaac Sim environment cleaned up")\r\n\r\n\r\ndef main():\r\n    # Initialize simulation\r\n    sim_env = IsaacSimEnvironment()\r\n    sim_env.setup_world()\r\n\r\n    # Add robot (replace with actual robot USD path)\r\n    # robot_path = "path/to/robot.usd"  # This would be your robot model\r\n    # sim_env.add_robot(robot_path, position=(0, 0, 0.5))\r\n\r\n    # For demonstration, we\'ll just show the structure\r\n    print("Isaac Sim environment structure ready")\r\n\r\n    # Add sensors to robot\r\n    sim_env.add_camera("front_camera", position=(0.3, 0, 0.1))\r\n    sim_env.add_camera("left_camera", position=(0, 0.2, 0.1))\r\n    sim_env.add_camera("right_camera", position=(0, -0.2, 0.1))\r\n    sim_env.add_lidar("front_lidar")\r\n\r\n    # Run simulation (short for demonstration)\r\n    sim_env.run_simulation(steps=100)\r\n\r\n    # Clean up\r\n    sim_env.cleanup()\r\n\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"To run this simulation setup:"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Save it as ",(0,t.jsx)(e.code,{children:"isaac_sim_setup.py"})]}),"\n",(0,t.jsx)(e.li,{children:"Make sure Isaac Sim is properly installed with Omniverse"}),"\n",(0,t.jsx)(e.li,{children:"Replace the robot path with your actual robot USD model"}),"\n",(0,t.jsx)(e.li,{children:"Run in Isaac Sim's Python environment"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,t.jsx)(e.p,{children:"Now let's create a script for generating synthetic training data with domain randomization:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# synthetic_data_generator.py\r\nimport numpy as np\r\nimport random\r\nimport os\r\nfrom PIL import Image\r\nimport json\r\nimport cv2\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.robots import Robot\r\nfrom omni.isaac.sensor import Camera\r\nimport omni.kit.commands\r\nfrom pxr import Usd, Gf\r\n\r\nclass SyntheticDataGenerator:\r\n    """\r\n    A class for generating synthetic training data using Isaac Sim\r\n    with domain randomization techniques.\r\n    """\r\n\r\n    def __init__(self, output_dir="synthetic_data"):\r\n        self.output_dir = output_dir\r\n        self.world = None\r\n        self.cameras = []\r\n        self.objects = []\r\n\r\n        # Create output directory\r\n        os.makedirs(output_dir, exist_ok=True)\r\n        os.makedirs(f"{output_dir}/images", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/labels", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/masks", exist_ok=True)\r\n\r\n    def setup_environment(self):\r\n        """Set up the basic environment with domain randomization"""\r\n        self.world = World(stage_units_in_meters=1.0)\r\n\r\n        # Randomize environment properties\r\n        self.randomize_lighting()\r\n        self.randomize_environment()\r\n\r\n        print("Synthetic data environment initialized")\r\n\r\n    def randomize_lighting(self):\r\n        """Randomize lighting conditions for domain randomization"""\r\n        # Randomize sun light\r\n        sun_intensity = random.uniform(500, 2000)\r\n        sun_color = [\r\n            random.uniform(0.8, 1.0),  # R\r\n            random.uniform(0.8, 1.0),  # G\r\n            random.uniform(0.8, 1.0)   # B\r\n        ]\r\n\r\n        # Create random light direction\r\n        azimuth = random.uniform(0, 2*np.pi)\r\n        elevation = random.uniform(0.2, 0.8)\r\n\r\n        light_pos = [\r\n            10 * np.cos(azimuth) * np.cos(elevation),\r\n            10 * np.sin(azimuth) * np.cos(elevation),\r\n            10 * np.sin(elevation)\r\n        ]\r\n\r\n        print(f"Lighting randomized: intensity={sun_intensity}, position={light_pos}")\r\n\r\n    def randomize_environment(self):\r\n        """Randomize environmental conditions"""\r\n        # Randomize textures and materials\r\n        self.randomize_floor_materials()\r\n        self.randomize_background_objects()\r\n\r\n    def randomize_floor_materials(self):\r\n        """Randomize floor materials for domain randomization"""\r\n        materials = [\r\n            "concrete", "wood", "tile", "carpet", "grass", "metal"\r\n        ]\r\n        selected_material = random.choice(materials)\r\n        print(f"Floor material randomized to: {selected_material}")\r\n\r\n    def randomize_background_objects(self):\r\n        """Add randomized background objects"""\r\n        num_objects = random.randint(5, 15)\r\n\r\n        for i in range(num_objects):\r\n            # Random position and rotation\r\n            x = random.uniform(-5, 5)\r\n            y = random.uniform(-5, 5)\r\n            z = random.uniform(0, 1)\r\n\r\n            # Random object type\r\n            obj_types = ["box", "cylinder", "sphere", "cone"]\r\n            obj_type = random.choice(obj_types)\r\n\r\n            print(f"Added {obj_type} at ({x:.2f}, {y:.2f}, {z:.2f})")\r\n\r\n        print(f"Added {num_objects} background objects")\r\n\r\n    def capture_image_data(self, camera, step_num):\r\n        """Capture image and generate synthetic labels"""\r\n        try:\r\n            # In real Isaac Sim, you would capture the actual image\r\n            # For this example, we\'ll simulate the data generation\r\n\r\n            # Generate a random image for demonstration\r\n            height, width = 640, 480\r\n            image = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\r\n\r\n            # Simulate object detection labels\r\n            num_objects = random.randint(1, 5)\r\n            labels = []\r\n\r\n            for i in range(num_objects):\r\n                # Random bounding box\r\n                x1 = random.randint(0, width - 50)\r\n                y1 = random.randint(0, height - 50)\r\n                w = random.randint(20, 100)\r\n                h = random.randint(20, 100)\r\n                x2 = min(x1 + w, width)\r\n                y2 = min(y1 + h, height)\r\n\r\n                # Random class\r\n                classes = ["robot", "person", "box", "table", "chair"]\r\n                class_name = random.choice(classes)\r\n\r\n                label = {\r\n                    "class": class_name,\r\n                    "bbox": [x1, y1, x2, y2],\r\n                    "confidence": 1.0\r\n                }\r\n                labels.append(label)\r\n\r\n            # Save image\r\n            img_path = f"{self.output_dir}/images/img_{step_num:06d}.png"\r\n            cv2.imwrite(img_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\r\n\r\n            # Save labels\r\n            label_path = f"{self.output_dir}/labels/labels_{step_num:06d}.json"\r\n            with open(label_path, \'w\') as f:\r\n                json.dump({\r\n                    "image_path": img_path,\r\n                    "labels": labels,\r\n                    "metadata": {\r\n                        "lighting_condition": "randomized",\r\n                        "background_objects": len([o for o in range(random.randint(5, 15))]),\r\n                        "domain_randomization": True\r\n                    }\r\n                }, f, indent=2)\r\n\r\n            return True\r\n\r\n        except Exception as e:\r\n            print(f"Error capturing image data: {e}")\r\n            return False\r\n\r\n    def generate_training_data(self, num_samples=1000):\r\n        """Generate synthetic training data with domain randomization"""\r\n        print(f"Generating {num_samples} synthetic training samples...")\r\n\r\n        successful_captures = 0\r\n\r\n        for i in range(num_samples):\r\n            # Randomize environment for each sample\r\n            if i % 10 == 0:  # Every 10 samples\r\n                self.randomize_environment()\r\n\r\n            # Capture image data\r\n            if self.capture_image_data(None, i):\r\n                successful_captures += 1\r\n\r\n            # Progress indicator\r\n            if i % 100 == 0:\r\n                print(f"Progress: {i}/{num_samples} samples generated")\r\n\r\n        print(f"Completed! Generated {successful_captures} successful samples out of {num_samples}")\r\n\r\n        # Create dataset info file\r\n        dataset_info = {\r\n            "total_samples": num_samples,\r\n            "successful_samples": successful_captures,\r\n            "image_size": [640, 480],\r\n            "classes": ["robot", "person", "box", "table", "chair"],\r\n            "domain_randomization": True,\r\n            "synthetic_data": True\r\n        }\r\n\r\n        with open(f"{self.output_dir}/dataset_info.json", \'w\') as f:\r\n            json.dump(dataset_info, f, indent=2)\r\n\r\n        print(f"Dataset info saved to {self.output_dir}/dataset_info.json")\r\n\r\n    def add_object_detection_task(self, object_types, detection_config):\r\n        """Set up object detection in the simulation environment"""\r\n        print(f"Setting up object detection for: {object_types}")\r\n        print(f"Detection config: {detection_config}")\r\n\r\n        # In real implementation, this would configure Isaac ROS detection nodes\r\n        # and connect them to the simulation sensors\r\n\r\n    def cleanup(self):\r\n        """Clean up resources"""\r\n        if self.world:\r\n            self.world.clear()\r\n        print("Synthetic data generator cleaned up")\r\n\r\n\r\ndef main():\r\n    # Initialize synthetic data generator\r\n    data_gen = SyntheticDataGenerator(output_dir="synthetic_training_data")\r\n    data_gen.setup_environment()\r\n\r\n    # Add object detection configuration\r\n    object_types = ["robot", "person", "box", "table", "chair"]\r\n    detection_config = {\r\n        "confidence_threshold": 0.5,\r\n        "max_objects": 10,\r\n        "image_topic": "/camera/rgb/image_raw"\r\n    }\r\n\r\n    data_gen.add_object_detection_task(object_types, detection_config)\r\n\r\n    # Generate synthetic training data\r\n    data_gen.generate_training_data(num_samples=500)  # Reduced for demonstration\r\n\r\n    # Clean up\r\n    data_gen.cleanup()\r\n\r\n    print("\\nSynthetic data generation completed!")\r\n    print("Generated files:")\r\n    print("- Images saved in synthetic_training_data/images/")\r\n    print("- Labels saved in synthetic_training_data/labels/")\r\n    print("- Dataset info in synthetic_training_data/dataset_info.json")\r\n\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"To run this data generation:"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Save it as ",(0,t.jsx)(e.code,{children:"synthetic_data_generator.py"})]}),"\n",(0,t.jsx)(e.li,{children:"Make sure Isaac Sim is properly configured"}),"\n",(0,t.jsx)(e.li,{children:"Run to generate synthetic training data with domain randomization"}),"\n",(0,t.jsx)(e.li,{children:"The script will create a dataset suitable for training AI models"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,t.jsx)(e.p,{children:"Create a complete Isaac Sim scene that includes:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"A complex environment with multiple rooms and obstacles"}),"\n",(0,t.jsx)(e.li,{children:"A robot with multiple sensors (RGB camera, depth camera, IMU)"}),"\n",(0,t.jsx)(e.li,{children:"Dynamic objects that move during simulation"}),"\n",(0,t.jsx)(e.li,{children:"Data capture pipeline that saves sensor data in ROS bag format"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Hint:"})," Use Isaac Sim's extension system to add multiple sensors to your robot and implement a data recording pipeline that captures synchronized sensor data."]}),"\n",(0,t.jsx)(e.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,t.jsx)(e.p,{children:"To verify that your Isaac Sim setup is working correctly:"}),"\n",(0,t.jsx)(e.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"When starting Isaac Sim: Proper initialization messages and GPU detection"}),"\n",(0,t.jsx)(e.li,{children:"When loading robot models: Model loading confirmation and physics setup"}),"\n",(0,t.jsx)(e.li,{children:"When capturing data: Progress indicators and data statistics"}),"\n",(0,t.jsx)(e.li,{children:"When running simulation: Frame rate and timing information"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"The 3D environment renders with realistic lighting and physics"}),"\n",(0,t.jsx)(e.li,{children:"Robot responds properly to physics simulation"}),"\n",(0,t.jsx)(e.li,{children:"Sensors generate realistic data streams"}),"\n",(0,t.jsx)(e.li,{children:"Domain randomization visibly changes scene properties"}),"\n",(0,t.jsx)(e.li,{children:"Data capture pipeline produces labeled datasets"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,t.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Isaac Sim environment properly configured with GPU acceleration"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Robot model successfully integrated with sensors"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Domain randomization techniques implemented"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Synthetic data generation pipeline working"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Complete scene with dynamic objects (Try Yourself task)"]}),"\n",(0,t.jsxs)(e.li,{className:"task-list-item",children:[(0,t.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Data capture pipeline producing ROS bag files"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This chapter introduced the NVIDIA Isaac Sim platform for creating advanced robotics simulation environments. You learned about the key components of Isaac Sim, including its GPU-accelerated rendering, sensor simulation capabilities, and synthetic data generation features. The examples demonstrated setting up simulation environments with domain randomization and creating synthetic training datasets for AI models."}),"\n",(0,t.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["NVIDIA. (2023). ",(0,t.jsx)(e.em,{children:"NVIDIA Isaac Sim User Guide"}),". Retrieved from ",(0,t.jsx)(e.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html",children:"https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html"})]}),"\n",(0,t.jsxs)(e.li,{children:["NVIDIA. (2023). ",(0,t.jsx)(e.em,{children:"Isaac Extensions for Robotics Simulation"}),". Technical Report."]}),"\n",(0,t.jsxs)(e.li,{children:["Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., & Abbeel, P. (2017). Domain randomization for transferring deep neural networks from simulation to the real world. ",(0,t.jsx)(e.em,{children:"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"}),", 23-30."]}),"\n",(0,t.jsxs)(e.li,{children:["NVIDIA. (2023). ",(0,t.jsx)(e.em,{children:"Omniverse Platform Architecture for Robotics Simulation"}),". White Paper."]}),"\n",(0,t.jsxs)(e.li,{children:["James, S., Freese, M., & Murai, A. (2019). PyRobot: An open-source robotics research platform. ",(0,t.jsx)(e.em,{children:"arXiv preprint arXiv:1906.08825"}),"."]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>o});var i=r(6540);const t={},a=i.createContext(t);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);