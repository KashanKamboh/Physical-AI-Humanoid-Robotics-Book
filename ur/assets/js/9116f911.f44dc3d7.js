"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[2493],{2835:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/chapter-13","title":"Chapter 13: Capstone Execution","description":"Learning Outcomes","source":"@site/docs/module-4/chapter-13.md","sourceDirName":"module-4","slug":"/module-4/chapter-13","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-4/chapter-13","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-4/chapter-13.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 12: Cognitive Task Planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-4/chapter-12"},"next":{"title":"AI Notes - Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-4/ai-notes"}}');var s=r(4848),a=r(8453);const i={sidebar_position:4},o="Chapter 13: Capstone Execution",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"The Complete VLA System Architecture",id:"the-complete-vla-system-architecture",level:3},{value:"Integration Challenges",id:"integration-challenges",level:3},{value:"System Evaluation Metrics",id:"system-evaluation-metrics",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function m(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-13-capstone-execution",children:"Chapter 13: Capstone Execution"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate all previous modules into a complete autonomous humanoid system"}),"\n",(0,s.jsx)(n.li,{children:"Execute end-to-end voice-commanded robotic operations"}),"\n",(0,s.jsx)(n.li,{children:"Implement the complete VLA pipeline: VOICE \u27f6 PLAN \u27f6 NAVIGATE \u27f6 RECOGNIZE OBJECT \u27f6 MANIPULATE"}),"\n",(0,s.jsx)(n.li,{children:"Deploy and test the complete humanoid robotics system"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate and optimize system performance across all components"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,s.jsx)(n.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 Humble Hawksbill (or newer)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All packages from Modules 1-3 (ROS 2, Gazebo, Unity, Isaac)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","OpenAI Whisper and NLP libraries"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","BehaviorTree.CPP and planning libraries"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed all previous modules and chapters"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understanding of complete ROS 2 architecture and communication"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Experience with simulation and real-world deployment"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Knowledge of perception, navigation, and manipulation systems"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Familiarity with voice processing and task planning"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All code from previous modules and chapters"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Complete robot model and configuration files"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Trained models for perception and voice processing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,s.jsx)(n.h3,{id:"the-complete-vla-system-architecture",children:"The Complete VLA System Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The capstone project integrates all components into a complete Vision-Language-Action system following the pipeline:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VOICE"}),": Natural language command input through speech recognition"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Wake word detection and audio preprocessing"}),"\n",(0,s.jsx)(n.li,{children:"ASR (Automatic Speech Recognition) with Whisper or similar"}),"\n",(0,s.jsx)(n.li,{children:"Natural Language Understanding (NLU) for intent and entity extraction"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"PLAN"}),": Cognitive task planning and decomposition"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Hierarchical task network generation"}),"\n",(0,s.jsx)(n.li,{children:"Resource allocation and constraint satisfaction"}),"\n",(0,s.jsx)(n.li,{children:"Failure recovery and replanning mechanisms"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"NAVIGATE"}),": Autonomous navigation to goal locations"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Global and local path planning"}),"\n",(0,s.jsx)(n.li,{children:"Obstacle avoidance and dynamic replanning"}),"\n",(0,s.jsx)(n.li,{children:"Localization and mapping integration"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"RECOGNIZE OBJECT"}),": Vision-based object detection and identification"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time object detection with Isaac ROS"}),"\n",(0,s.jsx)(n.li,{children:"3D object pose estimation"}),"\n",(0,s.jsx)(n.li,{children:"Semantic segmentation and scene understanding"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"MANIPULATE"}),": Robotic manipulation of identified objects"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Grasp planning and execution"}),"\n",(0,s.jsx)(n.li,{children:"Trajectory generation and control"}),"\n",(0,s.jsx)(n.li,{children:"Force control and compliance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"integration-challenges",children:"Integration Challenges"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-time Performance"}),": Ensuring all components operate within timing constraints:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Audio processing at 16kHz"}),"\n",(0,s.jsx)(n.li,{children:"Vision processing at 10-30 FPS"}),"\n",(0,s.jsx)(n.li,{children:"Control loops at 100Hz+"}),"\n",(0,s.jsx)(n.li,{children:"Communication latencies under 100ms"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"State Consistency"}),": Maintaining synchronized world state across components:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"TF tree consistency"}),"\n",(0,s.jsx)(n.li,{children:"Shared world representation"}),"\n",(0,s.jsx)(n.li,{children:"Multi-sensor fusion"}),"\n",(0,s.jsx)(n.li,{children:"Action execution tracking"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Managing failures across the complete pipeline:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Graceful degradation"}),"\n",(0,s.jsx)(n.li,{children:"Recovery mechanisms"}),"\n",(0,s.jsx)(n.li,{children:"User feedback and clarification"}),"\n",(0,s.jsx)(n.li,{children:"Safe failure modes"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"system-evaluation-metrics",children:"System Evaluation Metrics"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Functional Metrics"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Command success rate"}),"\n",(0,s.jsx)(n.li,{children:"Task completion time"}),"\n",(0,s.jsx)(n.li,{children:"Accuracy of object recognition"}),"\n",(0,s.jsx)(n.li,{children:"Navigation success rate"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance Metrics"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time processing latencies"}),"\n",(0,s.jsx)(n.li,{children:"CPU/GPU utilization"}),"\n",(0,s.jsx)(n.li,{children:"Memory usage"}),"\n",(0,s.jsx)(n.li,{children:"Communication throughput"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"User Experience Metrics"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Naturalness of interaction"}),"\n",(0,s.jsx)(n.li,{children:"Time to task completion"}),"\n",(0,s.jsx)(n.li,{children:"Error recovery effectiveness"}),"\n",(0,s.jsx)(n.li,{children:"System reliability"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Autonomous Humanoid System] --\x3e B[VOICE Processing]\r\n    A --\x3e C[PLAN Generation]\r\n    A --\x3e D[NAVIGATE Execution]\r\n    A --\x3e E[RECOGNIZE OBJECT]\r\n    A --\x3e F[MANIPULATE Action]\r\n    A --\x3e G[System Integration]\r\n\r\n    B --\x3e B1[Speech Recognition]\r\n    B --\x3e B2[Natural Language Understanding]\r\n    B --\x3e B3[Intent Classification]\r\n\r\n    C --\x3e C1[Task Decomposition]\r\n    C --\x3e C2[Behavior Tree Execution]\r\n    C --\x3e C3[Resource Management]\r\n\r\n    D --\x3e D1[Path Planning]\r\n    D --\x3e D2[Obstacle Avoidance]\r\n    D --\x3e D3[Localization]\r\n\r\n    E --\x3e E1[Object Detection]\r\n    E --\x3e E2[3D Pose Estimation]\r\n    E --\x3e E3[Scene Understanding]\r\n\r\n    F --\x3e F1[Grasp Planning]\r\n    F --\x3e F2[Trajectory Execution]\r\n    F --\x3e F3[Force Control]\r\n\r\n    G --\x3e G1[State Management]\r\n    G --\x3e G2[Error Handling]\r\n    G --\x3e G3[Performance Monitoring]\r\n\r\n    B1 --\x3e C\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    E1 --\x3e F\r\n    F1 --\x3e G\r\n    G1 --\x3e B\n"})}),"\n",(0,s.jsx)(n.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,s.jsx)(n.p,{children:"Let's create the main capstone integration system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# capstone_integration.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient, ActionServer\r\nfrom rclpy.qos import QoSProfile\r\nfrom std_msgs.msg import String, Bool, Int8\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom control_msgs.action import FollowJointTrajectory\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom builtin_interfaces.msg import Duration\r\n\r\nimport threading\r\nimport time\r\nimport json\r\nimport queue\r\nfrom typing import Dict, List, Optional, Any\r\nfrom dataclasses import dataclass\r\nfrom enum import Enum\r\n\r\n\r\nclass SystemState(Enum):\r\n    \"\"\"Overall system state\"\"\"\r\n    IDLE = \"idle\"\r\n    LISTENING = \"listening\"\r\n    PROCESSING = \"processing\"\r\n    PLANNING = \"planning\"\r\n    EXECUTING = \"executing\"\r\n    ERROR = \"error\"\r\n    COMPLETED = \"completed\"\r\n\r\n\r\n@dataclass\r\nclass CapstoneCommand:\r\n    \"\"\"Data class for capstone commands\"\"\"\r\n    voice_command: str\r\n    intent: str\r\n    entities: Dict[str, Any]\r\n    confidence: float\r\n    timestamp: float\r\n\r\n\r\nclass CapstoneIntegrationNode(Node):\r\n    \"\"\"\r\n    The main capstone integration node that brings together all system components.\r\n    This implements the complete VLA pipeline: VOICE \u27f6 PLAN \u27f6 NAVIGATE \u27f6 RECOGNIZE OBJECT \u27f6 MANIPULATE\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('capstone_integration')\r\n\r\n        # Initialize all subsystems\r\n        self.initialize_subsystems()\r\n\r\n        # Publishers for system status and feedback\r\n        self.status_pub = self.create_publisher(String, '/capstone/status', 10)\r\n        self.feedback_pub = self.create_publisher(String, '/capstone/feedback', 10)\r\n        self.system_state_pub = self.create_publisher(String, '/capstone/system_state', 10)\r\n\r\n        # Subscribers for all inputs\r\n        self.voice_command_sub = self.create_subscription(\r\n            String,\r\n            '/voice_agent/commands',\r\n            self.voice_command_callback,\r\n            10\r\n        )\r\n\r\n        self.detection_sub = self.create_subscription(\r\n            Detection2DArray,\r\n            '/perception/fused_detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.nav_status_sub = self.create_subscription(\r\n            String,\r\n            '/navigation/status',\r\n            self.nav_status_callback,\r\n            10\r\n        )\r\n\r\n        # Action clients for all major components\r\n        self.nav_action_client = ActionClient(\r\n            self,\r\n            NavigateToPose,\r\n            'navigate_to_pose'\r\n        )\r\n\r\n        self.manipulation_action_client = ActionClient(\r\n            self,\r\n            FollowJointTrajectory,\r\n            'manipulation_controller/follow_joint_trajectory'\r\n        )\r\n\r\n        # Internal state\r\n        self.current_state = SystemState.IDLE\r\n        self.command_queue = queue.Queue()\r\n        self.active_command = None\r\n        self.is_executing = False\r\n        self.execution_thread = None\r\n        self.world_state = {\r\n            'robot_pose': None,\r\n            'detected_objects': [],\r\n            'navigation_status': 'idle',\r\n            'manipulation_status': 'idle'\r\n        }\r\n\r\n        # Performance monitoring\r\n        self.start_time = time.time()\r\n        self.command_count = 0\r\n        self.success_count = 0\r\n\r\n        # Timer for system monitoring\r\n        self.monitor_timer = self.create_timer(1.0, self.system_monitor)\r\n\r\n        self.get_logger().info('Capstone Integration System initialized')\r\n\r\n    def initialize_subsystems(self):\r\n        \"\"\"Initialize all subsystems\"\"\"\r\n        # This would initialize each module's components\r\n        self.get_logger().info('Initializing subsystems...')\r\n\r\n        # Voice processing (from Module 4)\r\n        self.voice_initialized = True\r\n\r\n        # Planning system (from Module 4)\r\n        self.planning_initialized = True\r\n\r\n        # Navigation (from Module 3)\r\n        self.navigation_initialized = True\r\n\r\n        # Perception (from Module 3)\r\n        self.perception_initialized = True\r\n\r\n        # Manipulation (from Module 1-3)\r\n        self.manipulation_initialized = True\r\n\r\n        self.get_logger().info('All subsystems initialized')\r\n\r\n    def voice_command_callback(self, msg):\r\n        \"\"\"Process incoming voice commands\"\"\"\r\n        try:\r\n            # Parse the command data\r\n            command_data = json.loads(msg.data)\r\n\r\n            command = CapstoneCommand(\r\n                voice_command=command_data['text'],\r\n                intent=command_data['intent'],\r\n                entities=command_data['entities'],\r\n                confidence=command_data['confidence'],\r\n                timestamp=command_data['timestamp']\r\n            )\r\n\r\n            # Add to command queue\r\n            self.command_queue.put(command)\r\n            self.command_count += 1\r\n\r\n            self.get_logger().info(f'Received command: {command.intent} - {command.voice_command}')\r\n\r\n            # If system is idle, start processing\r\n            if self.current_state == SystemState.IDLE:\r\n                self.process_next_command()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing voice command: {e}')\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Process object detections\"\"\"\r\n        try:\r\n            detected_objects = []\r\n            for detection in msg.detections:\r\n                if detection.results:\r\n                    obj_class = detection.results[0].hypothesis.class_id\r\n                    confidence = detection.results[0].hypothesis.score\r\n                    bbox = detection.bbox\r\n\r\n                    detected_objects.append({\r\n                        'class': obj_class,\r\n                        'confidence': confidence,\r\n                        'bbox': [bbox.center.x, bbox.center.y, bbox.size_x, bbox.size_y]\r\n                    })\r\n\r\n            self.world_state['detected_objects'] = detected_objects\r\n            self.get_logger().debug(f'Detected {len(detected_objects)} objects')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing detections: {e}')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming images\"\"\"\r\n        # In real implementation, this would trigger perception processing\r\n        # For now, just log that we received an image\r\n        pass\r\n\r\n    def nav_status_callback(self, msg):\r\n        \"\"\"Process navigation status updates\"\"\"\r\n        self.world_state['navigation_status'] = msg.data\r\n\r\n    def process_next_command(self):\r\n        \"\"\"Process the next command in the queue\"\"\"\r\n        if not self.command_queue.empty():\r\n            self.active_command = self.command_queue.get()\r\n            self.current_state = SystemState.PROCESSING\r\n\r\n            # Update system state\r\n            self.update_system_state()\r\n\r\n            # Execute the command based on intent\r\n            success = self.execute_capstone_command(self.active_command)\r\n\r\n            if success:\r\n                self.success_count += 1\r\n                self.current_state = SystemState.COMPLETED\r\n                self.get_logger().info(f'Command completed successfully: {self.active_command.intent}')\r\n            else:\r\n                self.current_state = SystemState.ERROR\r\n                self.get_logger().error(f'Command failed: {self.active_command.intent}')\r\n\r\n            # Reset for next command\r\n            self.active_command = None\r\n            self.current_state = SystemState.IDLE\r\n\r\n            # Process next command if available\r\n            if not self.command_queue.empty():\r\n                self.process_next_command()\r\n\r\n    def execute_capstone_command(self, command: CapstoneCommand) -> bool:\r\n        \"\"\"Execute a capstone command following the VLA pipeline\"\"\"\r\n        self.get_logger().info(f'Executing capstone command: {command.intent}')\r\n\r\n        # Step 1: VOICE - Already processed, command is our input\r\n\r\n        # Step 2: PLAN - Create and execute plan based on intent\r\n        if not self.execute_planning_step(command):\r\n            return False\r\n\r\n        # Step 3: NAVIGATE - Execute navigation if required\r\n        if command.intent in ['navigation', 'fetch', 'go_to']:\r\n            if not self.execute_navigation_step(command):\r\n                return False\r\n\r\n        # Step 4: RECOGNIZE OBJECT - Detect and identify objects\r\n        if command.intent in ['fetch', 'grasp', 'find']:\r\n            if not self.execute_recognition_step(command):\r\n                return False\r\n\r\n        # Step 5: MANIPULATE - Execute manipulation if required\r\n        if command.intent in ['grasp', 'manipulate', 'fetch']:\r\n            if not self.execute_manipulation_step(command):\r\n                return False\r\n\r\n        return True\r\n\r\n    def execute_planning_step(self, command: CapstoneCommand) -> bool:\r\n        \"\"\"Execute the planning step of the VLA pipeline\"\"\"\r\n        self.get_logger().info('Executing planning step...')\r\n\r\n        # Update state\r\n        self.current_state = SystemState.PLANNING\r\n        self.update_system_state()\r\n\r\n        # In real implementation, this would use the planning system from Chapter 12\r\n        # For now, we'll simulate planning success\r\n\r\n        # Publish planning status\r\n        status_msg = String()\r\n        status_msg.data = f\"Planning for command: {command.intent}\"\r\n        self.status_pub.publish(status_msg)\r\n\r\n        # Simulate planning time\r\n        time.sleep(0.5)\r\n\r\n        self.get_logger().info('Planning completed')\r\n        return True\r\n\r\n    def execute_navigation_step(self, command: CapstoneCommand) -> bool:\r\n        \"\"\"Execute the navigation step of the VLA pipeline\"\"\"\r\n        self.get_logger().info('Executing navigation step...')\r\n\r\n        # Update state\r\n        self.current_state = SystemState.EXECUTING\r\n        self.update_system_state()\r\n\r\n        # Determine target location\r\n        target_location = self.get_location_coordinates(command.entities.get('location', ''))\r\n\r\n        if not target_location:\r\n            self.get_logger().error(f'Unknown location: {command.entities.get(\"location\", \"\")}')\r\n            return False\r\n\r\n        # Execute navigation\r\n        success = self.navigate_to_location(target_location)\r\n\r\n        if success:\r\n            self.get_logger().info('Navigation completed successfully')\r\n            return True\r\n        else:\r\n            self.get_logger().error('Navigation failed')\r\n            return False\r\n\r\n    def execute_recognition_step(self, command: CapstoneCommand) -> bool:\r\n        \"\"\"Execute the object recognition step of the VLA pipeline\"\"\"\r\n        self.get_logger().info('Executing object recognition step...')\r\n\r\n        # Update state\r\n        self.current_state = SystemState.EXECUTING\r\n        self.update_system_state()\r\n\r\n        # Wait for object detections\r\n        timeout = 10.0  # 10 second timeout\r\n        start_time = time.time()\r\n\r\n        while time.time() - start_time < timeout:\r\n            if self.world_state['detected_objects']:\r\n                # Check if target object is detected\r\n                target_object = command.entities.get('object', '').lower()\r\n\r\n                for obj in self.world_state['detected_objects']:\r\n                    if target_object in obj['class'].lower() and obj['confidence'] > 0.7:\r\n                        self.get_logger().info(f'Found target object: {obj[\"class\"]} with confidence {obj[\"confidence\"]:.2f}')\r\n                        return True\r\n\r\n            time.sleep(0.1)\r\n\r\n        self.get_logger().error('Target object not found within timeout')\r\n        return False\r\n\r\n    def execute_manipulation_step(self, command: CapstoneCommand) -> bool:\r\n        \"\"\"Execute the manipulation step of the VLA pipeline\"\"\"\r\n        self.get_logger().info('Executing manipulation step...')\r\n\r\n        # Update state\r\n        self.current_state = SystemState.EXECUTING\r\n        self.update_system_state()\r\n\r\n        # Determine manipulation action\r\n        action = command.entities.get('action', 'grasp')\r\n        target_object = command.entities.get('object', '')\r\n\r\n        if action == 'grasp' or 'grasp' in action.lower():\r\n            success = self.execute_grasp_action(target_object)\r\n        elif action == 'place' or 'place' in action.lower():\r\n            target_location = command.entities.get('location', 'default')\r\n            success = self.execute_place_action(target_object, target_location)\r\n        else:\r\n            self.get_logger().info(f'Unknown manipulation action: {action}')\r\n            success = False\r\n\r\n        if success:\r\n            self.get_logger().info('Manipulation completed successfully')\r\n            return True\r\n        else:\r\n            self.get_logger().error('Manipulation failed')\r\n            return False\r\n\r\n    def navigate_to_location(self, coordinates: tuple) -> bool:\r\n        \"\"\"Execute navigation to specified coordinates\"\"\"\r\n        try:\r\n            # Wait for action server\r\n            if not self.nav_action_client.wait_for_server(timeout_sec=1.0):\r\n                self.get_logger().error('Navigation action server not available')\r\n                return False\r\n\r\n            # Create navigation goal\r\n            goal_msg = NavigateToPose.Goal()\r\n            goal_msg.pose.header.frame_id = 'map'\r\n            goal_msg.pose.pose.position.x = coordinates[0]\r\n            goal_msg.pose.pose.position.y = coordinates[1]\r\n            goal_msg.pose.pose.position.z = coordinates[2]\r\n            goal_msg.pose.pose.orientation.w = 1.0\r\n\r\n            # Send goal\r\n            goal_future = self.nav_action_client.send_goal_async(goal_msg)\r\n            rclpy.spin_until_future_complete(self, goal_future)\r\n\r\n            goal_handle = goal_future.result()\r\n            if not goal_handle.accepted:\r\n                self.get_logger().error('Navigation goal was rejected')\r\n                return False\r\n\r\n            result_future = goal_handle.get_result_async()\r\n            rclpy.spin_until_future_complete(self, result_future)\r\n\r\n            result = result_future.result().result\r\n            status = result.error_code\r\n\r\n            return status == result.SUCCESS\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in navigation: {e}')\r\n            return False\r\n\r\n    def execute_grasp_action(self, target_object: str) -> bool:\r\n        \"\"\"Execute grasping action for target object\"\"\"\r\n        self.get_logger().info(f'Executing grasp for: {target_object}')\r\n\r\n        # In real implementation, this would use manipulation planning\r\n        # For simulation, return success\r\n        return True\r\n\r\n    def execute_place_action(self, target_object: str, location: str) -> bool:\r\n        \"\"\"Execute placing action for target object\"\"\"\r\n        self.get_logger().info(f'Executing place for: {target_object} at {location}')\r\n\r\n        # In real implementation, this would use manipulation planning\r\n        # For simulation, return success\r\n        return True\r\n\r\n    def get_location_coordinates(self, location_name: str) -> Optional[tuple]:\r\n        \"\"\"Get coordinates for location names\"\"\"\r\n        location_map = {\r\n            'kitchen': (3.0, 2.0, 0.0),\r\n            'bedroom': (-2.0, 1.0, 0.0),\r\n            'living room': (0.0, 0.0, 0.0),\r\n            'office': (1.0, -2.0, 0.0),\r\n            'bathroom': (-1.0, -1.0, 0.0),\r\n            'table': (0.5, 0.5, 0.0),\r\n            'shelf': (-0.5, 1.5, 0.0)\r\n        }\r\n\r\n        return location_map.get(location_name.lower())\r\n\r\n    def update_system_state(self):\r\n        \"\"\"Update and publish system state\"\"\"\r\n        state_msg = String()\r\n        state_msg.data = self.current_state.value\r\n        self.system_state_pub.publish(state_msg)\r\n\r\n        # Log state change\r\n        self.get_logger().info(f'System state: {self.current_state.value}')\r\n\r\n    def system_monitor(self):\r\n        \"\"\"Monitor system performance and health\"\"\"\r\n        # Publish system status\r\n        status_msg = String()\r\n        status_data = {\r\n            'state': self.current_state.value,\r\n            'commands_processed': self.command_count,\r\n            'success_rate': self.success_count / max(1, self.command_count) if self.command_count > 0 else 0,\r\n            'uptime': time.time() - self.start_time,\r\n            'queue_size': self.command_queue.qsize()\r\n        }\r\n        status_msg.data = json.dumps(status_data)\r\n        self.status_pub.publish(status_msg)\r\n\r\n        # Log performance metrics periodically\r\n        if self.command_count > 0 and self.command_count % 10 == 0:\r\n            success_rate = self.success_count / self.command_count\r\n            self.get_logger().info(f'System performance - Success rate: {success_rate:.2%}, Commands: {self.command_count}')\r\n\r\n\r\nclass AdvancedCapstoneNode(CapstoneIntegrationNode):\r\n    \"\"\"\r\n    Extended capstone node with advanced features and optimization\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Advanced features\r\n        self.context_awareness_enabled = True\r\n        self.adaptive_planning_enabled = True\r\n        self.multi_modal_feedback_enabled = True\r\n        self.learning_enabled = True\r\n\r\n        # Learning components\r\n        self.execution_history = []\r\n        self.performance_metrics = {}\r\n\r\n        # Timer for learning updates\r\n        self.learning_timer = self.create_timer(5.0, self.update_learning_model)\r\n\r\n        self.get_logger().info('Advanced Capstone Node initialized')\r\n\r\n    def execute_capstone_command(self, command: CapstoneCommand) -> bool:\r\n        \"\"\"Enhanced command execution with learning and adaptation\"\"\"\r\n        start_time = time.time()\r\n\r\n        # Execute the basic pipeline\r\n        success = super().execute_capstone_command(command)\r\n\r\n        # Record execution for learning\r\n        execution_record = {\r\n            'command': command,\r\n            'success': success,\r\n            'execution_time': time.time() - start_time,\r\n            'timestamp': time.time()\r\n        }\r\n\r\n        self.execution_history.append(execution_record)\r\n        if len(self.execution_history) > 100:  # Keep last 100 records\r\n            self.execution_history.pop(0)\r\n\r\n        # Update performance metrics\r\n        self.update_performance_metrics(command.intent, success)\r\n\r\n        return success\r\n\r\n    def update_performance_metrics(self, intent: str, success: bool):\r\n        \"\"\"Update performance metrics for different intents\"\"\"\r\n        if intent not in self.performance_metrics:\r\n            self.performance_metrics[intent] = {\r\n                'attempts': 0,\r\n                'successes': 0,\r\n                'avg_time': 0.0\r\n            }\r\n\r\n        metrics = self.performance_metrics[intent]\r\n        metrics['attempts'] += 1\r\n        if success:\r\n            metrics['successes'] += 1\r\n\r\n        # Update average time (simplified)\r\n        # In real implementation, track execution times more precisely\r\n\r\n    def update_learning_model(self):\r\n        \"\"\"Update learning model based on execution history\"\"\"\r\n        if len(self.execution_history) < 5:  # Need some data to learn from\r\n            return\r\n\r\n        # Analyze recent performance\r\n        recent_executions = self.execution_history[-10:]  # Last 10 executions\r\n        success_rate = sum(1 for ex in recent_executions if ex['success']) / len(recent_executions)\r\n\r\n        self.get_logger().info(f'Recent success rate: {success_rate:.2%}')\r\n\r\n        # In real implementation, this would update planning strategies,\r\n        # adjust parameters, or retrain models based on performance\r\n        if success_rate < 0.7:  # If success rate is low, consider adaptation\r\n            self.get_logger().warning('Low success rate detected, consider system adaptation')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    # Create capstone integration node\r\n    capstone_node = AdvancedCapstoneNode()\r\n\r\n    try:\r\n        capstone_node.get_logger().info('Capstone Integration System running...')\r\n        rclpy.spin(capstone_node)\r\n    except KeyboardInterrupt:\r\n        capstone_node.get_logger().info('Shutting down Capstone Integration System')\r\n    finally:\r\n        capstone_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To run this capstone integration system:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Save it as ",(0,s.jsx)(n.code,{children:"capstone_integration.py"})]}),"\n",(0,s.jsx)(n.li,{children:"Make sure all dependencies from previous modules are installed"}),"\n",(0,s.jsxs)(n.li,{children:["Run: ",(0,s.jsx)(n.code,{children:"ros2 run <package_name> capstone_integration"})]}),"\n",(0,s.jsx)(n.li,{children:"Send commands to the system to test the complete VLA pipeline"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a comprehensive system evaluation and testing framework:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# capstone_evaluation.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.qos import QoSProfile\r\nfrom std_msgs.msg import String, Float32, Int32\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nimport time\r\nimport json\r\nimport csv\r\nimport os\r\nfrom datetime import datetime\r\nfrom typing import Dict, List, Tuple, Optional\r\nimport threading\r\nimport statistics\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n\r\nclass CapstoneEvaluationNode(Node):\r\n    \"\"\"\r\n    A comprehensive evaluation framework for the capstone system.\r\n    This evaluates the complete VLA pipeline performance across multiple metrics.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('capstone_evaluation')\r\n\r\n        # Publishers for evaluation metrics\r\n        self.performance_pub = self.create_publisher(Float32, '/evaluation/performance_score', 10)\r\n        self.accuracy_pub = self.create_publisher(Float32, '/evaluation/accuracy_score', 10)\r\n        self.latency_pub = self.create_publisher(Float32, '/evaluation/latency', 10)\r\n        self.success_rate_pub = self.create_publisher(Float32, '/evaluation/success_rate', 10)\r\n\r\n        # Subscribers for system monitoring\r\n        self.status_sub = self.create_subscription(\r\n            String,\r\n            '/capstone/status',\r\n            self.status_callback,\r\n            10\r\n        )\r\n\r\n        self.feedback_sub = self.create_subscription(\r\n            String,\r\n            '/capstone/feedback',\r\n            self.feedback_callback,\r\n            10\r\n        )\r\n\r\n        self.system_state_sub = self.create_subscription(\r\n            String,\r\n            '/capstone/system_state',\r\n            self.system_state_callback,\r\n            10\r\n        )\r\n\r\n        # Internal state for evaluation\r\n        self.evaluation_data = {\r\n            'commands': [],\r\n            'responses': [],\r\n            'performance_metrics': [],\r\n            'accuracy_metrics': [],\r\n            'latency_metrics': [],\r\n            'success_rates': []\r\n        }\r\n\r\n        self.command_start_times = {}\r\n        self.evaluation_results = {}\r\n        self.evaluation_enabled = True\r\n\r\n        # Timer for periodic evaluation\r\n        self.evaluation_timer = self.create_timer(2.0, self.run_evaluation_cycle)\r\n\r\n        # Timer for saving results\r\n        self.save_timer = self.create_timer(30.0, self.save_evaluation_results)\r\n\r\n        # Initialize results directory\r\n        self.results_dir = \"capstone_evaluation_results\"\r\n        os.makedirs(self.results_dir, exist_ok=True)\r\n\r\n        self.get_logger().info('Capstone Evaluation Framework initialized')\r\n\r\n    def status_callback(self, msg):\r\n        \"\"\"Process system status updates\"\"\"\r\n        try:\r\n            status_data = json.loads(msg.data)\r\n\r\n            # Record performance metrics\r\n            self.evaluation_data['performance_metrics'].append({\r\n                'timestamp': time.time(),\r\n                'data': status_data\r\n            })\r\n\r\n            # Calculate and publish metrics\r\n            self.calculate_and_publish_metrics()\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing status: {e}')\r\n\r\n    def feedback_callback(self, msg):\r\n        \"\"\"Process system feedback\"\"\"\r\n        # Record feedback for evaluation\r\n        self.evaluation_data['responses'].append({\r\n            'timestamp': time.time(),\r\n            'feedback': msg.data\r\n        })\r\n\r\n    def system_state_callback(self, msg):\r\n        \"\"\"Process system state updates\"\"\"\r\n        # Monitor system state for evaluation\r\n        current_state = msg.data\r\n        self.get_logger().debug(f'System state: {current_state}')\r\n\r\n    def run_evaluation_cycle(self):\r\n        \"\"\"Run periodic evaluation cycle\"\"\"\r\n        if not self.evaluation_enabled:\r\n            return\r\n\r\n        # Calculate current metrics\r\n        current_metrics = self.calculate_current_metrics()\r\n\r\n        # Publish metrics\r\n        self.publish_current_metrics(current_metrics)\r\n\r\n        # Log metrics\r\n        self.get_logger().info(f'Evaluation - Success: {current_metrics[\"success_rate\"]:.2%}, '\r\n                              f'Latency: {current_metrics[\"avg_latency\"]:.3f}s, '\r\n                              f'Accuracy: {current_metrics[\"accuracy\"]:.2%}')\r\n\r\n    def calculate_current_metrics(self) -> Dict[str, float]:\r\n        \"\"\"Calculate current performance metrics\"\"\"\r\n        metrics = {\r\n            'success_rate': 0.0,\r\n            'avg_latency': 0.0,\r\n            'accuracy': 0.0,\r\n            'throughput': 0.0,\r\n            'resource_utilization': 0.0\r\n        }\r\n\r\n        # Calculate success rate from status data\r\n        success_count = 0\r\n        total_count = 0\r\n        latencies = []\r\n\r\n        for perf_data in self.evaluation_data['performance_metrics'][-20:]:  # Last 20 data points\r\n            data = perf_data['data']\r\n            if 'success_rate' in data:\r\n                success_count += int(data['success_rate'] * 10)\r\n                total_count += 10  # Assuming rate is out of 10\r\n\r\n            if 'latency' in data:\r\n                latencies.append(data['latency'])\r\n\r\n        if total_count > 0:\r\n            metrics['success_rate'] = success_count / total_count if total_count > 0 else 0.0\r\n\r\n        if latencies:\r\n            metrics['avg_latency'] = statistics.mean(latencies)\r\n\r\n        # Calculate accuracy (simplified - in real implementation, compare expected vs actual)\r\n        metrics['accuracy'] = min(1.0, len(self.evaluation_data['responses']) / max(1, len(self.evaluation_data['commands'])))\r\n\r\n        # Calculate throughput (commands per second)\r\n        if self.evaluation_data['performance_metrics']:\r\n            time_span = (time.time() - self.evaluation_data['performance_metrics'][0]['timestamp']) or 1\r\n            metrics['throughput'] = len(self.evaluation_data['commands']) / time_span\r\n\r\n        return metrics\r\n\r\n    def publish_current_metrics(self, metrics: Dict[str, float]):\r\n        \"\"\"Publish current metrics\"\"\"\r\n        # Publish success rate\r\n        success_msg = Float32()\r\n        success_msg.data = metrics['success_rate']\r\n        self.success_rate_pub.publish(success_msg)\r\n\r\n        # Publish latency\r\n        latency_msg = Float32()\r\n        latency_msg.data = metrics['avg_latency']\r\n        self.latency_pub.publish(latency_msg)\r\n\r\n        # Publish accuracy\r\n        accuracy_msg = Float32()\r\n        accuracy_msg.data = metrics['accuracy']\r\n        self.accuracy_pub.publish(accuracy_msg)\r\n\r\n        # Publish performance score (weighted combination)\r\n        performance_msg = Float32()\r\n        performance_msg.data = (\r\n            metrics['success_rate'] * 0.4 +\r\n            (1 - min(1.0, metrics['avg_latency'] / 5.0)) * 0.3 +  # Invert latency (lower is better)\r\n            metrics['accuracy'] * 0.3\r\n        )\r\n        self.performance_pub.publish(performance_msg)\r\n\r\n    def save_evaluation_results(self):\r\n        \"\"\"Save evaluation results to files\"\"\"\r\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\r\n\r\n        # Save metrics to CSV\r\n        csv_path = os.path.join(self.results_dir, f'evaluation_metrics_{timestamp}.csv')\r\n        self.save_metrics_to_csv(csv_path)\r\n\r\n        # Save detailed results\r\n        json_path = os.path.join(self.results_dir, f'evaluation_results_{timestamp}.json')\r\n        self.save_results_to_json(json_path)\r\n\r\n        # Generate summary\r\n        summary_path = os.path.join(self.results_dir, f'evaluation_summary_{timestamp}.txt')\r\n        self.generate_summary(summary_path)\r\n\r\n        self.get_logger().info(f'Evaluation results saved to {self.results_dir}/')\r\n\r\n    def save_metrics_to_csv(self, csv_path: str):\r\n        \"\"\"Save metrics to CSV file\"\"\"\r\n        if not self.evaluation_data['performance_metrics']:\r\n            return\r\n\r\n        with open(csv_path, 'w', newline='') as csvfile:\r\n            fieldnames = ['timestamp', 'success_rate', 'latency', 'accuracy', 'throughput']\r\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\r\n\r\n            writer.writeheader()\r\n            for perf_data in self.evaluation_data['performance_metrics']:\r\n                data = perf_data['data']\r\n                writer.writerow({\r\n                    'timestamp': perf_data['timestamp'],\r\n                    'success_rate': data.get('success_rate', 0),\r\n                    'latency': data.get('latency', 0),\r\n                    'accuracy': data.get('accuracy', 0),\r\n                    'throughput': data.get('throughput', 0)\r\n                })\r\n\r\n    def save_results_to_json(self, json_path: str):\r\n        \"\"\"Save detailed results to JSON file\"\"\"\r\n        results = {\r\n            'evaluation_data': self.evaluation_data,\r\n            'evaluation_timestamp': time.time(),\r\n            'summary_metrics': self.calculate_summary_metrics()\r\n        }\r\n\r\n        with open(json_path, 'w') as f:\r\n            json.dump(results, f, indent=2)\r\n\r\n    def calculate_summary_metrics(self) -> Dict[str, float]:\r\n        \"\"\"Calculate summary metrics for the evaluation\"\"\"\r\n        if not self.evaluation_data['performance_metrics']:\r\n            return {}\r\n\r\n        success_rates = []\r\n        latencies = []\r\n        accuracies = []\r\n\r\n        for perf_data in self.evaluation_data['performance_metrics']:\r\n            data = perf_data['data']\r\n            if 'success_rate' in data:\r\n                success_rates.append(data['success_rate'])\r\n            if 'latency' in data:\r\n                latencies.append(data['latency'])\r\n            if 'accuracy' in data:\r\n                accuracies.append(data['accuracy'])\r\n\r\n        summary = {}\r\n        if success_rates:\r\n            summary['avg_success_rate'] = statistics.mean(success_rates)\r\n            summary['min_success_rate'] = min(success_rates)\r\n            summary['max_success_rate'] = max(success_rates)\r\n        if latencies:\r\n            summary['avg_latency'] = statistics.mean(latencies)\r\n            summary['min_latency'] = min(latencies)\r\n            summary['max_latency'] = max(latencies)\r\n        if accuracies:\r\n            summary['avg_accuracy'] = statistics.mean(accuracies)\r\n            summary['min_accuracy'] = min(accuracies)\r\n            summary['max_accuracy'] = max(accuracies)\r\n\r\n        return summary\r\n\r\n    def generate_summary(self, summary_path: str):\r\n        \"\"\"Generate evaluation summary\"\"\"\r\n        summary_metrics = self.calculate_summary_metrics()\r\n\r\n        with open(summary_path, 'w') as f:\r\n            f.write(\"Capstone System Evaluation Summary\\n\")\r\n            f.write(\"=\" * 40 + \"\\n\\n\")\r\n\r\n            f.write(f\"Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\r\n            f.write(f\"Total Performance Records: {len(self.evaluation_data['performance_metrics'])}\\n\\n\")\r\n\r\n            f.write(\"Performance Metrics:\\n\")\r\n            f.write(\"-\" * 20 + \"\\n\")\r\n            for key, value in summary_metrics.items():\r\n                f.write(f\"{key}: {value:.4f}\\n\")\r\n\r\n            f.write(f\"\\nCommands Processed: {len(self.evaluation_data['commands'])}\\n\")\r\n            f.write(f\"Feedback Records: {len(self.evaluation_data['responses'])}\\n\")\r\n\r\n    def run_comprehensive_test_suite(self):\r\n        \"\"\"Run a comprehensive test suite for the capstone system\"\"\"\r\n        self.get_logger().info('Starting comprehensive test suite...')\r\n\r\n        # Define test scenarios\r\n        test_scenarios = [\r\n            {\r\n                'name': 'Basic Navigation',\r\n                'commands': ['go to kitchen', 'navigate to bedroom'],\r\n                'expected_outcomes': ['navigation_success'] * 2\r\n            },\r\n            {\r\n                'name': 'Object Fetching',\r\n                'commands': ['fetch the cup', 'bring me the book'],\r\n                'expected_outcomes': ['manipulation_success'] * 2\r\n            },\r\n            {\r\n                'name': 'Complex Tasks',\r\n                'commands': ['go to kitchen and bring me a cup', 'find the red ball and place it on the table'],\r\n                'expected_outcomes': ['complex_task_success'] * 2\r\n            }\r\n        ]\r\n\r\n        results = {}\r\n\r\n        for scenario in test_scenarios:\r\n            self.get_logger().info(f'Running scenario: {scenario[\"name\"]}')\r\n            scenario_results = self.run_scenario_tests(scenario)\r\n            results[scenario['name']] = scenario_results\r\n\r\n        # Generate comprehensive report\r\n        self.generate_comprehensive_report(results)\r\n\r\n        return results\r\n\r\n    def run_scenario_tests(self, scenario: Dict) -> Dict:\r\n        \"\"\"Run tests for a specific scenario\"\"\"\r\n        results = {\r\n            'scenario': scenario['name'],\r\n            'tests_run': len(scenario['commands']),\r\n            'tests_passed': 0,\r\n            'tests_failed': 0,\r\n            'details': []\r\n        }\r\n\r\n        for i, command in enumerate(scenario['commands']):\r\n            expected = scenario['expected_outcomes'][i]\r\n\r\n            # In real implementation, send command to system and wait for response\r\n            # For simulation, we'll record the test\r\n            test_result = {\r\n                'command': command,\r\n                'expected': expected,\r\n                'actual': expected,  # For simulation, assume success\r\n                'success': True,  # For simulation\r\n                'latency': np.random.uniform(1.0, 3.0),  # Simulated latency\r\n                'timestamp': time.time()\r\n            }\r\n\r\n            results['details'].append(test_result)\r\n\r\n            if test_result['success']:\r\n                results['tests_passed'] += 1\r\n            else:\r\n                results['tests_failed'] += 1\r\n\r\n        return results\r\n\r\n    def generate_comprehensive_report(self, results: Dict):\r\n        \"\"\"Generate a comprehensive evaluation report\"\"\"\r\n        report_path = os.path.join(self.results_dir, f'comprehensive_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt')\r\n\r\n        with open(report_path, 'w') as f:\r\n            f.write(\"COMPREHENSIVE CAPSTONE SYSTEM EVALUATION REPORT\\n\")\r\n            f.write(\"=\" * 60 + \"\\n\\n\")\r\n\r\n            f.write(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\r\n\r\n            total_tests = 0\r\n            total_passed = 0\r\n            total_failed = 0\r\n\r\n            for scenario_name, scenario_results in results.items():\r\n                f.write(f\"Scenario: {scenario_name}\\n\")\r\n                f.write(\"-\" * 30 + \"\\n\")\r\n                f.write(f\"Tests Run: {scenario_results['tests_run']}\\n\")\r\n                f.write(f\"Tests Passed: {scenario_results['tests_passed']}\\n\")\r\n                f.write(f\"Tests Failed: {scenario_results['tests_failed']}\\n\")\r\n\r\n                if scenario_results['tests_run'] > 0:\r\n                    success_rate = scenario_results['tests_passed'] / scenario_results['tests_run']\r\n                    f.write(f\"Success Rate: {success_rate:.2%}\\n\")\r\n\r\n                f.write(\"\\n\")\r\n\r\n                total_tests += scenario_results['tests_run']\r\n                total_passed += scenario_results['tests_passed']\r\n                total_failed += scenario_results['tests_failed']\r\n\r\n            f.write(f\"TOTAL SUMMARY\\n\")\r\n            f.write(\"-\" * 15 + \"\\n\")\r\n            f.write(f\"Total Tests: {total_tests}\\n\")\r\n            f.write(f\"Total Passed: {total_passed}\\n\")\r\n            f.write(f\"Total Failed: {total_failed}\\n\")\r\n            if total_tests > 0:\r\n                f.write(f\"Overall Success Rate: {total_passed/total_tests:.2%}\\n\")\r\n\r\n            # System recommendations based on results\r\n            f.write(f\"\\nRECOMMENDATIONS\\n\")\r\n            f.write(\"-\" * 15 + \"\\n\")\r\n            if total_passed/total_tests < 0.8:\r\n                f.write(\"\u2022 Success rate is below 80%, investigate failure patterns\\n\")\r\n                f.write(\"\u2022 Consider improving error recovery mechanisms\\n\")\r\n            else:\r\n                f.write(\"\u2022 Success rate is acceptable\\n\")\r\n\r\n            f.write(\"\u2022 Monitor latency metrics for real-time performance\\n\")\r\n            f.write(\"\u2022 Continue collecting data for long-term analysis\\n\")\r\n\r\n        self.get_logger().info(f'Comprehensive report saved to {report_path}')\r\n\r\n\r\nclass CapstoneDeploymentNode(CapstoneEvaluationNode):\r\n    \"\"\"\r\n    Extended node for deployment and real-world testing\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Deployment-specific features\r\n        self.deployment_mode = 'simulation'  # 'simulation', 'testbed', 'real_world'\r\n        self.safety_enabled = True\r\n        self.user_interaction_enabled = True\r\n\r\n        # Add deployment-specific subscribers/publishers if needed\r\n        self.deployment_status_pub = self.create_publisher(\r\n            String, '/deployment/status', 10\r\n        )\r\n\r\n        self.get_logger().info('Capstone Deployment Node initialized')\r\n\r\n    def run_deployment_tests(self):\r\n        \"\"\"Run deployment-specific tests\"\"\"\r\n        self.get_logger().info(f'Running deployment tests in {self.deployment_mode} mode')\r\n\r\n        # Different test strategies based on deployment mode\r\n        if self.deployment_mode == 'simulation':\r\n            self.run_simulation_tests()\r\n        elif self.deployment_mode == 'testbed':\r\n            self.run_testbed_tests()\r\n        elif self.deployment_mode == 'real_world':\r\n            self.run_real_world_tests()\r\n\r\n    def run_simulation_tests(self):\r\n        \"\"\"Run tests in simulation environment\"\"\"\r\n        self.get_logger().info('Running simulation-specific tests...')\r\n        # Simulation tests would go here\r\n\r\n    def run_testbed_tests(self):\r\n        \"\"\"Run tests in physical testbed\"\"\"\r\n        self.get_logger().info('Running testbed-specific tests...')\r\n        # Testbed tests would go here\r\n\r\n    def run_real_world_tests(self):\r\n        \"\"\"Run tests in real-world environment\"\"\"\r\n        self.get_logger().info('Running real-world tests...')\r\n        # Real-world tests would go here\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    # Create evaluation node\r\n    eval_node = CapstoneDeploymentNode()\r\n\r\n    try:\r\n        eval_node.get_logger().info('Capstone Evaluation Framework running...')\r\n\r\n        # Run comprehensive test suite\r\n        test_thread = threading.Thread(target=eval_node.run_comprehensive_test_suite)\r\n        test_thread.daemon = True\r\n        test_thread.start()\r\n\r\n        # Run deployment tests\r\n        deployment_thread = threading.Thread(target=eval_node.run_deployment_tests)\r\n        deployment_thread.daemon = True\r\n        deployment_thread.start()\r\n\r\n        rclpy.spin(eval_node)\r\n    except KeyboardInterrupt:\r\n        eval_node.get_logger().info('Shutting down Capstone Evaluation Framework')\r\n    finally:\r\n        eval_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To run this evaluation framework:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Save it as ",(0,s.jsx)(n.code,{children:"capstone_evaluation.py"})]}),"\n",(0,s.jsxs)(n.li,{children:["Install required dependencies: ",(0,s.jsx)(n.code,{children:"pip install matplotlib numpy"})]}),"\n",(0,s.jsxs)(n.li,{children:["Run: ",(0,s.jsx)(n.code,{children:"ros2 run <package_name> capstone_evaluation"})]}),"\n",(0,s.jsx)(n.li,{children:"The system will automatically evaluate the capstone performance and generate reports"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,s.jsx)(n.p,{children:"Create a complete deployment and optimization system that includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time performance optimization that adjusts system parameters based on load"}),"\n",(0,s.jsx)(n.li,{children:"A user feedback loop that learns from human corrections"}),"\n",(0,s.jsx)(n.li,{children:"Anomaly detection that identifies unusual system behavior"}),"\n",(0,s.jsx)(n.li,{children:"A continuous integration pipeline for system updates and improvements"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hint:"})," Use machine learning models to predict system load and adjust resources, implement a feedback collection system for human corrections, use statistical methods for anomaly detection, and create automated testing pipelines for continuous improvement."]}),"\n",(0,s.jsx)(n.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,s.jsx)(n.p,{children:"To verify that your complete capstone system is working correctly:"}),"\n",(0,s.jsx)(n.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When starting the system: Initialization messages for all components"}),"\n",(0,s.jsx)(n.li,{children:"When processing commands: Step-by-step execution logs"}),"\n",(0,s.jsx)(n.li,{children:"When evaluating performance: Metrics and success rates"}),"\n",(0,s.jsx)(n.li,{children:"When generating reports: Summary of system capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Complete VLA pipeline executes end-to-end in Gazebo/Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"All modules (ROS 2, Perception, Navigation, Voice) work together"}),"\n",(0,s.jsx)(n.li,{children:"Performance metrics are collected and visualized"}),"\n",(0,s.jsx)(n.li,{children:"System demonstrates autonomous humanoid capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Complete VLA pipeline integration (VOICE \u27f6 PLAN \u27f6 NAVIGATE \u27f6 RECOGNIZE OBJECT \u27f6 MANIPULATE)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Capstone system evaluation and testing framework"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance metrics and optimization"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Real-world deployment considerations"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Complete autonomous humanoid system (Try Yourself task)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Continuous integration and improvement pipeline"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter completed the capstone project by integrating all previous modules into a complete autonomous humanoid system. You learned how to connect the voice processing, planning, navigation, perception, and manipulation components into a cohesive Vision-Language-Action pipeline. The examples demonstrated implementing the complete system with evaluation frameworks and deployment considerations. The capstone project demonstrates the full potential of physical AI and humanoid robotics as envisioned in the course."}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Brooks, R. A. (1991). Intelligence without representation. ",(0,s.jsx)(n.em,{children:"Artificial Intelligence"}),", 47(1-3), 139-159."]}),"\n",(0,s.jsxs)(n.li,{children:["Nilsson, N. J. (2007). ",(0,s.jsx)(n.em,{children:"The Quest for Artificial Intelligence: A History of Ideas and Achievements"}),". Cambridge University Press."]}),"\n",(0,s.jsxs)(n.li,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,s.jsx)(n.em,{children:"Springer Handbook of Robotics"}),". Springer."]}),"\n",(0,s.jsxs)(n.li,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,s.jsx)(n.em,{children:"Probabilistic Robotics"}),". MIT Press."]}),"\n",(0,s.jsxs)(n.li,{children:["Russell, S., & Norvig, P. (2020). ",(0,s.jsx)(n.em,{children:"Artificial Intelligence: A Modern Approach"})," (4th ed.). Pearson."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var t=r(6540);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);