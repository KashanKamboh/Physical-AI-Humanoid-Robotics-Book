"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[5594],{8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},9236:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-1/chapter-4","title":"Chapter 4: Integrating Python Agents","description":"Learning Outcomes","source":"@site/docs/module-1/chapter-4.md","sourceDirName":"module-1","slug":"/module-1/chapter-4","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-1/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-1/chapter-4.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: URDF for Humanoids","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-1/chapter-3"},"next":{"title":"AI Notes - Module 1: Robotic Nervous System (ROS 2)","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-1/ai-notes"}}');var s=r(4848),i=r(8453);const a={sidebar_position:5},o="Chapter 4: Integrating Python Agents",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites Checklist",id:"prerequisites-checklist",level:2},{value:"Required Software Installed",id:"required-software-installed",level:3},{value:"Required Module Completion",id:"required-module-completion",level:3},{value:"Files Needed",id:"files-needed",level:3},{value:"Core Concept Explanation",id:"core-concept-explanation",level:2},{value:"AI-Robot Integration Architecture",id:"ai-robot-integration-architecture",level:3},{value:"Agent Communication Patterns",id:"agent-communication-patterns",level:3},{value:"Cognitive Architecture Patterns",id:"cognitive-architecture-patterns",level:3},{value:"Diagram or Pipeline",id:"diagram-or-pipeline",level:2},{value:"Runnable Code Example A",id:"runnable-code-example-a",level:2},{value:"Runnable Code Example B",id:"runnable-code-example-b",level:2},{value:"&quot;Try Yourself&quot; Mini Task",id:"try-yourself-mini-task",level:2},{value:"Verification Procedure",id:"verification-procedure",level:2},{value:"What appears in terminal?",id:"what-appears-in-terminal",level:3},{value:"What changes in simulation?",id:"what-changes-in-simulation",level:3},{value:"Checklist for Completion",id:"checklist-for-completion",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-4-integrating-python-agents",children:"Chapter 4: Integrating Python Agents"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate Python-based AI agents with ROS 2 systems"}),"\n",(0,s.jsx)(n.li,{children:"Create intelligent behaviors using Python libraries"}),"\n",(0,s.jsx)(n.li,{children:"Implement agent communication with ROS nodes"}),"\n",(0,s.jsx)(n.li,{children:"Design cognitive architectures that bridge AI and robotics"}),"\n",(0,s.jsx)(n.li,{children:"Execute GPT-generated tasks as executable ROS code"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites-checklist",children:"Prerequisites Checklist"}),"\n",(0,s.jsx)(n.h3,{id:"required-software-installed",children:"Required Software Installed"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 Humble Hawksbill (or newer)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Python 3.8+ with pip"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","OpenAI Python library (for AI integration)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed Chapters 1-3 content"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"required-module-completion",children:"Required Module Completion"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understanding of ROS 2 nodes and communication"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Basic knowledge of Python AI libraries"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Familiarity with URDF robot models"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"files-needed",children:"Files Needed"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completed robot model from Chapter 3"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Access to OpenAI API or local LLM (optional)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concept-explanation",children:"Core Concept Explanation"}),"\n",(0,s.jsx)(n.h3,{id:"ai-robot-integration-architecture",children:"AI-Robot Integration Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The integration of AI agents with robotic systems involves several key components:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Perception Interface"}),": Translating sensor data into a format suitable for AI processing\r\n",(0,s.jsx)(n.strong,{children:"Cognition Engine"}),": The AI system that processes information and makes decisions\r\n",(0,s.jsx)(n.strong,{children:"Action Interface"}),": Converting AI decisions into robot actions\r\n",(0,s.jsx)(n.strong,{children:"Feedback Loop"}),": Monitoring and adjusting behavior based on outcomes"]}),"\n",(0,s.jsx)(n.h3,{id:"agent-communication-patterns",children:"Agent Communication Patterns"}),"\n",(0,s.jsx)(n.p,{children:"AI agents communicate with ROS systems through:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Topics"}),": For continuous data streams (sensor data, status updates)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Services"}),": For on-demand information requests"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Actions"}),": For long-running tasks with feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),": For configuration and settings"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cognitive-architecture-patterns",children:"Cognitive Architecture Patterns"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Reactive Agents"}),": Respond directly to environmental stimuli\r\n",(0,s.jsx)(n.strong,{children:"Deliberative Agents"}),": Plan actions based on goals and world models\r\n",(0,s.jsx)(n.strong,{children:"Hybrid Agents"}),": Combine reactive and deliberative approaches\r\n",(0,s.jsx)(n.strong,{children:"Learning Agents"}),": Adapt behavior based on experience"]}),"\n",(0,s.jsx)(n.h2,{id:"diagram-or-pipeline",children:"Diagram or Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TB\r\n    A[AI-Robot Integration] --\x3e B[Perception Interface]\r\n    A --\x3e C[Cognition Engine]\r\n    A --\x3e D[Action Interface]\r\n    A --\x3e E[Feedback Loop]\r\n\r\n    B --\x3e B1[Sensor Data Processing]\r\n    B --\x3e B2[Feature Extraction]\r\n    B --\x3e B3[World State Representation]\r\n\r\n    C --\x3e C1[Goal Reasoning]\r\n    C --\x3e C2[Plan Generation]\r\n    C --\x3e C3[Decision Making]\r\n\r\n    D --\x3e D1[Action Selection]\r\n    D --\x3e D2[Motor Command Generation]\r\n    D --\x3e D3[Execution Monitoring]\r\n\r\n    E --\x3e E1[Performance Evaluation]\r\n    E --\x3e E2[Learning Update]\r\n    E --\x3e E3[Behavior Adjustment]\r\n\r\n    B1 --\x3e C\r\n    C1 --\x3e D\r\n    D1 --\x3e E\r\n    E1 --\x3e B\n"})}),"\n",(0,s.jsx)(n.h2,{id:"runnable-code-example-a",children:"Runnable Code Example A"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a simple AI agent that controls a robot based on sensor inputs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# ai_controller.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String, Float32\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan\r\nimport random\r\nimport time\r\n\r\n\r\nclass AIController(Node):\r\n    \"\"\"\r\n    A simple AI controller that makes decisions based on sensor input.\r\n    This demonstrates the integration of basic AI logic with ROS 2.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('ai_controller')\r\n\r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        # Subscribers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        # Internal state\r\n        self.obstacle_distance = float('inf')\r\n        self.current_state = 'SEARCHING'  # SEARCHING, AVOIDING, STOPPED\r\n        self.last_action_time = time.time()\r\n\r\n        # Timer for AI decision making\r\n        self.ai_timer = self.create_timer(0.1, self.ai_decision_loop)\r\n\r\n        self.get_logger().info('AI Controller initialized')\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Process laser scan data to detect obstacles\"\"\"\r\n        # Get the minimum distance in the front 60-degree field\r\n        front_distances = msg.ranges[150:210]  # Assuming 360-degree scan\r\n        front_distances = [d for d in front_distances if not (d != d or d > 10.0)]  # Filter invalid readings\r\n\r\n        if front_distances:\r\n            self.obstacle_distance = min(front_distances)\r\n        else:\r\n            self.obstacle_distance = float('inf')\r\n\r\n    def ai_decision_loop(self):\r\n        \"\"\"Main AI decision loop\"\"\"\r\n        current_time = time.time()\r\n\r\n        # Update state based on sensor data\r\n        if self.obstacle_distance < 0.5:\r\n            self.current_state = 'AVOIDING'\r\n        elif self.obstacle_distance > 1.0:\r\n            self.current_state = 'SEARCHING'\r\n\r\n        # Execute appropriate behavior based on state\r\n        cmd_msg = Twist()\r\n\r\n        if self.current_state == 'SEARCHING':\r\n            # Move forward with slight random turn\r\n            cmd_msg.linear.x = 0.2\r\n            cmd_msg.angular.z = random.uniform(-0.2, 0.2)\r\n\r\n        elif self.current_state == 'AVOIDING':\r\n            # Turn away from obstacle\r\n            cmd_msg.linear.x = 0.0\r\n            cmd_msg.angular.z = 0.5  # Turn right to avoid obstacle\r\n\r\n        # Publish command\r\n        self.cmd_vel_pub.publish(cmd_msg)\r\n\r\n        # Log current state\r\n        self.get_logger().info(\r\n            f'State: {self.current_state}, '\r\n            f'Distance: {self.obstacle_distance:.2f}, '\r\n            f'Lin: {cmd_msg.linear.x:.2f}, '\r\n            f'Ang: {cmd_msg.angular.z:.2f}'\r\n        )\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    ai_controller = AIController()\r\n\r\n    try:\r\n        rclpy.spin(ai_controller)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        ai_controller.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To run this code:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Save it as ",(0,s.jsx)(n.code,{children:"ai_controller.py"})]}),"\n",(0,s.jsx)(n.li,{children:"Make sure your ROS 2 workspace is sourced"}),"\n",(0,s.jsxs)(n.li,{children:["Run: ",(0,s.jsx)(n.code,{children:"ros2 run <package_name> ai_controller"})]}),"\n",(0,s.jsxs)(n.li,{children:["Make sure you have a robot simulation running that publishes to ",(0,s.jsx)(n.code,{children:"/scan"})," and accepts commands on ",(0,s.jsx)(n.code,{children:"/cmd_vel"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"runnable-code-example-b",children:"Runnable Code Example B"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a more advanced agent that can interpret high-level commands and convert them to robot actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# command_interpreter_agent.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, Pose\r\nfrom nav_msgs.msg import Odometry\r\nimport json\r\nimport time\r\n\r\n\r\nclass CommandInterpreterAgent(Node):\r\n    """\r\n    An AI agent that interprets high-level commands and executes them.\r\n    This demonstrates GPT-generated task to executable ROS code conversion.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'command_interpreter_agent\')\r\n\r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.status_pub = self.create_publisher(String, \'/agent_status\', 10)\r\n\r\n        # Subscribers\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'/high_level_commands\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            \'/odom\',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n\r\n        # Internal state\r\n        self.current_pose = Pose()\r\n        self.current_command = None\r\n        self.command_queue = []\r\n        self.is_executing = False\r\n\r\n        # Timer for command execution\r\n        self.execution_timer = self.create_timer(0.1, self.execute_command)\r\n\r\n        self.get_logger().info(\'Command Interpreter Agent initialized\')\r\n\r\n    def odom_callback(self, msg):\r\n        """Update current robot pose from odometry"""\r\n        self.current_pose = msg.pose.pose\r\n\r\n    def command_callback(self, msg):\r\n        """Process high-level commands"""\r\n        try:\r\n            # Parse command from JSON string\r\n            command_data = json.loads(msg.data)\r\n            self.get_logger().info(f\'Received command: {command_data}\')\r\n\r\n            # Add to execution queue\r\n            self.command_queue.append(command_data)\r\n\r\n            # Publish status\r\n            status_msg = String()\r\n            status_msg.data = f\'Command received: {command_data["action"]}\'\r\n            self.status_pub.publish(status_msg)\r\n\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error(f\'Invalid JSON command: {msg.data}\')\r\n        except KeyError:\r\n            self.get_logger().error(f\'Missing required fields in command: {msg.data}\')\r\n\r\n    def execute_command(self):\r\n        """Execute commands from the queue"""\r\n        if not self.command_queue or self.is_executing:\r\n            return\r\n\r\n        command = self.command_queue.pop(0)\r\n        self.is_executing = True\r\n\r\n        action = command.get(\'action\', \'\')\r\n\r\n        if action == \'move_forward\':\r\n            self.execute_move_forward(command)\r\n        elif action == \'turn\':\r\n            self.execute_turn(command)\r\n        elif action == \'navigate_to\':\r\n            self.execute_navigate_to(command)\r\n        elif action == \'stop\':\r\n            self.execute_stop()\r\n        else:\r\n            self.get_logger().error(f\'Unknown action: {action}\')\r\n            self.is_executing = False\r\n            return\r\n\r\n        # Publish completion status\r\n        status_msg = String()\r\n        status_msg.data = f\'Completed: {action}\'\r\n        self.status_pub.publish(status_msg)\r\n\r\n    def execute_move_forward(self, command):\r\n        """Execute move forward command"""\r\n        distance = command.get(\'distance\', 1.0)  # meters\r\n        speed = command.get(\'speed\', 0.2)       # m/s\r\n\r\n        # Simple open-loop control (in real application, use feedback control)\r\n        duration = distance / speed\r\n        start_time = time.time()\r\n\r\n        cmd_msg = Twist()\r\n        cmd_msg.linear.x = speed\r\n\r\n        while time.time() - start_time < duration and rclpy.ok():\r\n            self.cmd_vel_pub.publish(cmd_msg)\r\n            time.sleep(0.01)\r\n\r\n        # Stop the robot\r\n        cmd_msg.linear.x = 0.0\r\n        self.cmd_vel_pub.publish(cmd_msg)\r\n        self.is_executing = False\r\n\r\n    def execute_turn(self, command):\r\n        """Execute turn command"""\r\n        angle = command.get(\'angle\', 90.0)    # degrees\r\n        speed = command.get(\'speed\', 0.5)     # rad/s\r\n\r\n        angle_rad = angle * 3.14159 / 180.0\r\n        duration = angle_rad / speed\r\n        start_time = time.time()\r\n\r\n        cmd_msg = Twist()\r\n        cmd_msg.angular.z = speed\r\n\r\n        while time.time() - start_time < duration and rclpy.ok():\r\n            self.cmd_vel_pub.publish(cmd_msg)\r\n            time.sleep(0.01)\r\n\r\n        # Stop the robot\r\n        cmd_msg.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(cmd_msg)\r\n        self.is_executing = False\r\n\r\n    def execute_navigate_to(self, command):\r\n        """Execute navigate to position command"""\r\n        target_x = command.get(\'x\', 0.0)\r\n        target_y = command.get(\'y\', 0.0)\r\n\r\n        # Simple proportional controller (in real application, use proper path planning)\r\n        kp_linear = 0.5\r\n        kp_angular = 1.0\r\n\r\n        max_linear_speed = 0.3\r\n        max_angular_speed = 0.5\r\n\r\n        cmd_msg = Twist()\r\n\r\n        # Navigate until close to target (within 0.1m)\r\n        while rclpy.ok():\r\n            # Calculate distance to target\r\n            dx = target_x - self.current_pose.position.x\r\n            dy = target_y - self.current_pose.position.y\r\n            distance = (dx**2 + dy**2)**0.5\r\n\r\n            if distance < 0.1:  # Close enough\r\n                cmd_msg.linear.x = 0.0\r\n                cmd_msg.angular.z = 0.0\r\n                self.cmd_vel_pub.publish(cmd_msg)\r\n                break\r\n\r\n            # Calculate required angle to target\r\n            target_angle = math.atan2(dy, dx)\r\n            current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)\r\n            angle_diff = target_angle - current_yaw\r\n\r\n            # Normalize angle difference\r\n            while angle_diff > 3.14159:\r\n                angle_diff -= 2 * 3.14159\r\n            while angle_diff < -3.14159:\r\n                angle_diff += 2 * 3.14159\r\n\r\n            # Set velocities\r\n            cmd_msg.linear.x = min(kp_linear * distance, max_linear_speed)\r\n            cmd_msg.angular.z = min(kp_angular * angle_diff, max_angular_speed)\r\n\r\n            # Limit angular velocity when moving fast linearly\r\n            if cmd_msg.linear.x > max_linear_speed * 0.7:\r\n                cmd_msg.angular.z *= 0.5\r\n\r\n            self.cmd_vel_pub.publish(cmd_msg)\r\n            time.sleep(0.01)\r\n\r\n        self.is_executing = False\r\n\r\n    def execute_stop(self):\r\n        """Execute stop command"""\r\n        cmd_msg = Twist()\r\n        cmd_msg.linear.x = 0.0\r\n        cmd_msg.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(cmd_msg)\r\n        self.is_executing = False\r\n\r\n    def get_yaw_from_quaternion(self, quaternion):\r\n        """Extract yaw from quaternion (simplified for 2D)"""\r\n        import math\r\n        siny_cosp = 2 * (quaternion.w * quaternion.z + quaternion.x * quaternion.y)\r\n        cosy_cosp = 1 - 2 * (quaternion.y * quaternion.y + quaternion.z * quaternion.z)\r\n        return math.atan2(siny_cosp, cosy_cosp)\r\n\r\n\r\ndef main(args=None):\r\n    import math  # Import here to avoid issues in the class\r\n    rclpy.init(args=args)\r\n    agent = CommandInterpreterAgent()\r\n\r\n    try:\r\n        rclpy.spin(agent)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        agent.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To run this code:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Save it as ",(0,s.jsx)(n.code,{children:"command_interpreter_agent.py"})]}),"\n",(0,s.jsxs)(n.li,{children:["In one terminal, run: ",(0,s.jsx)(n.code,{children:"ros2 run <package_name> command_interpreter_agent"})]}),"\n",(0,s.jsxs)(n.li,{children:["Send commands using: ",(0,s.jsx)(n.code,{children:'ros2 topic pub /high_level_commands std_msgs/String "data: \'{\\"action\\": \\"move_forward\\", \\"distance\\": 2.0, \\"speed\\": 0.3}\'"'})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"try-yourself-mini-task",children:'"Try Yourself" Mini Task'}),"\n",(0,s.jsx)(n.p,{children:"Create an AI agent that learns from demonstration. The agent should:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'Record a sequence of robot movements when in "record" mode'}),"\n",(0,s.jsx)(n.li,{children:'Play back the recorded sequence when in "playback" mode'}),"\n",(0,s.jsx)(n.li,{children:"Use a service to switch between modes"}),"\n",(0,s.jsx)(n.li,{children:"Store the demonstration in a simple format (list of poses or commands)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hint:"})," Use a service server to switch modes and maintain a list of recorded poses that can be replayed."]}),"\n",(0,s.jsx)(n.h2,{id:"verification-procedure",children:"Verification Procedure"}),"\n",(0,s.jsx)(n.p,{children:"To verify that your AI agents are working correctly:"}),"\n",(0,s.jsx)(n.h3,{id:"what-appears-in-terminal",children:"What appears in terminal?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When running the AI controller: Continuous logging of state, distance, and commands"}),"\n",(0,s.jsx)(n.li,{children:"When sending commands to the interpreter: Processing messages and execution status"}),"\n",(0,s.jsx)(n.li,{children:"When the agent executes actions: Robot movement and status updates"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"what-changes-in-simulation",children:"What changes in simulation?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The robot should respond intelligently to sensor inputs (Example A)"}),"\n",(0,s.jsx)(n.li,{children:"The robot should execute high-level commands correctly (Example B)"}),"\n",(0,s.jsx)(n.li,{children:"In RViz2, you should see the robot following planned paths"}),"\n",(0,s.jsx)(n.li,{children:"System monitoring should show proper AI decision-making patterns"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"checklist-for-completion",children:"Checklist for Completion"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Simple AI controller created and tested"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Command interpreter agent created and functional"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","High-level commands successfully executed"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Learning agent with demonstration capability (Try Yourself task)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Verification steps completed successfully"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","AI-ROS integration patterns understood"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:'This chapter demonstrated how to integrate Python-based AI agents with ROS 2 systems. You learned about different integration architectures, communication patterns, and cognitive architectures. The examples showed both reactive and goal-based AI systems that can interpret high-level commands and execute them as robot actions. The "Try Yourself" task extended this to include learning capabilities.'}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Source 005: Research on integrating Python-based AI agents with ROS systems"}),"\n",(0,s.jsxs)(n.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,s.jsx)(n.em,{children:"Reinforcement Learning: An Introduction"})," (2nd ed.). MIT Press. ISBN: 978-0262039246."]}),"\n",(0,s.jsx)(n.li,{children:"Source 009: Study on agent-based architectures for robotics and middleware integration"}),"\n",(0,s.jsxs)(n.li,{children:["OpenAI. (2023). ",(0,s.jsx)(n.em,{children:"GPT-4 Technical Report"}),". arXiv preprint arXiv:2303.08774."]}),"\n",(0,s.jsx)(n.li,{children:"Source 032: Technical report on large language models for task planning"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);