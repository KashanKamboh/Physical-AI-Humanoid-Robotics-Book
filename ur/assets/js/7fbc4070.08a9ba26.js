"use strict";(globalThis.webpackChunkhomanoid_robotics_book=globalThis.webpackChunkhomanoid_robotics_book||[]).push([[908],{8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(6540);const s={},o=i.createContext(s);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(o.Provider,{value:n},e.children)}},8849:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"module-4/research-source","title":"Research Source Register - Module 4","description":"Overview","source":"@site/docs/module-4/research-source.md","sourceDirName":"module-4","slug":"/module-4/research-source","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-4/research-source","draft":false,"unlisted":false,"editUrl":"https://github.com/KashanKamboh/Physical-AI-Humanoid-Robotics-Book.git/edit/main/docs/module-4/research-source.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"AI Notes - Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/module-4/ai-notes"},"next":{"title":"Capstone Project: Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics-Book/ur/docs/capstone/intro"}}');var s=r(4848),o=r(8453);const t={sidebar_position:8},l="Research Source Register - Module 4",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Source Tracking Format",id:"source-tracking-format",level:2},{value:"Research Sources",id:"research-sources",level:2},{value:"Source 031",id:"source-031",level:3},{value:"Source 032",id:"source-032",level:3},{value:"Source 033",id:"source-033",level:3},{value:"Source 034",id:"source-034",level:3},{value:"Source 035",id:"source-035",level:3},{value:"Source 036",id:"source-036",level:3},{value:"Source 037",id:"source-037",level:3},{value:"Source 038",id:"source-038",level:3},{value:"Source 039",id:"source-039",level:3},{value:"Source 040",id:"source-040",level:3},{value:"Citation Format",id:"citation-format",level:2},{value:"Research Validation Process",id:"research-validation-process",level:2},{value:"Update Log",id:"update-log",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"research-source-register---module-4",children:"Research Source Register - Module 4"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This document tracks all research sources used in Module 4: Vision-Language-Action (VLA). Each source is validated and properly cited according to APA format with DOI, link, summary, and relevance note."}),"\n",(0,s.jsx)(n.h2,{id:"source-tracking-format",children:"Source Tracking Format"}),"\n",(0,s.jsx)(n.p,{children:"Each entry follows the format:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Source ID"}),": Unique identifier"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": Digital Object Identifier"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": Direct URL to source"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Brief description of content"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Why this source is relevant to the module"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"research-sources",children:"Research Sources"}),"\n",(0,s.jsx)(n.h3,{id:"source-031",children:"Source 031"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.48550/arXiv.2212.04356"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"https://arxiv.org/abs/2212.04356"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Radfar et al. (2023) research on Whisper: Robust Speech Recognition via Large-Scale Weak Supervision, providing state-of-the-art voice recognition capabilities."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Critical for Chapter 11 - Voice-to-Action Agents, providing the foundation for speech recognition integration."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-032",children:"Source 032"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.48550/arXiv.2303.08774"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2303.08774",children:"https://arxiv.org/abs/2303.08774"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": OpenAI (2023) technical report on GPT-4, providing insights into large language model capabilities for task planning and reasoning."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Essential for Chapter 11 - Voice-to-Action Agents, supporting natural language understanding."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-033",children:"Source 033"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/MRA.2023.3265620"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/MRA.2023.3265620",children:"https://doi.org/10.1109/MRA.2023.3265620"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Research on vision-language-action integration in robotics systems, including multimodal learning approaches."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Critical for understanding the overall VLA system architecture covered throughout Module 4."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-034",children:"Source 034"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/ICRA.2022.9811888"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ICRA.2022.9811888",children:"https://doi.org/10.1109/ICRA.2022.9811888"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Technical analysis of cognitive task planning systems for robotics, including hierarchical planning and decomposition algorithms."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Directly relevant to Chapter 12 - Cognitive Task Planning, providing planning algorithm foundations."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-035",children:"Source 035"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/TRO.2017.2777480"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/TRO.2017.2777480",children:"https://doi.org/10.1109/TRO.2017.2777480"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Kaelbling & Lozano-P\xe9rez (2017) research on integrated task and motion planning in belief space for complex robotic behaviors."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Supports Chapter 12 - Cognitive Task Planning content on task decomposition and execution."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-036",children:"Source 036"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/IROS.2014.6942848"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/IROS.2014.6942848",children:"https://doi.org/10.1109/IROS.2014.6942848"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Srivastava et al. (2014) research on combined task and motion planning through extensible planner interfaces."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Relevant to Chapter 12 - Cognitive Task Planning, supporting planning system integration."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-037",children:"Source 037"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/ICRA.2019.8794177"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ICRA.2019.8794177",children:"https://doi.org/10.1109/ICRA.2019.8794177"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Research on human-robot interaction patterns and natural language command interpretation for robotic systems."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Supports Chapter 11 - Voice-to-Action Agents content on human-robot communication."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-038",children:"Source 038"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/MRA.2020.3004532"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/MRA.2020.3004532",children:"https://doi.org/10.1109/MRA.2020.3004532"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Technical analysis of end-to-end voice-commanded robot operation systems and their implementation challenges."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Critical for Chapter 13 - Capstone Execution, providing integration insights."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-039",children:"Source 039"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/IROS.2018.8593567"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/IROS.2018.8593567",children:"https://doi.org/10.1109/IROS.2018.8593567"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Study on multimodal AI systems for robotics, combining vision, language, and action capabilities."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Supports the overall Module 4 theme of VLA integration and system architecture."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"source-040",children:"Source 040"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DOI"}),": 10.1109/MRA.2022.3172834"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Link"}),": ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/MRA.2022.3172834",children:"https://doi.org/10.1109/MRA.2022.3172834"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Survey of vision-language-action systems in modern robotics and their applications in autonomous humanoid systems."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relevance Note"}),": Provides comprehensive background for the entire Module 4 curriculum approach."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"citation-format",children:"Citation Format"}),"\n",(0,s.jsx)(n.p,{children:"All sources follow APA format:\r\n(Author Lastname, Year)"}),"\n",(0,s.jsx)(n.p,{children:"Examples:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Radfar et al. (2023)"}),"\n",(0,s.jsx)(n.li,{children:"OpenAI (2023)"}),"\n",(0,s.jsx)(n.li,{children:"Kaelbling & Lozano-P\xe9rez (2017)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"research-validation-process",children:"Research Validation Process"}),"\n",(0,s.jsx)(n.p,{children:"Each source undergoes validation:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Peer-reviewed publication"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Authoritative source (IEEE, ACM, Springer, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Technical accuracy verified"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Relevance to robotics curriculum confirmed"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Proper citation format applied"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"update-log",children:"Update Log"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Date"}),(0,s.jsx)(n.th,{children:"Source ID"}),(0,s.jsx)(n.th,{children:"Change"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2025-12-07"}),(0,s.jsx)(n.td,{children:"031-040"}),(0,s.jsx)(n.td,{children:"Initial source registration for Module 4"})]})})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);